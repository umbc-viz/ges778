[
  {
    "objectID": "decision_checklist.html",
    "href": "decision_checklist.html",
    "title": "Decisionmaking checklist",
    "section": "",
    "text": "When you’re planning out your own work, it’s important to make conscious decisions about what you’re doing and why. I keep saying this, but one of the things I dislike the most about no-code visualization products like Excel or Tableau is that they encourage you to not make these decisions, but to just rely on defaults or automated suggestions.\nIn contrast to just going with defaults, you as a thoughtful visualization practitioner should go through the decisions you could make in order to determine what decisions you should make. This is something you do in your EDA process, as well as in sketching charts on paper.\nIt’s also useful for understanding a chart someone else made, whether just to read it, or to revise or critique it.",
    "crumbs": [
      "Overview",
      "Decisionmaking checklist"
    ]
  },
  {
    "objectID": "decision_checklist.html#checklist",
    "href": "decision_checklist.html#checklist",
    "title": "Decisionmaking checklist",
    "section": "Checklist",
    "text": "Checklist\nWhat decisions could be / should be / were made about…\n\nwhat data to include\nwhat data to exclude\nwhat visual encodings to use\nwhat types of scales to use (sequential, qualitative, diverging)\nwhat to put on the x- and y-axes\nhow to scale the x- and y-axes\nhow to use color\nhow to use text\nhow to use annotations\nwhere to draw attention\nwhat conclusion to suggest\n\nWhat have I forgotten from this list?",
    "crumbs": [
      "Overview",
      "Decisionmaking checklist"
    ]
  },
  {
    "objectID": "readings.html",
    "href": "readings.html",
    "title": "Readings",
    "section": "",
    "text": "Here are the readings for each week. Please have the readings done before the corresponding class. I’ll try not to assign too much reading, so please come ready to discuss!\n(Please also don’t make me regret saying this but in case you absolutely can’t get all the reading done, they’re in order of importance so start at the top each week.)",
    "crumbs": [
      "Overview",
      "Readings"
    ]
  },
  {
    "objectID": "readings.html#week-2-feb-7-encoding-data-to-visuals-making-meaning-of-data",
    "href": "readings.html#week-2-feb-7-encoding-data-to-visuals-making-meaning-of-data",
    "title": "Readings",
    "section": "Week 2, Feb 7: encoding data to visuals; making meaning of data",
    "text": "Week 2, Feb 7: encoding data to visuals; making meaning of data\n\nWilke, C. (2019). Fundamentals of data visualization: A primer on making informative and compelling figures (First edition). O’Reilly. https://clauswilke.com/dataviz/ chapters 2 & 3\nCairo, A. (2019). How charts lie: Getting smarter about visual information (First edition). W. W. Norton & Company. intro (on Blackboard)\nPick one (or both. They’re both great, but if you only choose one I’d go with the podcast.):\n\nRead D’Ignazio, C. (2015). What would feminist data visualization look like? https://civic.mit.edu/feminist-data-visualization.html\nListen to PolicyViz, Schwabish, J. (n.d.). Catherine D’Ignazio and Lauren Klein (142). Retrieved January 30, 2024, from https://policyviz.com/podcast/episode-142-catherine-dignazio-and-lauren-klein/\n\nWatch Kosara, R. (2019). The DataSaurus, Anscombe’s Quartet, and why summary statistics need to be taken with a grain of salt. https://www.youtube.com/watch?v=RbHCeANCbW0 (5 min video)",
    "crumbs": [
      "Overview",
      "Readings"
    ]
  },
  {
    "objectID": "readings.html#week-3-feb-14-writing-good-code-color",
    "href": "readings.html#week-3-feb-14-writing-good-code-color",
    "title": "Readings",
    "section": "Week 3, Feb 14: writing good code; color",
    "text": "Week 3, Feb 14: writing good code; color\n\nWilke, C. (2019). Fundamentals of data visualization: A primer on making informative and compelling figures (First edition). O’Reilly. https://clauswilke.com/dataviz/ chapters 4, 6, 17 (color, amounts, proportional ink)\nWickham, H., Çetinkaya-Rundel, M., & Grolemund, G. (2023). R for data science: Import, tidy, transform, visualize, and model data (2nd edition). O’Reilly. https://r4ds.hadley.nz/ chapters 1 & 9 (okay to skim as a code reference) (data visualization, layers)",
    "crumbs": [
      "Overview",
      "Readings"
    ]
  },
  {
    "objectID": "readings.html#week-4-feb-21-text-annotation",
    "href": "readings.html#week-4-feb-21-text-annotation",
    "title": "Readings",
    "section": "Week 4, Feb 21: text & annotation",
    "text": "Week 4, Feb 21: text & annotation\n\nPick one (or both) of these styleguides to browse through. Take note of how they use color, text, annotations, and iconography, and what suggestions they have on other elements (labeling axes, direct labeling of points, style of titles, placement and rotation of labels, etc). How do these rules carry over between chart types or for different audiences? How strongly do they create a recognizable visual style or brand?\n\nUrban Institute (2023). Urban Institute Data Visualization style guide. http://urbaninstitute.github.io/graphics-styleguide/\nWorld Health Organization (2023). WHO Data Design Language v.0.9.2. https://apps.who.int/gho/data/design-language/ (it’s not super clear but you have to click through to each section)",
    "crumbs": [
      "Overview",
      "Readings"
    ]
  },
  {
    "objectID": "readings.html#week-5-feb-28-uncertainty-distributions-making-good-decisions",
    "href": "readings.html#week-5-feb-28-uncertainty-distributions-making-good-decisions",
    "title": "Readings",
    "section": "Week 5, Feb 28: uncertainty, distributions, making good decisions",
    "text": "Week 5, Feb 28: uncertainty, distributions, making good decisions\n\nListen to Data Stories, Bertini, E., & Stefaner, M. (n.d.). Visualizing Uncertainty with Jessica Hullman and Matthew Kay (134). Retrieved January 30, 2024, from https://datastori.es/134-visualizing-uncertainty-with-jessica-hullman-and-matthew-kay/\nWilke, C. (2019). Fundamentals of data visualization: A primer on making informative and compelling figures (First edition). O’Reilly. https://clauswilke.com/dataviz/ chapters 7, 9, 10, 13, 14, 20 (okay to skim)\nPick one. If you post on Discord or tell us in class what you learned, you’ll get 1 participation point:\n\nCairo, A. (2019). How charts lie: Getting smarter about visual information (First edition). W. W. Norton & Company. chapter 1 (on Blackboard)\nSmith, N. (2023). How not to be fooled by viral charts. https://www.noahpinion.blog/p/how-not-to-be-fooled-by-viral-charts\nListen to Bertini, E., & Stefaner, M. (n.d.). Cognitive Bias and Visualization with Evanthia Dimara (116). Retrieved February 14, 2024, from https://datastori.es/116-cognitive-bias-and-visualization-with-evanthia-dimara/",
    "crumbs": [
      "Overview",
      "Readings"
    ]
  },
  {
    "objectID": "readings.html#week-6-march-6-responsibility",
    "href": "readings.html#week-6-march-6-responsibility",
    "title": "Readings",
    "section": "Week 6, March 6: responsibility",
    "text": "Week 6, March 6: responsibility\nFocus on finishing the case studies—only one reading and it’s a podcast\n\nListen to Schwabish, J. (n.d.). Frank Elavsky (208). Retrieved February 29, 2024, from https://policyviz.com/podcast/episode-208-frank-elavsky/ and browse through the accessibility tool they discuss: Elavsky, F. (2022). The Chartability Workbook. In Chartability. https://chartability.github.io/POUR-CAF/",
    "crumbs": [
      "Overview",
      "Readings"
    ]
  },
  {
    "objectID": "readings.html#week-7-march-13-accessibility-and-empathy",
    "href": "readings.html#week-7-march-13-accessibility-and-empathy",
    "title": "Readings",
    "section": "Week 7, March 13: accessibility and empathy",
    "text": "Week 7, March 13: accessibility and empathy\n\nI forgot to put last week’s podcast on Blackboard, so if you missed it please listen to it for this week\nMakulec, A. (2020). Ten Considerations Before you Create another Chart about COVID-19. In Nightingale. https://medium.com/nightingale/ten-considerations-before-you-create-another-chart-about-covid-19-27d3bd691be8\nBocoupLLC (2017). A Data Point Walks Into a Bar: Designing Data For Empathy - Lisa Charlotte Rost. https://www.youtube.com/watch?v=8XgF-RmNwUc\nElghany, S. (2023). How Ethical Data Visualization Tells the Human Story. In Nightingale. https://nightingaledvs.com/ethical-data-visualization-tells-the-human-story/",
    "crumbs": [
      "Overview",
      "Readings"
    ]
  },
  {
    "objectID": "readings.html#week-8-march-27-midterm-projects",
    "href": "readings.html#week-8-march-27-midterm-projects",
    "title": "Readings",
    "section": "Week 8, March 27: midterm projects",
    "text": "Week 8, March 27: midterm projects\n\nYau, N. (2017). One Dataset, Visualized 25 Ways. In FlowingData. https://flowingdata.com/2017/01/24/one-dataset-visualized-25-ways/",
    "crumbs": [
      "Overview",
      "Readings"
    ]
  },
  {
    "objectID": "readings.html#week-9-april-3-intro-to-spatial-data-viz",
    "href": "readings.html#week-9-april-3-intro-to-spatial-data-viz",
    "title": "Readings",
    "section": "Week 9, April 3: intro to spatial data viz",
    "text": "Week 9, April 3: intro to spatial data viz\n\nWilke, C. (2019). Fundamentals of data visualization: A primer on making informative and compelling figures (First edition). O’Reilly. https://clauswilke.com/dataviz/ chapter 15\nListen to Nussbaumer Knaflic, C. (n.d.). Maps with Kenneth Field (41). Retrieved March 27, 2024, from https://storytellingwithdata.libsyn.com/storytelling-with-data-41-maps-with-kenneth-field",
    "crumbs": [
      "Overview",
      "Readings"
    ]
  },
  {
    "objectID": "readings.html#week-10-april-10-color-text-annotations-part-2",
    "href": "readings.html#week-10-april-10-color-text-annotations-part-2",
    "title": "Readings",
    "section": "Week 10, April 10: color, text, annotations part 2",
    "text": "Week 10, April 10: color, text, annotations part 2\n\nMuth, L. C. (2024). How to choose a color palette for choropleth maps. In Datawrapper. https://blog.datawrapper.de/how-to-choose-a-color-palette-for-choropleth-maps/\nMuth, L. C. (2024). How to choose an interpolation for your color scale - Datawrapper Blog. In Datawrapper. https://blog.datawrapper.de/interpolation-for-color-scales-and-maps/\nListen or read (or both): O’Donohue, D. (n.d.). Communicating With Maps - The Art Of Cartography. Retrieved March 31, 2024, from https://mapscaping.com/podcast/communicating-with-maps-the-art-of-cartography/\nLovelace, R., Nowosad, J., & Muenchow, J. (2019). Chapter 2 Geographic data in R. In Geocomputation with R. https://r.geocompx.org/spatial-class.html—skim / use just as a reference",
    "crumbs": [
      "Overview",
      "Readings"
    ]
  },
  {
    "objectID": "readings.html#week-11-april-17-responsibility-and-context",
    "href": "readings.html#week-11-april-17-responsibility-and-context",
    "title": "Readings",
    "section": "Week 11, April 17: responsibility and context",
    "text": "Week 11, April 17: responsibility and context\n\nSadler, R. C. (2016). How ZIP codes nearly masked the lead problem in Flint. In The Conversation. http://theconversation.com/how-zip-codes-nearly-masked-the-lead-problem-in-flint-65626\nSimmon, R. (2024). From Space to Story in Data Journalism, Nightingale. In Nightingale. https://nightingaledvs.com/from-space-to-story-in-data-journalism/\nWiseman, A. (2015). Bad Maps Are Everywhere These Days. Here’s How to Avoid Being Fooled. Bloomberg CityLab. https://www.bloomberg.com/news/articles/2015-06-25/how-to-avoid-being-fooled-by-bad-maps",
    "crumbs": [
      "Overview",
      "Readings"
    ]
  },
  {
    "objectID": "weeks/04_understanding_data.html",
    "href": "weeks/04_understanding_data.html",
    "title": "4. Learning about your data",
    "section": "",
    "text": "From Wickham et al. (2023), Exploratory Data Analysis chapter:\nWe’ll follow the steps of the EDA chapter using the acs dataset in the {justviz} package. For simplicity, we’ll focus on Maryland census tracts and just a few variables dealing with housing and income.\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(justviz)\nacs_tr &lt;- acs |&gt;\n  filter(level == \"tract\") |&gt;\n  select(county, name, total_pop, total_hh,\n         homeownership, total_cost_burden, renter_cost_burden,\n         owner_cost_burden, no_vehicle_hh, median_hh_income, pop_density)\nknitr::kable(head(acs_tr))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncounty\nname\ntotal_pop\ntotal_hh\nhomeownership\ntotal_cost_burden\nrenter_cost_burden\nowner_cost_burden\nno_vehicle_hh\nmedian_hh_income\npop_density\n\n\n\n\nAllegany County\n24001000100\n3474\n1577\n0.78\n0.18\n0.39\n0.12\n0.06\n56232\n18.48533\n\n\nAllegany County\n24001000200\n4052\n1390\n0.86\n0.20\n0.33\n0.18\n0.04\n66596\n84.29020\n\n\nAllegany County\n24001000500\n2304\n683\n0.60\n0.20\n0.36\n0.09\n0.13\n47781\n520.91879\n\n\nAllegany County\n24001000600\n3005\n1374\n0.70\n0.20\n0.36\n0.14\n0.10\n48607\n1898.93501\n\n\nAllegany County\n24001000700\n3233\n1462\n0.49\n0.37\n0.48\n0.25\n0.25\n36090\n4538.49941\n\n\nAllegany County\n24001000800\n1932\n786\n0.48\n0.52\n0.68\n0.35\n0.17\n27130\n1529.51773\n\n\n\n\nsummary(acs_tr)\n\n    county              name             total_pop        total_hh   \n Length:1460        Length:1460        Min.   :    5   Min.   :   0  \n Class :character   Class :character   1st Qu.: 2960   1st Qu.:1120  \n Mode  :character   Mode  :character   Median : 4026   Median :1528  \n                                       Mean   : 4220   Mean   :1588  \n                                       3rd Qu.: 5358   3rd Qu.:1999  \n                                       Max.   :14109   Max.   :4644  \n                                                                     \n homeownership    total_cost_burden renter_cost_burden owner_cost_burden\n Min.   :0.0000   Min.   :0.0400    Min.   :0.00       Min.   :0.0000   \n 1st Qu.:0.5000   1st Qu.:0.2100    1st Qu.:0.31       1st Qu.:0.1600   \n Median :0.7400   Median :0.2800    Median :0.44       Median :0.2100   \n Mean   :0.6752   Mean   :0.2932    Mean   :0.43       Mean   :0.2181   \n 3rd Qu.:0.8800   3rd Qu.:0.3700    3rd Qu.:0.55       3rd Qu.:0.2700   \n Max.   :1.0000   Max.   :0.7400    Max.   :1.00       Max.   :1.0000   \n NA's   :4        NA's   :4         NA's   :7          NA's   :12       \n no_vehicle_hh     median_hh_income  pop_density      \n Min.   :0.00000   Min.   : 10000   Min.   :    1.05  \n 1st Qu.:0.02000   1st Qu.: 70525   1st Qu.:  984.33  \n Median :0.05000   Median : 98164   Median : 3413.10  \n Mean   :0.09468   Mean   :104585   Mean   : 4968.47  \n 3rd Qu.:0.12000   3rd Qu.:132052   3rd Qu.: 6822.62  \n Max.   :0.85000   Max.   :250001   Max.   :57424.37  \n NA's   :4         NA's   :6\nWhat types of values do each of these variables represent? Why are their scales so different?",
    "crumbs": [
      "Weekly notes",
      "4. Learning about your data"
    ]
  },
  {
    "objectID": "weeks/04_understanding_data.html#variation",
    "href": "weeks/04_understanding_data.html#variation",
    "title": "4. Learning about your data",
    "section": "Variation",
    "text": "Variation\nFirst a histogram of median household income values:\n\nggplot(acs_tr, aes(x = median_hh_income)) +\n  geom_histogram(color = \"white\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nWarning: Removed 6 rows containing non-finite values (`stat_bin()`).\n\n\n\n\n\n\n\n\nFigure 1\n\n\n\n\n\nThere’s a message and a warning: the message suggests being intentional about the number of bins, and the warning calls our attention to missing values in this column.\nUse the next few chunks of code to experiment with bin specifications. Does your understanding of the data’s distribution change?\n\nggplot(acs_tr, aes(x = median_hh_income)) +\n  geom_histogram(bins = 50) # bins can be determined by setting the number of bins\n\nWarning: Removed 6 rows containing non-finite values (`stat_bin()`).\n\n\n\n\n\n\n\n\n\n\nggplot(acs_tr, aes(x = median_hh_income)) +\n  geom_histogram(binwidth = 10000) # or by the width of bins, with a scale corresponding to the x-axis\n\nWarning: Removed 6 rows containing non-finite values (`stat_bin()`).\n\n\n\n\n\n\n\n\n\nWhat are some values of bins or binwidth that seem reasonable? At what point do either of them start to obscure data?\nAs for the missing values:\n\nacs_tr |&gt;\n  filter(is.na(median_hh_income))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncounty\nname\ntotal_pop\ntotal_hh\nhomeownership\ntotal_cost_burden\nrenter_cost_burden\nowner_cost_burden\nno_vehicle_hh\nmedian_hh_income\npop_density\n\n\n\n\nAnne Arundel County\n24003740400\n4241\n0\nNA\nNA\nNA\nNA\nNA\nNA\n6000.504539\n\n\nAnne Arundel County\n24003980000\n5\n0\nNA\nNA\nNA\nNA\nNA\nNA\n1.048504\n\n\nWashington County\n24043011000\n5212\n0\nNA\nNA\nNA\nNA\nNA\nNA\n3049.954209\n\n\nBaltimore city\n24510100300\n1999\n0\nNA\nNA\nNA\nNA\nNA\nNA\n33122.764732\n\n\nBaltimore city\n24510190300\n2122\n839\n0.20\n0.48\n0.52\n0.34\n0.68\nNA\n15572.409097\n\n\nBaltimore city\n24510250600\n11\n11\n0.55\n0.55\n0.00\n1.00\n0.00\nNA\n5.033091\n\n\n\n\n\n\nEven though we’re probably not going to use the total population and total household variables for any analysis here, I kept them because those sorts of variables that define what your observational unit is are important for checking what’s going on in your data. By which I mean a census tract is made up of a bunch of people (usually about 4,000) in a contiguous area who mostly live in households. But if you work with census data enough, you’ll know that some places have population but few households, or only very small populations altogether—a tract might actually be a jail or a set of college dorms, or maybe the majority of a tract is those sorts of group quarters, and the remainder is too small to reliably calculate some of the data. What we want to do with those tracts can depend on context, but I’ll drop them here.\n\nacs_tr2 &lt;- filter(acs_tr, !is.na(median_hh_income))\n\n\nTypical values\nDoes anything seem weird about the median household income values? Look back at Figure 1 where it may be more apparent. (We’ll talk about this anomaly in the data.)\nSwitching to cost burden rates:\n\nggplot(acs_tr2, aes(x = total_cost_burden)) +\n  geom_histogram(binwidth = 0.03)\n\n\n\n\n\n\n\n\nThis approaches a normal curve, but is skewed. From the histogram, the mean looks to be around 0.3 (looking back at the summary, this is correct), but with quite a few tracts with higher rates. Because this is a proportion, we don’t expect there to be any values below 0 or above 1.\nA boxplot can make it a little easier to figure out what’s typical in your distribution.\n\n# use a dummy value for x because ggplot expects boxplots to be done by a discrete variable\nggplot(acs_tr2, aes(x = 1, y = total_cost_burden)) +\n  geom_boxplot() +\n  coord_flip()\n\n\n\n\n\n\n\n\n\n\nUnusual values\n\nggplot(acs_tr2, aes(x = pop_density)) +\n  geom_histogram(binwidth = 1000)\n\n\n\n\n\n\n\n\nThere are a few tracts that are extremely dense. If we wanted to get a sense of more typical tracts, we could filter those, either from the data or within the limits of the chart:\n\nacs_tr2 |&gt;\n  filter(pop_density &lt; 30000) |&gt;\n  ggplot(aes(x = pop_density)) +\n  geom_histogram(binwidth = 1000)\n\n\n\n\n\n\n\n# if you want bars to be between tick marks instead of centered over them, set boundary = TRUE\nacs_tr2 |&gt;\n  ggplot(aes(x = pop_density)) +\n  geom_histogram(binwidth = 1000, boundary = TRUE, color = \"white\") +\n  scale_x_continuous(limits = c(NA, 30000))\n\nWarning: Removed 8 rows containing non-finite values (`stat_bin()`).\n\n\nWarning: Removed 1 rows containing missing values (`geom_bar()`).\n\n\n\n\n\n\n\n\n\nWe could decide to investigate those high-density tracts. For example, if we’re interested in housing costs, we might drop tracts that seem to mostly be dorms. However, at least these tracts in Montgomery County are actually high-rise condos bordering DC, so we should keep them in.",
    "crumbs": [
      "Weekly notes",
      "4. Learning about your data"
    ]
  },
  {
    "objectID": "weeks/04_understanding_data.html#covariation",
    "href": "weeks/04_understanding_data.html#covariation",
    "title": "4. Learning about your data",
    "section": "Covariation",
    "text": "Covariation\nEspecially when we talk about housing and socio-economic data, we expect things to be correlated—probably even more so than with naturally occurring phenomena, since so much of where we live and what resources we have are determined by history and policy decisions. So it shouldn’t surprise you to find correlations in data like this. In fact, the CDC PLACES dataset uses demographic data to model health measures where they don’t have direct measurements available, so in cases like that you actually want to lean away from digging into correlations too much, or you might end up just confirming the makeup of the model, not finding anything new.\n\nA categorical and a numerical variable\nI’ll reshape the data to get housing tenure into one categorical variable. (If this code doesn’t make sense it’s okay.)\n\ncost_burden &lt;- acs_tr2 |&gt;\n  tidyr::pivot_longer(cols = matches(\"cost_burden\"), \n                      names_to = c(\"tenure\", \".value\"), # split column names into tenure and the name of the measure (cost_burden)\n                      names_pattern = \"(^[a-z]+)_(\\\\w+$)\", # use regex to match a set of lowercase letters at the start of the string, followed by an underscore, then match word characters until the end\n                      names_ptypes = list(tenure = factor())) |&gt;\n  filter(!is.na(cost_burden))\n\n\nggplot(cost_burden, aes(x = cost_burden, color = tenure)) +\n  geom_freqpoly(binwidth = 0.02)\n\n\n\n\n\n\n\n\nThe bit about calling after_stat in the book chapter doesn’t apply here, since we have the same number of observations for each tenure.\n\nggplot(cost_burden, aes(x = tenure, y = cost_burden)) +\n  geom_boxplot()\n\n\n\n\n\n\n\n\n\nggplot(cost_burden, aes(x = forcats::fct_reorder(tenure, cost_burden), \n                        y = cost_burden)) +\n  geom_boxplot()\n\n\n\n\n\n\n\n\n\n\nTwo categorical variables\nThis is a pretty contrived example to match section 10.5.2, but I’ll bin homeownership and housing cost burden into categorical variables, and look at these by county.\n\nacs_tr2 |&gt;\n  mutate(ownership_brk = cut(homeownership, \n                             breaks = c(0, 0.25, 0.5, 0.75, 1), \n                             include.lowest = TRUE, right = FALSE)) |&gt;\n  mutate(income_brk = cut(median_hh_income, \n                          breaks = c(0, 5e4, 1e5, 1.5e5, Inf),\n                          include.lowest = TRUE, right = FALSE)) |&gt;\n  count(income_brk, ownership_brk) |&gt;\n  ggplot(aes(x = income_brk, y = ownership_brk)) +\n  geom_point(aes(size = n)) +\n  scale_size_area()\n\n\n\n\n\n\n\n\nSee if you can adjust the code to use tiles and a color (fill) scale.\n\n\nTwo numerical variables\nSame variables, without the binning\n\nggplot(acs_tr2, aes(x = median_hh_income, y = homeownership)) +\n  geom_point()\n\n\n\n\n\n\n\n\nUse the methods in the book (changing alpha or using a 2D density) to deal with overplotting here.\n\nggplot(acs_tr2, aes(x = median_hh_income, y = homeownership)) +\n  geom_point(alpha = 0.4)\n\n\n\n\n\n\n\n\n\nggplot(acs_tr2, aes(x = median_hh_income, y = homeownership)) +\n  geom_point(shape = \"circle open\")\n\n\n\n\n\n\n\n\n\nggplot(acs_tr2, aes(x = median_hh_income, y = homeownership)) +\n  geom_bin2d()",
    "crumbs": [
      "Weekly notes",
      "4. Learning about your data"
    ]
  },
  {
    "objectID": "weeks/10_accessibility.html",
    "href": "weeks/10_accessibility.html",
    "title": "10. Accessibility and literacy",
    "section": "",
    "text": "For each of these two charts:\n\nDraft a possible headline-style title for this chart that would be appropriate for a general audience\nRevise that headline to what you estimate would be a US 6th grade reading level.\nWrite a very short (2-4 concise sentences) description of the chart that says what type of chart it is, what’s being measured, what types of groups are included (don’t name them all individually), and some important data points.\n\n\n\n\n\n\n\n\n\nFigure 1\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 2",
    "crumbs": [
      "Weekly notes",
      "10. Accessibility and literacy"
    ]
  },
  {
    "objectID": "weeks/10_accessibility.html#warm-up",
    "href": "weeks/10_accessibility.html#warm-up",
    "title": "10. Accessibility and literacy",
    "section": "",
    "text": "For each of these two charts:\n\nDraft a possible headline-style title for this chart that would be appropriate for a general audience\nRevise that headline to what you estimate would be a US 6th grade reading level.\nWrite a very short (2-4 concise sentences) description of the chart that says what type of chart it is, what’s being measured, what types of groups are included (don’t name them all individually), and some important data points.\n\n\n\n\n\n\n\n\n\nFigure 1\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 2",
    "crumbs": [
      "Weekly notes",
      "10. Accessibility and literacy"
    ]
  },
  {
    "objectID": "weeks/10_accessibility.html#accessibility",
    "href": "weeks/10_accessibility.html#accessibility",
    "title": "10. Accessibility and literacy",
    "section": "Accessibility",
    "text": "Accessibility\nAfter talking about making responsible decisions in data visualization, it’s embarrassing to admit that accessibility has been a major oversight of mine, but it’s true, and it’s for no other reason than privilege. On a day-to-day basis I don’t have to think about whether learning or interacting with something will depend on my ability to see well, read complicated text, speak a certain language, navigate stimuli, process information, or access technology and resources. In fact, until last week I hadn’t even bothered writing alt texts for the charts in these notes; I’m going back and doing that now, but my hope for you all is that you start out your data viz careers being more mindful than I’ve been.\nFor the most part when we talk about accessibility, we mean this with respect to disabilities; in static data visualization, this mostly means visual impairments such as blindness, low vision, and colorblindness. If you go on to do interactive or web-based visualization, you’ll also need to think about things like navigation (access for keyboards and assistive devices vs clicking menus only) and animation (can be overstimulating or hard to process). 1\n1 Circa 2017, scrollytelling was very cool and people were very intense with it. I’ve noticed in recent years people have eased up. It can be disorienting for some readers. Webb (2018) convinced me to scrap my scrollytelling plans for some projects during that era.\nWebb, E. (2018). Your Interactive Makes Me Sick. https://source.opennews.org/articles/motion-sick/\nSome of the simplest tasks we can do for static data visualization are using colorblind-friendly palettes, writing alt-text descriptions, and maintaining high contrast ratios between backgrounds and text.\n\nColorblindness\nYou should generally assume your work will be read by at least a few colorblind readers (or people with color-vision deficiency, CVD) and plan your color palettes accordingly. Wilke mentions this as a reason for redundant coding as well, so you’re not relying on color alone to differentiate values. 2\n2 Something that blew my mind is in Frank Elavsky’s interview on PolicyViz. He acknowledges that awareness of CVD has become the norm in data viz, but that it actually predominantly affects white men, and that it shouldn’t be too surprising that that is often the only accommodation made in a field where white men are overrepresented.The most common form of CVD is what’s called red-green colorblindness. Many common R color palettes are colorblind-friendly, and some tools will help you tell whether a palette is or not, or for which color deficiencies they are legible.\nSome code examples:\n\n# Not all Color Brewer palettes are CVD-friendly, but you can filter in the R package\n# or on the website for ones that are\nRColorBrewer::display.brewer.all(colorblindFriendly = TRUE)\n\n\n\n\n\n\n\n# Same goes for Carto Colors\nrcartocolor::display_carto_all(colorblind_friendly = TRUE)\n\n\n\n\n\n\n\n# All Viridis palettes are designed to be CVD-friendly\n# use them with e.g. ggplot2::scale_fill_viridis_c()\ncolorspace::swatchplot(viridisLite::viridis(n = 9))\n\n\n\n\n\n\n\n# Okabe-Ito is built into R and based on lots of research into CVD\ncolorspace::swatchplot(palette.colors(n = 9, palette = \"Okabe-Ito\"))\n\n\n\n\n\n\n\n\nThere are also a lot of tools to help you simulate different types of CVD. This is particularly useful for diverging palettes, which can be hard to make accessible.\n\nset.seed(1)\ncvd_data &lt;- data.frame(group = sample(letters[1:7], size = 200, replace = TRUE),\n                       value = rnorm(200))\ndiv_pal &lt;- RColorBrewer::brewer.pal(n = 7, name = \"RdYlGn\")\n\np &lt;- ggplot(cvd_data, aes(x = value, fill = group)) +\n  geom_dotplot(method = \"histodot\", binpositions = \"all\", binwidth = 0.2)\n\np + \n  scale_fill_manual(values = div_pal) +\n  labs(title = \"Brewer palette RdYlGn\")\n\np + \n  scale_fill_manual(values = colorspace::deutan(div_pal)) +\n  labs(title = \"Deuteranomaly\")\n\np + \n  scale_fill_manual(values = colorspace::protan(div_pal)) +\n  labs(title = \"Protanomaly\")\n\np + \n  scale_fill_manual(values = colorspace::tritan(div_pal)) +\n  labs(title = \"Tritanomaly\")\n\n\n\n\n\n\n\nFigure 3\n\n\n\n\n\n\n\n\n\n\n\nFigure 4\n\n\n\n\n\n\n\n\n\n\n\nFigure 5\n\n\n\n\n\n\n\n\n\n\n\nFigure 6\n\n\n\n\n\nThere are lots of tools to do similar simulations, although many of them require you to have a graphic already saved to a file. An online one that’s good for developing and adjusting palettes is Viz Palette by Susie Lu; this one also accounts for the size of your geometries.\n\n\nViz Palette takes a set of space- or comma-separated colors as hex values. If you have a vector of colors, call\ncat(div_pal, sep = \" \")\nto get it all in one line that you only have to copy & paste once.\n\n\nAlt text\nAlt text is the text that’s displayed in place of, or alongside, and image online and in some types of documents (certain PDF versions, Microsoft Word, etc). If someone is using a screen reader, it will read this text aloud. This can be embedded in posts on most social media platforms as well, and is autogenerated on some (if you ever look at Facebook with a bad internet connection, you might see alt text until the images load.) As the designer of your visualizations, you’re in a unique position to write alt text, since you will have close knowledge of the data and what’s important about it.\nIncluding alt text in R:\n\nFor exporting ggplot charts, you can include alt text in labs(alt = \"\").\nIn Rmarkdown documents, you can include it as the fig.alt chunk option\nSimilar for Quarto documents: fig-alt\nWhen directly including images in Markdown, use ![fig-name](fig_path){fig-alt=\"Alt text goes here\"}\n\n\n\nContrast\nDifferent pieces of your visualization need to have enough contrast to be legible at different sizes, especially between text and its background. This comes up with labels like titles, but especially with direct labels. Generally your labels will be all white or all black (or slightly darker or lighter, respectively), so if you’re putting direct labels on several bars with different colors, make sure you have enough contrast across all of them.\nFor example, this palette starts out very dark and ends very light, so neither white nor black will be legible across all bars. Switching between label colors (light on the dark bars, dark on the light bars) can be distracting or imply something about the data that isn’t there, so it’s better to use a palette where all labels can be the same color.\n\n\n\n\n\n\n\n\nFigure 7\n\n\n\n\n\nThe W3C recommends a minimum contrast ratio of 4.5 for regular-sized text, and 3 for large text. You can use colorspace::contrast_ratio to get calculations of these ratios.\n\ncolorspace::contrast_ratio(inferno, col2 = \"black\", plot = TRUE)\n\n\n\n\n\n\n\nFigure 8\n\n\n\n\n\n\n\nExercise\nGo back to the image descriptions you wrote in the warm-up. Using Cesal (2020) and W3C Web Accessibility Initiative (2024), revise your descriptions so they could work as alt text.\n\nCesal, A. (2020). Writing Alt Text for Data Visualization, Nightingale. In Nightingale. https://medium.com/nightingale/writing-alt-text-for-data-visualization-2a218ef43f81?source=friends_link&sk=32db60d651933b5ac2c5b6507f3763b5\n\nW3C Web Accessibility Initiative. (2024). Images Tutorial: Complex Images. In Web Accessibility Initiative (WAI). https://www.w3.org/WAI/tutorials/images/complex/",
    "crumbs": [
      "Weekly notes",
      "10. Accessibility and literacy"
    ]
  },
  {
    "objectID": "weeks/10_accessibility.html#literacy",
    "href": "weeks/10_accessibility.html#literacy",
    "title": "10. Accessibility and literacy",
    "section": "Literacy",
    "text": "Literacy\nWe easily take for granted the ability to read English fluently, but it’s important to remember that, depending on our audience, many of our readers may not be able to. Twenty-two percent of US adults ages 16 to 74 are rated as having low literacy; in Maryland, this is 20% (National Center for Education Statistics, 2020). 3 So if you’re creating a visualization that needs to work for a general audience, you’ll want to keep your sentences short, language simple, and chart types pretty standard. Or, you can get more complex with the use of annotations or other guidance.\n\nNational Center for Education Statistics. (2020). Program for the International Assessment of Adult Competencies (PIAAC). National Center for Education Statistics. https://nces.ed.gov/surveys/piaac/state-county-estimates.asp\n3 This program outlines definitions of “low literacy,” but in news stories and Wikipedia it’s being referred to as corresponding to a sixth grade reading level. I haven’t found anything directly connected to the program that corroborates that.\nExercise\nTest the reading level of the headlines you wrote using each of these tools:\n\nWordcalc: https://www.wordcalc.com/readability/\nHemingway: https://hemingwayapp.com/\n\nFor the versions of the headlines that you thought were at sixth grade level, what level were they actually? Try to get them to sixth grade or below. What revisions did you have to make to get there?\n\nIf you have access to decent AI tools, they might help with this also. I tried with Chat GPT (GPT-3.5) with mixed results. It came out to 5th grade in the end, but loses some of the memorability of “one in four.”\n\nUser Rewrite the following sentence to be at a US 6th grade reading level: “One in four Black adults, and one in five Latinos, have been unfairly stopped or harassed by police.”\nChatGPT “Police have unfairly stopped or harassed one out of every four Black adults, and one out of every five Latinos.”\nUser Based on the Flesch-Kincaid scale, that’s a 9.9 reading level\nChatGPT Here’s a revised sentence with a lower reading level:\n“Police have unfairly stopped or harassed one out of every four Black adults, and one out of every five Latinos.”\nThis should be around a 6th grade reading level according to the Flesch-Kincaid scale.\nUser That’s the same sentence\nChatGPT I apologize for the confusion. Here’s another attempt:\n“Police have unfairly stopped or harassed 25% of Black adults and 20% of Latinos.”\nThis should be around a 6th grade reading level according to the Flesch-Kincaid scale.",
    "crumbs": [
      "Weekly notes",
      "10. Accessibility and literacy"
    ]
  },
  {
    "objectID": "weeks/08_uncertainty.html",
    "href": "weeks/08_uncertainty.html",
    "title": "8. Uncertainty and distributions",
    "section": "",
    "text": "Fill in this quick survey about probability:",
    "crumbs": [
      "Weekly notes",
      "8. Uncertainty and distributions"
    ]
  },
  {
    "objectID": "weeks/08_uncertainty.html#warm-up",
    "href": "weeks/08_uncertainty.html#warm-up",
    "title": "8. Uncertainty and distributions",
    "section": "",
    "text": "Fill in this quick survey about probability:",
    "crumbs": [
      "Weekly notes",
      "8. Uncertainty and distributions"
    ]
  },
  {
    "objectID": "weeks/08_uncertainty.html#distributions",
    "href": "weeks/08_uncertainty.html#distributions",
    "title": "8. Uncertainty and distributions",
    "section": "Distributions",
    "text": "Distributions\nWhen we visualize data, one of the most important decisions we make is what values exactly we’ll display. That’s because you usually can’t include every data point, so you’ll have to do some amount of summarizing. However, that means you’re losing a lot of information in the process. That’s a fine line you need to figure out how to walk, and like most things we’ve done so far, how you do that will depend on context, audience, and purpose. In my own work, I know I fall on the side of oversimplifying more often than I should.\nThe problem is that unless people have a background in statistics or other quantitative research-heavy fields, they probably aren’t used to hearing about ranges of data—let alone ways to describe distributions, like skew and kurtosis. So the chart types that are best for showing distributions or uncertainty are generally pretty technical.\nFor example, look at the range of median household incomes by county:\n\n\n\n\n\n\n\n\n\nThere’s already a wide range, but compare that to all their tracts:\n\n\n\n\n\n\n\n\n\nOf the counties in the Baltimore metro area, Howard County has a much higher overall median income than Baltimore city, yet there’s also a lot of overlapping values. Baltimore city has several tracts with pretty high incomes, but that fact gets washed out when we only look at summary values. Even just look at how different the income scales are!\nThink back to the wage gap data. When we just look at wages for men vs women, 1 we lose differences within and between those groups. Before we saw how median earnings increase with educational attainment, but women’s pay lags about one education level behind men’s pay. We’ll see other gaps when we look at earnings by sex and race/ethnicity.\n1 Speaking of distributions within data, there are gray areas in gender that aren’t captured by the Census.\n\n\n\n\n\n\n\n\nSide-by-side bars of race versus sex show much more of the range. When we talk about the wage gap as just men vs women, we miss the fact that white women have higher median earnings than Black or Latino men! 2\n2 We also miss gaps within those groups, such as disparities by ethnicity, origin, and immigration status. An intern I worked with a few years ago made this video looking at the wage gaps within Asian American groups.\n\n\n\n\n\n\n\n\nUnlike bars, dots don’t have to start at a 0 baseline, so we can zoom in on the actual range of the values. This view makes it easier to see that the gaps within white and Asian/Pacific Islander communities span about $20,000 each, but is nearly nonexistent for Black adults. (For charts like this, you’ll more often see the axes flipped and the groups ordered largest to smallest, or largest gap to smallest gap, but I’ll let you figure that out—take a look at forcats::fct_reorder.)\n\n\n\n\n\n\n\n\n\n\nExercise\nJust like we can disaggregate the data into race vs sex, we can also tease apart the distributions within those groups. The wage gap data has not just medians, but also 20th, 25th, 75th, and 80th percentile values for every group.\nHere’s a bad chart of this data that just dumps all the data into a bunch of points. Looking at the examples from Yau (2013) (on Blackboard), brainstorm some better ways to show this data, including what you would want to filter out.\n\nYau, N. (2013). Data points: Visualization that means something. John Wiley & Sons, Inc.\n\n# reshape to long in order to make dot plot\nwages_quants &lt;- wages_sex_race |&gt;\n  select(dimension, sex, race_eth, matches(\"earn_q\\\\d+\")) |&gt;\n  tidyr::pivot_longer(cols = earn_q20:earn_q80, names_to = c(\".value\", \"quantile\"), \n                      names_pattern = \"(^[a-z]+)_(.+$)\",\n                      names_ptypes = list(quantile = factor()))\n\n# diverging-ish palette is appropriate here, so I'll pull it to modify\n# diverging pals usually have a light color in the middle, which won't work well for points\n# div_pal &lt;- RColorBrewer::brewer.pal(n = 5, name = \"Spectral\")\ndiv_pal &lt;- viridisLite::turbo(n = 5, direction = -1, begin = 0.1, end = 0.9)\ndiv_pal[3] &lt;- \"gray20\"\n\nwages_quants |&gt;\n  ggplot(aes(x = sex, y = earn, color = quantile, shape = sex, group = sex)) +\n  geom_path(color = \"gray80\", linewidth = 2, alpha = 0.8) +\n  geom_point(size = 3, alpha = 0.9) +\n  scale_color_manual(values = div_pal) +\n  facet_wrap(vars(race_eth), scales = \"free_x\")\n\n\n\n\n\n\n\n\nHere’s one option, although I’m not super satisfied with it:",
    "crumbs": [
      "Weekly notes",
      "8. Uncertainty and distributions"
    ]
  },
  {
    "objectID": "weeks/08_uncertainty.html#uncertainty",
    "href": "weeks/08_uncertainty.html#uncertainty",
    "title": "8. Uncertainty and distributions",
    "section": "Uncertainty",
    "text": "Uncertainty\nIt can be really hard to imagine or estimate uncertainty. We expect data to be exact, and it pretty much never is. Visualization can help explain this to people, but that can conflict with our usual desire to make our visualizations simple and quick to read. In fact, I dropped all the margins of error from the datasets in the justviz package when I made it, so I’m part of the problem.\nWilke’s chapter on uncertainty has some good examples of how to show uncertainty in terms of margin of error in a few different types of charts. At the same time, there are some arguments against using error bars like he’s done in some of the examples. One of the case study readings (Correll & Gleicher (2014)) finds that these can actually harm people’s ability to understand uncertainty.\n\nCorrell, M., & Gleicher, M. (2014). Error Bars Considered Harmful: Exploring Alternate Encodings for Mean and Error. IEEE Transactions on Visualization and Computer Graphics, 20(12), 2142–2151. https://doi.org/10.1109/TVCG.2014.2346298\nProbably the most famous and famously controversial attempt at visualizing uncertainty was the gauge chart the New York Times used on election night 2016. It was meant to show that the vote margins were in flux as counts came in, but the jittering effect was actually hard-coded into the visualization rather than based directly on tallies updating. People got extremely stressed and mad.\nFor working in ggplot, the ggdist package (Kay (2024)) has some good options for showing distributions and uncertainty. 3\n\nKay, M. (2024). Mjskay/ggdist. https://github.com/mjskay/ggdist\n3 ggdist::geom_dots or ggdist::geom_interval and its arguments could make a good replacement for the boxplot of median income above.4 I can never define margin of error properly, but here’s a good overview.To make up for dropping margins of error before, I made a new dataset that we’ll use here. It’s the same wages data by sex, education, and occupation group, and it includes margins of error at both the 90% and 95% confidence levels. 4 When I analyzed the wage data, it came from a subset of the ACS, so a sample of a sample. With survey data like this, you have to worry about having large enough sample sizes to get reliable estimates. So far we’ve used fairly large groups (women in Maryland with bachelor’s degrees, etc), but when we slice the data more, we start to get less reliable estimates.\nFor example, calculating median income of women in military occupations by education leaves us with some very small sample sizes, and some very large MOEs. Rule of thumb is usually that you need at least 30 observations in your sample for estimates to be useful; you might also want to set a minimum number of estimated counts.\n\nwages_moe &lt;- readRDS(here::here(\"inputs/wages_sex_edu_occ.rds\")) |&gt;\n  mutate(occ_group = forcats::as_factor(occ_group) |&gt;\n           forcats::fct_relabel(stringr::str_replace, \",? \\\\band\\\\b\", \" &\")) |&gt;\n  mutate(lower_95 = earn_q50 - moe_95,\n         lower_90 = earn_q50 - moe_90,\n         upper_90 = earn_q50 + moe_90,\n         upper_95 = earn_q50 + moe_95)\n\nwages_moe |&gt;\n  filter(sex == \"Women\",\n         occ_group == \"Military Specific\") |&gt;\n  select(6:12)\n\n\n\n\n\nedu\ncount\nsample_n\nearn_q50\nse\nmoe_90\nmoe_95\n\n\n\n\nTotal\n2002\n92\n64738\n5916\n9731\n11595\n\n\nHigh school or less\n182\n14\n34600\n17175\n28250\n33662\n\n\nSome college or AA\n732\n28\n63586\n11043\n18164\n21644\n\n\nBachelors\n558\n27\n62580\n4182\n6879\n8197\n\n\nGraduate degree\n530\n23\n85000\n15611\n25678\n30597\n\n\n\n\n\n\nReporting that estimate for women in military occupations with a graduate degree would be silly: $85k ± $30k means that at a 95% confidence level, you’ve estimated the median to be between $55k and $115k, which tells you virtually nothing.\nError bars are one simple way to show the MOE. When we don’t split things up by education, and we look statewide, the MOEs aren’t so bad for most occupational groups (note which ones are pretty big, and for which genders).\n\n\n\n\n\n\n\n\n\nOnce we split by education, however, we get wider margins. Note also that we still have pretty wide distributions within occupation: what’s the difference between a healthcare job someone with a graduate degree has and one someone with at most a high school degree?\n\n\n\n\n\n\n\n\n\nAnother thing to know about margins of error is that they can be used as a kind of crude approximation of statistical testing (t-tests, etc). For example, the margins of error for people with graduate degrees in production occupations overlap, so we shouldn’t say they differ by sex until we do formal testing. For service jobs, however, the MOEs don’t overlap, so that’s a safer bet (but not a replacement for tests of statistical significance).",
    "crumbs": [
      "Weekly notes",
      "8. Uncertainty and distributions"
    ]
  },
  {
    "objectID": "weeks/08_uncertainty.html#missing-data",
    "href": "weeks/08_uncertainty.html#missing-data",
    "title": "8. Uncertainty and distributions",
    "section": "Missing data",
    "text": "Missing data\nThere’s a lot of different reasons data might be missing, and different ways to handle it. Here’s just one tidbit to handle those small samples from the wage data that you’re likely to encounter.\n\n# I'll use a few metrics to decide which observations to keep:\n# coefficient of variance (MOE / estimate) needs to be less than 0.3, based on 95% CI\n# sample size needs to be at least 50\nwages_sample_size &lt;- wages_moe |&gt;\n  mutate(cov = moe_95 / earn_q50) |&gt;\n  filter(sex != \"Total\",\n         edu == \"Graduate degree\") |&gt;\n  mutate(too_small = sample_n &lt; 50 | cov &gt; 0.3) |&gt;\n  select(sex, occ_group, earn_q50, too_small)\n\nwages_sample_size |&gt;\n  filter(too_small)\n\n\n\n\n\n\n\n\n\n\n\nsex\nocc_group\nearn_q50\ntoo_small\n\n\n\n\nWomen\nMilitary Specific\n85000\nTRUE\n\n\nWomen\nNatural Resources, Construction & Maintenance\n95378\nTRUE\n\n\n\n\n\n\nIf we drop those unreliable values, or have a similar dataset with missing values, we’ll get something like this:\n\nwages_sample_size |&gt;\n  filter(!too_small) |&gt;\n  ggplot(aes(x = occ_group, y = earn_q50, fill = sex)) +\n  geom_col(width = 0.8, position = position_dodge2()) +\n  coord_flip() +\n  scale_x_discrete(labels = scales::label_wrap(30)) +\n  scale_y_continuous(labels = dollar_k) +\n  scale_fill_manual(values = gender_pal) +\n  theme(panel.grid.major.x = element_line())\n\n\n\n\n\n\n\n\nDodged bars will fill up the available space by default. Instead, use preserve = \"single\" inside position_dodge.\n\nwages_sample_size |&gt;\n  filter(!too_small) |&gt;\n  ggplot(aes(x = occ_group, y = earn_q50, fill = sex, group = sex)) +\n  geom_col(width = 0.8, position = position_dodge2(preserve = \"single\")) +\n  coord_flip() +\n  scale_x_discrete(labels = scales::label_wrap(30)) +\n  scale_y_continuous(labels = dollar_k) +\n  scale_fill_manual(values = gender_pal) +\n  theme(panel.grid.major.x = element_line())\n\n\n\n\n\n\n\n\nWith time series data, you can usually assume the intervals are even (every month, every week, etc.). If it’s not for whatever reason, you might want to add some visual cues for transparency. This is a forced example where I drop some observations from the unemployment data.\n\n\n\n\n\n\n\n\n\nThere are 2 months missing here, but you can’t tell because the lines get connected regardless of discontinuities. Adding points makes it clearer when observations were made, although this might not work when you have a lot of points (that’s why I’m only using one year for this example).\n\n\n\n\n\n\n\n\n\nIn a lot of cases, that will be enough. If you need more accuracy, you might convert the data into a time series (I like the tsibble package because it works well with dplyr) and fill in missing observations. This also gives you options of imputing the missing values, but it’s outside the scope of this class.\n\n# install the tsibble library if you need to\nlibrary(tsibble)\n\n# fill_gaps adds explicitly missing observations in place of missing values, \n# in this case monthly\n# this isn't the best way to hack this chart together but it's the easiest\nunemp_ts &lt;- unemp_missing |&gt;\n  mutate(month = yearmonth(date)) |&gt;\n  as_tsibble(key = name, index = month) |&gt;\n  fill_gaps() |&gt;\n  as_tibble() |&gt;\n  mutate(month = lubridate::ym(month))\n\nunemp_ts |&gt;\n  ggplot(aes(x = month, y = adjusted_rate)) +\n  geom_line(aes(x = date), linetype = \"dashed\") +\n  geom_line() +\n  geom_point() +\n  scale_x_date(date_breaks = \"2 months\", date_labels = \"%b %Y\") +\n  theme(panel.grid.major.x = element_line())",
    "crumbs": [
      "Weekly notes",
      "8. Uncertainty and distributions"
    ]
  },
  {
    "objectID": "weeks/03_encoding.html",
    "href": "weeks/03_encoding.html",
    "title": "3. Visual encoding",
    "section": "",
    "text": "This is a walkthrough of Wickham et al. (2023) chapter 9 on chart layers, using the ACS data in the justviz package. For simplicity, we’ll focus on Maryland census tracts. I’m throwing in a few additional variables just to match the examples from the book.\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(justviz)\n# create a variable that flags tracts being in the city or surrounding counties. \n# other values get lumped into \"other counties\" group\nlocal_counties &lt;- c(\"Baltimore city\", \"Baltimore County\", \"Anne Arundel County\", \"Howard County\")\nacs_tr &lt;- acs |&gt;\n  filter(level == \"tract\") |&gt;\n  mutate(county2 = ifelse(county %in% local_counties, county, \"Other counties\")) |&gt;\n  na.omit() |&gt; # we'll talk about missing data in the next notebook\n  mutate(income_brk = cut(median_hh_income, \n                          breaks = c(0, 1e5, Inf), \n                          labels = c(\"under_100k\", \"above_100k\"),\n                          include.lowest = TRUE, right = FALSE))\nggplot(acs_tr, aes(x = homeownership, y = total_cost_burden)) +\n  geom_point(size = 1)",
    "crumbs": [
      "Weekly notes",
      "3. Visual encoding"
    ]
  },
  {
    "objectID": "weeks/03_encoding.html#aesthetic-mappings",
    "href": "weeks/03_encoding.html#aesthetic-mappings",
    "title": "3. Visual encoding",
    "section": "Aesthetic mappings",
    "text": "Aesthetic mappings\n\nggplot(acs_tr, aes(x = homeownership, y = total_cost_burden)) +\n  geom_point(aes(color = county2), size = 1)\n\n\n\n\n\n\n\nggplot(acs_tr, aes(x = homeownership, y = total_cost_burden)) +\n  geom_point(aes(shape = county2), size = 1)\n\n\n\n\n\n\n\n\nAs noted in the book, these are bad ideas:\n\nggplot(acs_tr, aes(x = homeownership, y = total_cost_burden)) +\n  geom_point(aes(size = county2), alpha = 0.5)\n\n\n\n\n\n\n\nggplot(acs_tr, aes(x = homeownership, y = total_cost_burden)) +\n  geom_point(aes(alpha = county2), size = 1)\n\n\n\n\n\n\n\n\nCan you think of any exceptions to this?\n\nWhat’s going on with the next two charts?\n\nggplot(acs_tr, aes(x = homeownership, y = total_cost_burden)) +\n  geom_point(color = \"slateblue\")\n\n\n\n\n\n\n\nggplot(acs_tr, aes(x = homeownership, y = total_cost_burden)) +\n  geom_point(aes(color = \"slateblue\"))\n\n\n\n\n\n\n\n\nWhy does this one throw an error?\n\nggplot(acs_tr, aes(x = homeownership, y = total_cost_burden)) +\n  geom_point(color = county2)",
    "crumbs": [
      "Weekly notes",
      "3. Visual encoding"
    ]
  },
  {
    "objectID": "weeks/03_encoding.html#geometric-objects",
    "href": "weeks/03_encoding.html#geometric-objects",
    "title": "3. Visual encoding",
    "section": "Geometric objects",
    "text": "Geometric objects\n\nggplot(acs_tr, aes(x = homeownership, y = total_cost_burden)) +\n  geom_point(size = 1)\n\n\n\n\n\n\n\nggplot(acs_tr, aes(x = homeownership, y = total_cost_burden)) +\n  geom_smooth()\n\n\n\n\n\n\n\n\n\nggplot(acs_tr, aes(x = homeownership, y = total_cost_burden, color = county2)) +\n  geom_point(size = 1) +\n  geom_smooth()\n\n\n\n\n\n\n\n\n\nggplot(acs_tr, aes(x = homeownership, y = total_cost_burden)) +\n  geom_smooth()\n\n\n\n\n\n\n\nggplot(acs_tr, aes(x = homeownership, y = total_cost_burden)) +\n  geom_smooth(aes(group = county2))\n\n\n\n\n\n\n\nggplot(acs_tr, aes(x = homeownership, y = total_cost_burden)) +\n  geom_smooth(aes(color = county2), show.legend = FALSE)\n\n\n\n\n\n\n\n\n\nggplot(acs_tr, aes(x = homeownership, y = total_cost_burden)) +\n  geom_point(aes(color = county2), size = 1) +\n  geom_smooth()\n\n\n\n\n\n\n\n\nI don’t like how they did this highlighting example in the book. Here’s a better one.\n\nggplot(acs_tr, aes(x = homeownership, y = total_cost_burden)) +\n  geom_point(aes(color = county == \"Baltimore city\")) +\n  scale_color_manual(values = c(\"TRUE\" = \"firebrick\", \"FALSE\" = \"gray60\"))\n\n\n\n\n\n\n\n\n\nggplot(acs_tr, aes(x = homeownership, y = county2, fill = county2, color = county2)) +\n  ggridges::geom_density_ridges(alpha = 0.5, show.legend = FALSE)",
    "crumbs": [
      "Weekly notes",
      "3. Visual encoding"
    ]
  },
  {
    "objectID": "weeks/03_encoding.html#facets",
    "href": "weeks/03_encoding.html#facets",
    "title": "3. Visual encoding",
    "section": "Facets",
    "text": "Facets\n\nacs_tr |&gt;\n  ggplot(aes(x = homeownership, y = total_cost_burden)) +\n  geom_point(size = 1) +\n  facet_grid(cols = vars(county2), rows = vars(income_brk))",
    "crumbs": [
      "Weekly notes",
      "3. Visual encoding"
    ]
  },
  {
    "objectID": "weeks/03_encoding.html#statistical-transformations",
    "href": "weeks/03_encoding.html#statistical-transformations",
    "title": "3. Visual encoding",
    "section": "Statistical transformations",
    "text": "Statistical transformations\nI am of the opinion that if you want to visualize summary statistics or other aggregations, you should calculate them explicitly, not let ggplot do them ad hoc, so I think the examples in section 9.5 are not great. Comparable charts with calculations:\n\nacs_tr |&gt;\n  group_by(county2) |&gt;\n  summarise(n = n()) |&gt; # these 2 steps can be done with `count`\n  ggplot(aes(x = county2, y = n)) +\n  geom_col()\n\n\n\n\n\n\n\n\n\nacs_tr |&gt;\n  group_by(county2) |&gt;\n  summarise(n = n()) |&gt; # keeping data grouped by county2 lets you calc proportions\n  mutate(prop = n / sum(n)) |&gt;\n  ggplot(aes(x = county2, y = prop)) +\n  geom_col()\n\n\n\n\n\n\n\n\n\nacs_tr |&gt;\n  group_by(county2) |&gt;\n  summarise(across(total_cost_burden, list(min = min, max = max, median = median))) |&gt;\n  ggplot(aes(x = county2)) +\n  geom_pointrange(aes(y = total_cost_burden_median, \n                     ymin = total_cost_burden_min, \n                     ymax = total_cost_burden_max))",
    "crumbs": [
      "Weekly notes",
      "3. Visual encoding"
    ]
  },
  {
    "objectID": "weeks/03_encoding.html#position-aesthetics",
    "href": "weeks/03_encoding.html#position-aesthetics",
    "title": "3. Visual encoding",
    "section": "Position aesthetics",
    "text": "Position aesthetics\n\ninc_by_county &lt;- acs_tr |&gt;\n  group_by(county2, income_brk) |&gt;\n  summarise(n = n())\n\nggplot(inc_by_county, aes(x = county2, y = n, color = income_brk)) +\n  geom_col()\n\n\n\n\n\n\n\nggplot(inc_by_county, aes(x = county2, y = n, fill = income_brk)) +\n  geom_col()\n\n\n\n\n\n\n\n\n\nggplot(inc_by_county, aes(x = county2, y = n, fill = income_brk)) +\n  geom_col(alpha = 1/5, position = position_identity())\n\n\n\n\n\n\n\nggplot(inc_by_county, aes(x = county2, y = n, fill = income_brk)) +\n  geom_col(position = position_fill())\n\n\n\n\n\n\n\nggplot(inc_by_county, aes(x = county2, y = n, fill = income_brk)) +\n  geom_col(position = position_dodge())\n\n\n\n\n\n\n\nggplot(inc_by_county, aes(x = county2, y = n, fill = income_brk)) +\n  geom_col(position = position_dodge2())\n\n\n\n\n\n\n\n\nOther than the first chart with the weird opacity, which kinda sucks, these give you different views of the same data. What can you pick up from each?\n\nggplot(acs_tr, aes(x = homeownership, y = total_cost_burden)) +\n  geom_point(size = 1, position = position_jitter(seed = 1))",
    "crumbs": [
      "Weekly notes",
      "3. Visual encoding"
    ]
  },
  {
    "objectID": "weeks/13_data_to_space.html",
    "href": "weeks/13_data_to_space.html",
    "title": "13. Mapping data to space",
    "section": "",
    "text": "Think back to the first principles we talked about with mapping data onto visual encodings (reading lengths of bar charts, reading darkness of color). With spatial data, we have many of the same principles, but we’re confined by location. We can’t just, say, flip our axes to make them easier to read, or change the aspect ratio to fit in a tight space on a page. We also can’t just arbitrarily change the size of things we’re drawing—for example, in a US state map, we can’t just make Texas less huge.\nThat said, there are a lot of decisions we do make about how we encode data to space or manipulate it, and these can change the nature of what we know about our data. That makes it important to think really hard about those decisions, and situate those decisions in the purpose of your visualization, your audience, the scope of your work, domain knowledge, etc etc. Often there are no perfectly right answers.\n\nlibrary(dplyr)\nlibrary(justviz)\nlibrary(sf)\nlibrary(ggplot2)\n\n# justviz::tracts_sf is only baltimore & neighboring counties\nall_tracts_sf &lt;- tigris::tracts(state = \"24\", cb = TRUE, progress_bar = FALSE) |&gt;\n  select(geoid = GEOID, county = NAMELSADCO)\ncounties_sf &lt;- all_tracts_sf |&gt;\n  group_by(county) |&gt;\n  summarise()\n\nWhen we looked at Hans Rosling’s bubble plots, the size (area) of points showed the population of each country. That makes sense perceptually—your brain is going to see things of different sizes, pick up on that difference, and think it means something. But in spatial data, it might not; different sizes might actually interfere with our ability to understand what the visual is meant to show. That is, the area of a location might have something to do with the data, but oftentimes it doesn’t (sometimes if anything it shows the opposite—some of the largest US states have among the smallest populations, same goes for Maryland counties). Census tracts are designed to have similar populations, but counties are arbitrary political boundaries.\n\nacs |&gt;\n  filter(level == \"county\") |&gt;\n  ggplot(aes(x = area_sqmi, y = total_pop)) +\n  geom_point() +\n  ggrepel::geom_text_repel(aes(label = name), \n                           data = ~filter(., between(total_pop, 5e5, 7e5))) +\n  labs(title = \"Total population vs area, Maryland counties\")\n\n\n\n\n\n\n\nacs |&gt;\n  filter(level %in% c(\"tract\", \"county\")) |&gt;\n  ggplot(aes(x = total_pop)) +\n  geom_histogram() +\n  facet_wrap(vars(level), scales = \"free\") +\n  labs(title = \"Total population, Maryland counties and tracts\")\n\n\n\n\n\n\n\n\nCan you guess what this is a map of?",
    "crumbs": [
      "Weekly notes",
      "13. Mapping data to space"
    ]
  },
  {
    "objectID": "weeks/13_data_to_space.html#space",
    "href": "weeks/13_data_to_space.html#space",
    "title": "13. Mapping data to space",
    "section": "",
    "text": "Think back to the first principles we talked about with mapping data onto visual encodings (reading lengths of bar charts, reading darkness of color). With spatial data, we have many of the same principles, but we’re confined by location. We can’t just, say, flip our axes to make them easier to read, or change the aspect ratio to fit in a tight space on a page. We also can’t just arbitrarily change the size of things we’re drawing—for example, in a US state map, we can’t just make Texas less huge.\nThat said, there are a lot of decisions we do make about how we encode data to space or manipulate it, and these can change the nature of what we know about our data. That makes it important to think really hard about those decisions, and situate those decisions in the purpose of your visualization, your audience, the scope of your work, domain knowledge, etc etc. Often there are no perfectly right answers.\n\nlibrary(dplyr)\nlibrary(justviz)\nlibrary(sf)\nlibrary(ggplot2)\n\n# justviz::tracts_sf is only baltimore & neighboring counties\nall_tracts_sf &lt;- tigris::tracts(state = \"24\", cb = TRUE, progress_bar = FALSE) |&gt;\n  select(geoid = GEOID, county = NAMELSADCO)\ncounties_sf &lt;- all_tracts_sf |&gt;\n  group_by(county) |&gt;\n  summarise()\n\nWhen we looked at Hans Rosling’s bubble plots, the size (area) of points showed the population of each country. That makes sense perceptually—your brain is going to see things of different sizes, pick up on that difference, and think it means something. But in spatial data, it might not; different sizes might actually interfere with our ability to understand what the visual is meant to show. That is, the area of a location might have something to do with the data, but oftentimes it doesn’t (sometimes if anything it shows the opposite—some of the largest US states have among the smallest populations, same goes for Maryland counties). Census tracts are designed to have similar populations, but counties are arbitrary political boundaries.\n\nacs |&gt;\n  filter(level == \"county\") |&gt;\n  ggplot(aes(x = area_sqmi, y = total_pop)) +\n  geom_point() +\n  ggrepel::geom_text_repel(aes(label = name), \n                           data = ~filter(., between(total_pop, 5e5, 7e5))) +\n  labs(title = \"Total population vs area, Maryland counties\")\n\n\n\n\n\n\n\nacs |&gt;\n  filter(level %in% c(\"tract\", \"county\")) |&gt;\n  ggplot(aes(x = total_pop)) +\n  geom_histogram() +\n  facet_wrap(vars(level), scales = \"free\") +\n  labs(title = \"Total population, Maryland counties and tracts\")\n\n\n\n\n\n\n\n\nCan you guess what this is a map of?",
    "crumbs": [
      "Weekly notes",
      "13. Mapping data to space"
    ]
  },
  {
    "objectID": "weeks/13_data_to_space.html#areal-units",
    "href": "weeks/13_data_to_space.html#areal-units",
    "title": "13. Mapping data to space",
    "section": "Areal units",
    "text": "Areal units\nOne major issue that comes up in mapping is the modifiable areal unit problem:\n\nThe MAUP occurs when statistical results or visual patterns embedded in maps differ according to changes in the scale or zoning of the partitioning scheme used to aggregate spatial data (Openshaw, 1984). The problem commonly occurs when data on individual objects, such as people, are disseminated in data sets in aggregated form, such that the actual spatial distribution of the original individual objects is unobtainable….\nOne consequence of the MAUP is that spatial patterns that may be observed in a map using one partitioning scheme may not be visible using another partitioning scheme where the scale or zoning differs. –Mennis (2019)\n\nMennis, J. (2019). Problems of Scale and Zoning. Geographic Information Science & Technology Body of Knowledge, 2019(Q1). https://doi.org/10.22224/gistbok/2019.1.2\n\nSometimes the units you use for analysis are ones you can decide on, but sometimes you don’t have a choice (e.g. data would be most useful at the tract level but is only available by town; in a survey you ask for people’s ZIP code because you know they probably don’t know their tract number).\nSimilar to this is the ecological fallacy, where you assume that patterns or phenomena that you see in aggregate are true for individuals or individual areas within that population.\n\nMAUP and ecological fallacy\nWhen we break a geography into areas that are too large, we lose the patterns that occur within them. If we break into areas that are too small, we might no longer be able to do the analysis we want, or we might be looking at something so closely that patterns aren’t able to develop. For example, counting trees across an entire city won’t tell you anything about tree density in people’s neighborhoods if most of those trees are within one large park; counting trees per parcel in that same city won’t be that useful if you only expect one or two trees at most in front of any house, and if residents get the same benefits of having a tree in front of their neighbor’s house as they do their own.\nFor example, Baltimore city is 27 percent white. That’s the lowest share of all the counties in the state except Prince George’s. But that population is very concentrated within the city.\n\nacs_county &lt;- acs |&gt;\n  filter(level == \"county\") |&gt;\n  mutate(name = forcats::as_factor(name) |&gt;\n           forcats::fct_reorder(white, .desc = FALSE))\n\nw1 &lt;- acs_county |&gt;\n  ggplot(aes(x = name, y = white)) +\n  geom_col(width = 0.8) +\n  coord_flip() +\n  scale_y_continuous(labels = scales::label_percent()) +\n  labs(x = NULL)\n\nw2 &lt;- acs |&gt;\n  filter(level == \"tract\") |&gt;\n  mutate(county = forcats::as_factor(county) |&gt;\n           forcats::fct_relevel(levels(acs_county$name))) |&gt;\n  ggplot(aes(x = county, y = white)) +\n  geom_boxplot() +\n  coord_flip() +\n  scale_y_continuous(labels = scales::label_percent()) +\n  labs(x = NULL)\n\nlibrary(patchwork)\n\nw1 + w2\n\n\n\n\n\n\n\n\nEven though a much lower share of Baltimore city overall is white than some of the smaller or less diverse counties like Garrett and Carroll, the city is so segregated that we have neighborhoods that do in fact rival those counties’ averages. We lose that fact when we only look at averages or aggregates, but it’s a major pattern underlying much of how the city and region work (hot take: that’s by design).\n\n\nJoining spatial and nonspatial data: a detour\nYou can join sf data frames with other data frames with dplyr and other tools like we’ve done before. Both tracts_sf and acs inherit the data.frame object type, but tract_sf also has the sf object type. You always know a sf object by how it prints, starting with “Simple feature collection …”.\n\nclass(tracts_sf)\n\n[1] \"sf\"         \"data.frame\"\n\nclass(acs)\n\n[1] \"tbl_df\"     \"tbl\"        \"data.frame\"\n\nhead(tracts_sf)\n\n\n\n\n\ncounty\ngeoid\ngeometry\n\n\n\n\nBaltimore city\n24510160300\nPOLYGON ((-76.64612 39.3031…\n\n\nBaltimore city\n24510020100\nPOLYGON ((-76.59047 39.2916…\n\n\nBaltimore County\n24005402602\nPOLYGON ((-76.76732 39.3673…\n\n\nBaltimore city\n24510190300\nPOLYGON ((-76.64696 39.2881…\n\n\nBaltimore city\n24510271801\nPOLYGON ((-76.68336 39.3517…\n\n\nAnne Arundel County\n24003730403\nPOLYGON ((-76.62029 39.1510…\n\n\n\n\n\nhead(acs)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nlevel\ncounty\nname\ntotal_pop\nwhite\nblack\nlatino\nasian\nother_race\ndiversity_idx\nforeign_born\ntotal_hh\nhomeownership\ntotal_cost_burden\ntotal_severe_cost_burden\nowner_cost_burden\nowner_severe_cost_burden\nrenter_cost_burden\nrenter_severe_cost_burden\nno_vehicle_hh\nmedian_hh_income\nages25plus\nless_than_high_school\nhigh_school_grad\nsome_college_or_aa\nbachelors\ngrad_degree\npov_status_determined\npoverty\nlow_income\narea_sqmi\npop_density\n\n\n\n\nus\nNA\nUnited States\n331097593\n0.59\n0.12\n0.19\n0.06\n0.05\n0.7376547\n0.14\n125736353\n0.65\n0.29\n0.14\n0.21\n0.09\n0.45\n0.23\n0.08\n75149\n226600992\n0.11\n0.26\n0.28\n0.21\n0.13\n323275448\n0.13\n0.29\nNA\nNA\n\n\nmsa\nNA\nAbilene, TX Metro Area\n176656\n0.63\n0.07\n0.25\n0.02\n0.03\n0.6353998\n0.06\n65075\n0.63\n0.27\n0.11\n0.18\n0.05\n0.43\n0.20\n0.04\n61924\n111552\n0.11\n0.31\n0.32\n0.17\n0.09\n164028\n0.14\n0.35\n2743.514\n64.39042\n\n\nmsa\nNA\nAkron, OH Metro Area\n700578\n0.78\n0.12\n0.02\n0.03\n0.05\n0.4932053\n0.05\n292223\n0.68\n0.25\n0.12\n0.16\n0.06\n0.43\n0.22\n0.08\n68647\n487995\n0.07\n0.31\n0.28\n0.21\n0.13\n684199\n0.12\n0.28\n900.198\n778.24875\n\n\nmsa\nNA\nAlbany, GA Metro Area\n147716\n0.41\n0.52\n0.03\n0.01\n0.03\n0.5978239\n0.03\n57043\n0.57\n0.30\n0.16\n0.18\n0.09\n0.46\n0.27\n0.09\n53173\n97010\n0.14\n0.32\n0.31\n0.13\n0.10\n142224\n0.22\n0.41\n1591.030\n92.84302\n\n\nmsa\nNA\nAlbany-Lebanon, OR Metro Area\n128598\n0.83\n0.00\n0.10\n0.01\n0.06\n0.3838219\n0.04\n49944\n0.67\n0.31\n0.14\n0.23\n0.10\n0.45\n0.20\n0.04\n69523\n89890\n0.09\n0.29\n0.41\n0.13\n0.07\n126135\n0.12\n0.33\n2289.276\n56.17408\n\n\nmsa\nNA\nAlbany-Schenectady-Troy, NY Metro Area\n901729\n0.77\n0.07\n0.06\n0.05\n0.05\n0.5241427\n0.08\n372919\n0.64\n0.25\n0.12\n0.17\n0.07\n0.41\n0.22\n0.10\n83360\n627234\n0.07\n0.25\n0.28\n0.22\n0.18\n871777\n0.11\n0.23\n2811.804\n320.69412\n\n\n\n\n\n\nA data frame can have a geometry column without the data frame being an sf object. If I convert tracts_sf to a base data frame, it still has a geometry column (the type is shown as sfc_POLYGON, and it’s a list-column of polygons), but it’s no longer an sf object.\n\ntracts_df &lt;- as.data.frame(tracts_sf)\nhead(tracts_df)\n\n\n\n\n\ncounty\ngeoid\ngeometry\n\n\n\n\nBaltimore city\n24510160300\nPOLYGON ((-76.64612 39.3031…\n\n\nBaltimore city\n24510020100\nPOLYGON ((-76.59047 39.2916…\n\n\nBaltimore County\n24005402602\nPOLYGON ((-76.76732 39.3673…\n\n\nBaltimore city\n24510190300\nPOLYGON ((-76.64696 39.2881…\n\n\nBaltimore city\n24510271801\nPOLYGON ((-76.68336 39.3517…\n\n\nAnne Arundel County\n24003730403\nPOLYGON ((-76.62029 39.1510…\n\n\n\n\n\nclass(tracts_df)\n\n[1] \"data.frame\"\n\n\nWhy this matters to you is this gotcha: when you join an sf data frame with a regular one, the “left” data frame in the join takes priority in setting the object type.\n\nsf_on_left &lt;- inner_join(tracts_sf, acs, by = c(\"geoid\" = \"name\"))\nsf_on_right &lt;- inner_join(acs, tracts_sf, by = c(\"name\" = \"geoid\"))\n\nclass(sf_on_left)\n\n[1] \"sf\"         \"data.frame\"\n\nclass(sf_on_right)\n\n[1] \"tbl_df\"     \"tbl\"        \"data.frame\"\n\n\nRemember that nice plot method for sf objects? It only applies to sf objects, i.e. sf_on_left but not sf_on_right.\n\nplot(sf_on_left[, \"median_hh_income\"])\n\n\n\n\n\n\n\nplot(sf_on_right[, \"median_hh_income\"])\n\n\n\n\n\n\n\n\nSimilarly, ggplot2::geom_sf expects an sf object, not just a geometry column: 1\n1 There is a workaround but it also requires telling ggplot your data’s CRS, and why not just do it the expected way?\n# this runs fine\nggplot(sf_on_left) +\n  geom_sf()\n\n# this throws an error:\n# `stat_sf()` requires the following missing aesthetics: geometry\nggplot(sf_on_right) +\n  geom_sf(aes(geometry = geometry))\n\nYou’ll probably make this mistake anyway—I do it all the time—but now you have a heads up about it.\nYou can also just convert your data frame with an sf column back into a sf data frame.\n\nsf_once_again &lt;- sf_on_right |&gt;\n  sf::st_as_sf()\n\nclass(sf_once_again)\n\n[1] \"sf\"         \"tbl_df\"     \"tbl\"        \"data.frame\"\n\n\n\n\nJoining spatial and nonspatial data for real\nNow that we know what to watch out for, we can actually join our spatial and nonspatial data and make a choropleth. This is a type of thematic map (probably the most common type) where you shade areas in a map based on some additional value. These can be very useful but also easily misleading.\n\nall_tracts_sf |&gt;\n  left_join(acs, by = c(\"geoid\" = \"name\", \"county\")) |&gt;\n  ggplot(aes(fill = white)) +\n  geom_sf(color = \"white\") +\n  scale_fill_distiller(palette = \"YlGnBu\", direction = 1) +\n  labs(title = \"Percent white, Maryland tracts, 2022\")\n\n\n\n\n\n\n\n\nThere’s a minor problem with this map, but it’s a byproduct of the very different areas of tracts here, particularly in dense urban tracts: some of those values in Baltimore or PG County look very low. Is it really because their values are close to 0, or is some of it just the white borders around tracts?\n\nall_tracts_sf |&gt;\n  left_join(acs, by = c(\"geoid\" = \"name\", \"county\")) |&gt;\n  ggplot(aes(fill = white)) +\n  # set linewidth to adjust thickness of borders\n  geom_sf(color = \"white\", linewidth = 0) +\n  scale_fill_distiller(palette = \"YlGnBu\", direction = 1) +\n  labs(title = \"Percent white, Maryland tracts, 2022\")\n\n\n\n\n\n\n\n\nIt’s both! I’m endlessly amazed by how often I see this in published work. Sometimes the borders are useful, but here they weren’t really.\nNote also that these are percentages, not the number of white people per tract. With tracts, since they’re of standardized sizes, this wouldn’t be as big a deal as with other geographies, but if we showed populations in this way, we’d just be displaying population density. That’s because what you’re reading is a value presumably spread over an area. You pretty much always want a choropleth to be a value that’s been scaled by some population measure, or something not based on a raw count:\n\npercentage of population ages 65+\nmedian household income\npercentage of households without a car\npolling places per 1,000 registered voters\nGDP per capita\n\nIn most cases, a choropleth would not be appropriate for count of people ages 65+ or total GDP, etc.",
    "crumbs": [
      "Weekly notes",
      "13. Mapping data to space"
    ]
  },
  {
    "objectID": "weeks/13_data_to_space.html#exercises",
    "href": "weeks/13_data_to_space.html#exercises",
    "title": "13. Mapping data to space",
    "section": "Exercises",
    "text": "Exercises\nTime to make a choropleth! Join the ACS data to one of the tract shapefiles (justviz::tracts_sf for just the city and its neighboring counties, or all_tracts_sf defined here for the whole state). Find a different variable of interest and make a choropleth of it like the ones above.\nFilter for just Baltimore city or a single county (use filter(county == \"county name\")), and make another choropleth of the same variable. Compare what you can learn from looking at that choropleth versus looking at the county-wide value. In some places, the tracts will be very different from the county aggregate; in a more homogenous area, they might not be.",
    "crumbs": [
      "Weekly notes",
      "13. Mapping data to space"
    ]
  },
  {
    "objectID": "weeks/00_definitions.html",
    "href": "weeks/00_definitions.html",
    "title": "0. Definitions",
    "section": "",
    "text": "A few definitions of data visualization:\n\nThe rendering of information in a visual format to help communicate data while also generating new patterns and knowledge through the act of visualization itself (Du Bois et al., 2018, p. 8)\n\nDu Bois, W. E. B., Battle-Baptiste, W., & Rusert, B. (2018). W.E.B Du Bois’s data portraits: Visualizing Black America (First edition). The W.E.B. Du Bois Center At the University of Massachusetts Amherst ; Princeton Architectural Press.\n\n\nThe representation and presentation of data to facilitate understanding (Kirk, 2016, p. 19)\n\nKirk, A. (2016). Data visualisation: A handbook for data driven design. SAGE.\n\nIt’s pretty common to find a book with hundreds of pages of details on data visualization, but no definition.\nWhat else could we add here?",
    "crumbs": [
      "Weekly notes",
      "0. Definitions"
    ]
  },
  {
    "objectID": "weeks/00_definitions.html#what-is-data-visualization",
    "href": "weeks/00_definitions.html#what-is-data-visualization",
    "title": "0. Definitions",
    "section": "",
    "text": "A few definitions of data visualization:\n\nThe rendering of information in a visual format to help communicate data while also generating new patterns and knowledge through the act of visualization itself (Du Bois et al., 2018, p. 8)\n\nDu Bois, W. E. B., Battle-Baptiste, W., & Rusert, B. (2018). W.E.B Du Bois’s data portraits: Visualizing Black America (First edition). The W.E.B. Du Bois Center At the University of Massachusetts Amherst ; Princeton Architectural Press.\n\n\nThe representation and presentation of data to facilitate understanding (Kirk, 2016, p. 19)\n\nKirk, A. (2016). Data visualisation: A handbook for data driven design. SAGE.\n\nIt’s pretty common to find a book with hundreds of pages of details on data visualization, but no definition.\nWhat else could we add here?",
    "crumbs": [
      "Weekly notes",
      "0. Definitions"
    ]
  },
  {
    "objectID": "weeks/00_definitions.html#what-should-visualization-do",
    "href": "weeks/00_definitions.html#what-should-visualization-do",
    "title": "0. Definitions",
    "section": "What should visualization do?",
    "text": "What should visualization do?\n\nData visualization is part art and part science. The challenge is to get the art right without getting the science wrong and vice versa. A data visualization first and foremost has to accurately convey the data. It must not mislead or distort…. At the same time, a data visualization should be aesthetically pleasing (Wilke, 2019, ch 1)\n\nWilke, C. (2019). Fundamentals of data visualization: A primer on making informative and compelling figures (First edition). O’Reilly. https://clauswilke.com/dataviz/",
    "crumbs": [
      "Weekly notes",
      "0. Definitions"
    ]
  },
  {
    "objectID": "weeks/00_definitions.html#why-visualize-data",
    "href": "weeks/00_definitions.html#why-visualize-data",
    "title": "0. Definitions",
    "section": "Why visualize data?",
    "text": "Why visualize data?\n\nExplore\nExplain\nBoth\n\nWhat’s the difference, and what happens in the overlaps?",
    "crumbs": [
      "Weekly notes",
      "0. Definitions"
    ]
  },
  {
    "objectID": "weeks/00_definitions.html#how-is-data-visualization-used",
    "href": "weeks/00_definitions.html#how-is-data-visualization-used",
    "title": "0. Definitions",
    "section": "How is data visualization used?",
    "text": "How is data visualization used?\n\n\n\n\n\n\nBrainstorming\n\n\n\n\nPositive / constructive\n\nfacilitates & documents change\nhighlights social justice concerns\njustifying decisions\ntelling a story\nconnecting dots\ninform\nefficiency & safety\nelicit emotion\nconvey lots of information\nmake data more understandable\n\n\n\nNegative / destructive\n\nfacilitates & documents change in ways that are harmful\ntelling a story (misinformation)\nelicit emotion (sensationalize)",
    "crumbs": [
      "Weekly notes",
      "0. Definitions"
    ]
  },
  {
    "objectID": "weeks/11_project1.html",
    "href": "weeks/11_project1.html",
    "title": "11. Project 1 troubleshooting",
    "section": "",
    "text": "Here are some notes on our troubleshooting in class.\nlibrary(ggplot2)\nlibrary(justviz)\nlibrary(dplyr)",
    "crumbs": [
      "Weekly notes",
      "11. Project 1 troubleshooting"
    ]
  },
  {
    "objectID": "weeks/11_project1.html#changing-facet-labels",
    "href": "weeks/11_project1.html#changing-facet-labels",
    "title": "11. Project 1 troubleshooting",
    "section": "Changing facet labels",
    "text": "Changing facet labels\nThere are a few functions for relabeling facets directly; the syntax is a little unusual but see ggplot2::labeller. However, this is one of those things that I think is easier / safer to do yourself as part of prepping your data before visualizing it. You can do this by changing factor labels. If the variable you’re faceting by isn’t already a factor, convert it to one with either as.factor (which will arrange in alphabetical order) or forcats::as_factor (which will keep the order it’s in when you call the function). I greatly prefer the forcats functions—they make it easy to do a variety of relabeling and reordering tasks.\nWithout making any changes, the levels of the indicator variable are:\n\nIn snake case (snake_case vs camelCase vs kebab-case vs Title Case vs Sentence case)\nIn an arbitrary order (i.e. order doesn’t tell us anything about the data)\nA few of the abbreviated labels are unclear\n\n\nej_natl |&gt;\n  filter(!is.na(d2_ptile)) |&gt;\n  ggplot(aes(x = d2_ptile)) +\n  geom_histogram() +\n  facet_wrap(vars(indicator))\n\n\n\n\n\n\n\n\nSome revisions (in practice I’d do all these mutate functions in one call, but by keeping them in separate lines you can go through and see individual changes):\n\nej_natl |&gt;\n  filter(!is.na(d2_ptile)) |&gt;\n  # change to sentence case\n  mutate(indicator = forcats::fct_relabel(indicator, snakecase::to_sentence_case)) |&gt;\n  # manually adjust a few labels\n  mutate(indicator = forcats::fct_recode(indicator,\n                                         \"PM-25\" = \"Pm 25\",\n                                         \"Air-related cancers\" = \"Air cancer\",\n                                         \"Air-related respiratory\" = \"Air respiratory\",\n                                         \"Superfund sites\" = \"Superfund\",\n                                         \"Hazardous waste treatment\" = \"Haz waste\",\n                                         \"Underground storage\" = \"Undergrnd storage\")) |&gt;\n  # reorder based on median per indicator\n  mutate(indicator = forcats::fct_reorder(indicator, d2_ptile, .fun = median)) |&gt;\n  ggplot(aes(x = d2_ptile)) +\n  geom_histogram(binwidth = 5) +\n  facet_wrap(vars(indicator))",
    "crumbs": [
      "Weekly notes",
      "11. Project 1 troubleshooting"
    ]
  },
  {
    "objectID": "weeks/11_project1.html#multiple-distributions",
    "href": "weeks/11_project1.html#multiple-distributions",
    "title": "11. Project 1 troubleshooting",
    "section": "Multiple distributions",
    "text": "Multiple distributions\nFor just a few groups, layering distributions as either histograms or density curves are another option, and less technical than boxplots.\n\nwaste_vs_superfund &lt;- ej_natl |&gt;\n  filter(indicator %in% c(\"wastewater\", \"superfund\"),\n         !is.na(d2_ptile))\n\n# use position identity to keep them from stacking\nggplot(waste_vs_superfund, aes(x = d2_ptile, fill = indicator)) +\n  geom_histogram(alpha = 0.4, position = position_identity(), binwidth = 5)\n\n\n\n\n\n\n\nggplot(waste_vs_superfund, aes(x = d2_ptile, color = indicator)) +\n  geom_density()",
    "crumbs": [
      "Weekly notes",
      "11. Project 1 troubleshooting"
    ]
  },
  {
    "objectID": "weeks/11_project1.html#correlation-matrix",
    "href": "weeks/11_project1.html#correlation-matrix",
    "title": "11. Project 1 troubleshooting",
    "section": "Correlation matrix",
    "text": "Correlation matrix\n\nacs_to_corr &lt;- acs |&gt;\n  filter(level == \"tract\") |&gt;\n  select(name, white, diversity_idx, homeownership, total_cost_burden, pop_density, median_hh_income) |&gt;\n  # drop any rows with NA; otherwise set `use` argument in `cor`\n  na.omit()\n\n# drop names--only want numeric columns\nacs_mtx &lt;- acs_to_corr |&gt;\n  select(-name) |&gt;\n  cor() |&gt;\n  # need base data.frame, not tibble, to keep row names\n  as.data.frame() |&gt;\n  tibble::rownames_to_column(var = \"var1\") |&gt;\n  # pivot to get 2 columns of variable names, 1 column of correlation coefficients\n  tidyr::pivot_longer(-var1, names_to = \"var2\", values_to = \"coef\") |&gt;\n  # make both vars factors, order by coefficients\n  mutate(across(var1:var2, forcats::as_factor)) |&gt;\n  mutate(across(var1:var2, function(x) forcats::fct_reorder(x, coef, .fun = median)))\n\nhead(acs_mtx)\n\n\n\n\n\nvar1\nvar2\ncoef\n\n\n\n\nwhite\nwhite\n1.0000000\n\n\nwhite\ndiversity_idx\n-0.2503525\n\n\nwhite\nhomeownership\n0.4539800\n\n\nwhite\ntotal_cost_burden\n-0.5478637\n\n\nwhite\npop_density\n-0.4011644\n\n\nwhite\nmedian_hh_income\n0.3198306\n\n\n\n\n\n# needs a diverging palette, but it's off center so I'll build it manually\ndiv_pal &lt;- RColorBrewer::brewer.pal(n = 7, name = \"BrBG\")\n\n# reverse one var so it starts in top left corner\nggplot(acs_mtx, aes(x = var1, y = forcats::fct_rev(var2), fill = coef)) +\n  geom_tile() +\n  # this doesn't work well since it's skewed\n  # scale_fill_distiller(palette = \"BrBG\")\n  # instead get first, last, middle values of diverging palette manually\n  scale_fill_gradient2(low = div_pal[1], high = div_pal[7], mid = div_pal[4],\n                       midpoint = 0)\n\n\n\n\n\n\n\n# alternatively cut coefficient values into bins\n# problem with this is it's tricky to align the palette\ndiv_pal2 &lt;- RColorBrewer::brewer.pal(10, \"BrBG\")\nacs_mtx |&gt;\n  mutate(coef_brk = cut(coef, breaks = seq(-1, 1, by = 0.25))) |&gt;\n  ggplot(aes(x = var1, y = forcats::fct_rev(var2), fill = coef_brk)) +\n  geom_tile() +\n  # set drop = FALSE in case some bins don't have observations\n  # subset palette to make sure it lines up--\n  # want lightest brown on -0.25-0, lightest green on 0-0.25\n  scale_fill_manual(values = div_pal2[2:10], drop = FALSE)\n\n\n\n\n\n\n\n\nIt’s also worth considering dropping the diagonals, since they skew the distribution of coefficients and don’t really add anything (you don’t expect the diagonals of a coefficient matrix to be anything other than 1) but I’ll leave that to you.",
    "crumbs": [
      "Weekly notes",
      "11. Project 1 troubleshooting"
    ]
  },
  {
    "objectID": "weeks/11_project1.html#heatmap",
    "href": "weeks/11_project1.html#heatmap",
    "title": "11. Project 1 troubleshooting",
    "section": "Heatmap",
    "text": "Heatmap\nAn alternative to cramming lots of dimensions into a scatterplot—however, this only really works when 1) the variables are all on the same scale (all percentages, all dollar amounts on the same order of magnitude, etc), and 2) you don’t have a ton of observations.\n\nacs |&gt;\n  filter(level == \"county\") |&gt;\n  select(name, less_than_high_school:grad_degree) |&gt;\n  # reshape to put variable on the x axis\n  tidyr::pivot_longer(-name, names_to = \"edu\") |&gt;\n  # keep same order since it's meaningful (less than HS through grad degree)\n  mutate(edu = forcats::as_factor(edu)) |&gt;\n  # reorder county based on increasing education levels AND value\n  # so counties are ordered by decreasing share of adults w/o high school diploma\n  # very annoying function to work with tho\n  mutate(name = forcats::as_factor(name) |&gt;\n           forcats::fct_reorder2(desc(edu), value, .desc = FALSE)) |&gt;\n  ggplot(aes(x = edu, y = name, fill = value)) +\n  geom_tile() +\n  scale_fill_distiller(palette = \"YlGnBu\", direction = 1)",
    "crumbs": [
      "Weekly notes",
      "11. Project 1 troubleshooting"
    ]
  },
  {
    "objectID": "weeks/11_project1.html#groups-of-scatterplots",
    "href": "weeks/11_project1.html#groups-of-scatterplots",
    "title": "11. Project 1 troubleshooting",
    "section": "Groups of scatterplots",
    "text": "Groups of scatterplots\nCompare a few of those acs variables vs median household income—gotta do a weird reshaping to make that happen. In class we did this with EJ data vs poverty from ACS.\n\nacs_to_corr |&gt;\n  select(name, median_hh_income, white, diversity_idx, pop_density) |&gt;\n  tidyr::pivot_longer(white:pop_density, names_to = \"var\") |&gt;\n  ggplot(aes(x = value, y = median_hh_income)) +\n  geom_point(alpha = 0.5, shape = \"circle open\") +\n  # these scales are very different---white is a percentage, \n  # diversity index isn't a percentage but scales 0 to 1\n  # pop density has range 0 to infinity\n  facet_wrap(vars(var), scales = \"free_x\") \n\n\n\n\n\n\n\n  # labeling axis ticks would be a pain tho because of the different scales\n  # ggh4x package has some fixes for this",
    "crumbs": [
      "Weekly notes",
      "11. Project 1 troubleshooting"
    ]
  },
  {
    "objectID": "weeks/11_project1.html#splitting-by-region",
    "href": "weeks/11_project1.html#splitting-by-region",
    "title": "11. Project 1 troubleshooting",
    "section": "Splitting by region",
    "text": "Splitting by region\nCopying Angela’s division of counties into regions of the state, but putting it into a data frame to join to ACS data.\n\ncentral_md &lt;- c(\"Anne Arundel County\", \"Baltimore County\", \"Harford County\", \"Howard County\")\neastern_shore &lt;- c(\"Caroline County\", \"Cecil County\", \"Dorchester County\", \"Kent County\", \"Queen Anne's County\", \"Somerset County\", \"Talbot County\", \"Wicomico County\", \"Worcester County\")\nsouthern_md &lt;- c(\"Calvert County\", \"Charles County\", \"Montgomery County\", \"Prince George's County\", \"St. Mary's County\")\nwestern_md &lt;- c(\"Allegany County\", \"Carroll County\", \"Frederick County\", \"Garrett County\", \"Washington County\")\nbaltimore_city &lt;- c(\"Baltimore city\")\n\n# lst creates names for the list from variable names\ncounty_regions &lt;- tibble::lst(central_md, eastern_shore, southern_md, western_md, baltimore_city) |&gt;\n  # then pack it into a data frame\n  tibble::enframe(name = \"region\", value = \"county\") |&gt;\n  # then unnest it---names start as a list-column\n  tidyr::unnest(county) |&gt;\n  # change from snake case to title, replace Md with MD\n  mutate(region = snakecase::to_title_case(region) |&gt;\n           stringr::str_replace(\"Md\", \"MD\"))\n\nhead(county_regions)\n\n\n\n\n\nregion\ncounty\n\n\n\n\nCentral MD\nAnne Arundel County\n\n\nCentral MD\nBaltimore County\n\n\nCentral MD\nHarford County\n\n\nCentral MD\nHoward County\n\n\nEastern Shore\nCaroline County\n\n\nEastern Shore\nCecil County\n\n\n\n\n\nacs |&gt;\n  filter(level == \"tract\") |&gt;\n  # use left join to catch if we missed any counties\n  left_join(county_regions, by = \"county\") |&gt;\n  # no NAs in region column so all good\n  ggplot(aes(x = grad_degree, y = low_income)) +\n  # 2D binning is a way to deal with overplotting---I don't love it\n  geom_bin2d() +\n  scale_fill_distiller(palette = \"YlGnBu\", direction = 1) +\n  facet_wrap(vars(region)) \n\n\n\n\n\n\n\n\nTo join to EJ data or anything else tract-based, you need to know which county each tract is in. You can chop up the FIPS codes and join them to a lookup table (e.g. tidycensus::fips_codes), but we already have tracts & counties available in the ACS data to use as a lookup table.\n\ntract_to_county &lt;- acs |&gt;\n  filter(level == \"tract\") |&gt;\n  select(tract = name, county)\n\nhead(tract_to_county)\n\n\n\n\n\ntract\ncounty\n\n\n\n\n24001000100\nAllegany County\n\n\n24001000200\nAllegany County\n\n\n24001000500\nAllegany County\n\n\n24001000600\nAllegany County\n\n\n24001000700\nAllegany County\n\n\n24001000800\nAllegany County\n\n\n\n\n\nwaste_vs_superfund |&gt;\n  # join to tract_to_county to get county names\n  inner_join(tract_to_county, by = \"tract\") |&gt;\n  # then join to regions table\n  left_join(county_regions, by = \"county\") |&gt;\n  # reshape to do a scatterplot\n  tidyr::pivot_wider(id_cols = c(tract, county, region), \n                     names_from = indicator, values_from = d2_ptile) |&gt;\n  ggplot(aes(x = superfund, y = wastewater)) +\n  geom_point(alpha = 0.4) +\n  facet_wrap(vars(region))",
    "crumbs": [
      "Weekly notes",
      "11. Project 1 troubleshooting"
    ]
  },
  {
    "objectID": "weeks/11_project1.html#direct-labels",
    "href": "weeks/11_project1.html#direct-labels",
    "title": "11. Project 1 troubleshooting",
    "section": "Direct labels",
    "text": "Direct labels\nWe’ll write some functions to do more complex direct labeling later in the semester but for simple cases (either single set of bars or faceted bars but all on similar scales) you can just bump values by a small offset.\n\nbalt_metro &lt;- readRDS(here::here(\"utils/balt_metro.rds\"))\n\n# define a function to make percentage labels\npercent_100 &lt;- function(x) {\n  scales::label_percent(accuracy = 1)(x)\n}\n\nmetro_edu &lt;- acs |&gt;\n  select(name, less_than_high_school:grad_degree) |&gt;\n  filter(name %in% balt_metro) |&gt;\n  mutate(name = forcats::as_factor(name) |&gt;\n           forcats::fct_reorder(less_than_high_school))\n\nggplot(metro_edu, aes(x = name, y = less_than_high_school)) +\n  geom_col() +\n  # adjust vertical justification to align top of text at value\n  # since numbers are small, only offset by half a percent\n  geom_text(aes(label = percent_100(less_than_high_school)), color = \"white\",\n            vjust = 1, nudge_y = -0.005) +\n  # since values are labeled, axis labels are redundant\n  scale_y_continuous(breaks = NULL)\n\n\n\n\n\n\n\n# similar but with coords flipped\n# smaller offset since the gap looks bigger on horizontal bars (trial & error, friends)\nggplot(metro_edu, aes(x = name, y = less_than_high_school)) +\n  geom_col() +\n  # switch vjust to hjust\n  geom_text(aes(label = percent_100(less_than_high_school)), color = \"white\",\n            hjust = 1, nudge_y = -0.0025) +\n  scale_y_continuous(breaks = NULL) +\n  coord_flip()\n\n\n\n\n\n\n\n\nWith stacked bars, align labels in centers of bars. It gets a little tricky:\n\nmetro_edu_long &lt;- metro_edu |&gt;\n  # reshape to have edu all in one column\n  tidyr::pivot_longer(-name, names_to = \"edu\") |&gt;\n  # keep factor order\n  mutate(edu = forcats::as_factor(edu) |&gt;\n           forcats::fct_recode(some_college_or_associates = \"some_college_or_aa\") |&gt;\n           forcats::fct_relabel(snakecase::to_sentence_case)) \n\n# reverse edu so less than HS is on bottom\nmetro_edu_long |&gt;\n  mutate(edu = forcats::fct_rev(edu)) |&gt;\n  ggplot(aes(x = name, y = value, fill = edu)) +\n  # set to position fill in case rounding made them not add to exactly 100%\n  geom_col(position = position_fill(), width = 0.8) +\n  # set same position for labels but align to center of bars\n  geom_text(aes(label = percent_100(value)), position = position_fill(vjust = 0.5)) +\n  scale_fill_brewer(palette = \"Set2\") +\n  scale_y_continuous(breaks = NULL) +\n  # wrap long labels\n  scale_x_discrete(labels = scales::label_wrap(width = 12))\n\n\n\n\n\n\n\n\nAn example with my stylehaven::offset_lbls helper function for more complicated situations. In this first one, all labels are offset by the same amount, but the scales are different (less than HS only goes up to 13, others are larger) so the gaps between labels and ends of bars is different in each facet.\n\n# if not already installed\n# devtools::install_github(\"CT-Data-Haven/stylehaven\")\n\nmetro_edu_long |&gt;\n  ggplot(aes(x = name, y = value)) +\n  geom_col(width = 0.8) +\n  geom_text(aes(label = percent_100(value)), hjust = 1, color = \"white\", nudge_y = -0.02) +\n  scale_x_discrete(labels = scales::label_wrap(width = 12)) +\n  # give a little extra space at upper limit\n  scale_y_continuous(expand = expansion(add = c(0, 0.015)), breaks = NULL) +\n  coord_flip() +\n  # let scale vary\n  facet_grid(cols = vars(edu), scales = \"free\", \n             labeller = labeller(.cols = label_wrap_gen(width = 15)))\n\n\n\n\n\n\n\n\n\n# check arguments in ?stylehaven::offset_lbls--use it by group\n# can supply a function for labeling, then use that in geom_text\n# creates a y column to use for label placement\n# other columns help with offsetting labels if some might be cutoff---see examples in docs\nedu_with_lbls &lt;- metro_edu_long |&gt;\n  group_by(edu) |&gt;\n  stylehaven::offset_lbls(value = value, frac = 0.075, fun = percent_100) \n\nhead(edu_with_lbls)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nname\nedu\nvalue\nis_small\noff\ny\njust\nlbl\n\n\n\n\nAnne Arundel County\nLess than high school\n0.06\nFALSE\n-0.00975\n0.05025\n1\n6%\n\n\nAnne Arundel County\nHigh school grad\n0.22\nFALSE\n-0.02175\n0.19825\n1\n22%\n\n\nAnne Arundel County\nSome college or associates\n0.27\nFALSE\n-0.02175\n0.24825\n1\n27%\n\n\nAnne Arundel County\nBachelors\n0.25\nFALSE\n-0.02325\n0.22675\n1\n25%\n\n\nAnne Arundel County\nGrad degree\n0.19\nFALSE\n-0.02475\n0.16525\n1\n19%\n\n\nBaltimore County\nLess than high school\n0.08\nFALSE\n-0.00975\n0.07025\n1\n8%\n\n\n\n\n\nedu_with_lbls |&gt;\n  ggplot(aes(x = name, y = value)) +\n  geom_col(width = 0.8) +\n  geom_text(aes(y = y, label = lbl), hjust = 1, color = \"white\") +\n  scale_x_discrete(labels = scales::label_wrap(width = 12)) +\n  # give a little extra space at upper limit\n  scale_y_continuous(expand = expansion(add = c(0, 0.015)), breaks = NULL) +\n  coord_flip() +\n  # let scale vary for the sake of illustration\n  facet_grid(cols = vars(edu), scales = \"free\", \n             labeller = labeller(.cols = label_wrap_gen(width = 15)))",
    "crumbs": [
      "Weekly notes",
      "11. Project 1 troubleshooting"
    ]
  },
  {
    "objectID": "weeks/02_components.html",
    "href": "weeks/02_components.html",
    "title": "2. Components of a chart",
    "section": "",
    "text": "Revisiting the wage gaps to break down a chart into its pieces and what they mean. This will be a subset of the wages data with just full-time workers by sex and education in Maryland.\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(justviz)\nlibrary(ggtext)\n\nsource(here::here(\"utils/plotting_utils.R\"))\ngender_pal &lt;- setNames(qual_pal[c(3, 6)], c(\"Men\", \"Women\"))\nsex_x_edu &lt;- wages |&gt;\n  filter(dimension == \"sex_x_edu\",\n         name == \"Maryland\") |&gt;\n  select(sex, edu, earn_q25, earn_q50, earn_q75) |&gt;\n  mutate(across(where(is.factor), forcats::fct_drop))\n\nknitr::kable(sex_x_edu)\n\n\n\n\nsex\nedu\nearn_q25\nearn_q50\nearn_q75\n\n\n\n\nMen\nHigh school or less\n33158\n49737\n70000\n\n\nMen\nSome college or AA\n43105\n63586\n93000\n\n\nMen\nBachelors\n60000\n91712\n135000\n\n\nMen\nGraduate degree\n82661\n121873\n171555\n\n\nWomen\nHigh school or less\n26974\n38000\n55475\n\n\nWomen\nSome college or AA\n35000\n50241\n75000\n\n\nWomen\nBachelors\n49737\n71842\n101684\n\n\nWomen\nGraduate degree\n65817\n92842\n129475\n\n\n\n\nsummary(sex_x_edu)\n\n    sex                     edu       earn_q25        earn_q50     \n Men  :4   High school or less:2   Min.   :26974   Min.   : 38000  \n Women:4   Some college or AA :2   1st Qu.:34540   1st Qu.: 50115  \n           Bachelors          :2   Median :46421   Median : 67714  \n           Graduate degree    :2   Mean   :49556   Mean   : 72479  \n                                   3rd Qu.:61454   3rd Qu.: 91994  \n                                   Max.   :82661   Max.   :121873  \n    earn_q75     \n Min.   : 55475  \n 1st Qu.: 73750  \n Median : 97342  \n Mean   :103899  \n 3rd Qu.:130856  \n Max.   :171555",
    "crumbs": [
      "Weekly notes",
      "2. Components of a chart"
    ]
  },
  {
    "objectID": "weeks/02_components.html#starting-point",
    "href": "weeks/02_components.html#starting-point",
    "title": "2. Components of a chart",
    "section": "Starting point",
    "text": "Starting point\nThis is the decent but not great chart from last week. We’re going to take a step back to break it into its components.\n\nwages |&gt;\n  filter(dimension == \"sex_x_edu\",\n         name == \"Maryland\") |&gt;\n  ggplot(aes(x = edu, y = earn_q50, fill = sex)) +\n  geom_col(width = 0.8, alpha = 0.9, position = position_dodge2()) +\n  scale_y_barcontinuous(labels = dollar_k) +\n  scale_fill_manual(values = gender_pal) +\n  labs(x = NULL, y = NULL, fill = NULL,\n       title = \"Median individual earnings\",\n       subtitle = \"Adults ages 25+ with positive earnings by sex and educational attainment, Maryland full-time workers, 2021\") +\n  theme(plot.subtitle = element_textbox_simple(margin = margin(0.2, 0, 0.2, 0, \"lines\")),\n        legend.position = \"bottom\")\n\n\n\n\n\n\n\nFigure 1",
    "crumbs": [
      "Weekly notes",
      "2. Components of a chart"
    ]
  },
  {
    "objectID": "weeks/02_components.html#basics",
    "href": "weeks/02_components.html#basics",
    "title": "2. Components of a chart",
    "section": "Basics",
    "text": "Basics\nFocusing first on median wages (earn_q50), values here range from 38,000 to 121,873, so we should expect our dependent axis (usually y, but we might change it) to range from somewhere below that to somewhere above that. If we make a chart and it goes down to e.g. 10,000 that’s a sign that something weird might be happening. On the dependent axis, we have 2 categories of sex :-/ and 4 of education; if we end up with only 3 bars, or with 15 bars, something’s wrong.\n\nggplot(sex_x_edu, aes(x = edu, y = earn_q50))\n\n\n\n\n\n\n\nFigure 2\n\n\n\n\n\nThese scales make sense so far—I haven’t signaled that sex will be included here, or that we’re making a bar chart which is why the dependent axis doesn’t have to go down to 0.\n\nggplot(sex_x_edu, aes(x = edu, y = earn_q50)) +\n  geom_col()\n\n\n\n\n\n\n\nFigure 3\n\n\n\n\n\nThe dependent scale has changed: it goes down to 0, which makes sense because now we have bars, but it goes up to 200,000, which is weird.\n\nggplot(sex_x_edu, aes(x = edu, y = earn_q50)) +\n  geom_col(color = \"white\")\n\n\n\n\n\n\n\nFigure 4\n\n\n\n\n\nThis still includes both men and women, but sex isn’t assigned to any aesthetic, so bars just get stacked. Setting the fill makes that clear.\n\nggplot(sex_x_edu, aes(x = edu, y = earn_q50, fill = sex)) +\n  geom_col()\n\n\n\n\n\n\n\nFigure 5\n\n\n\n\n\nThese bars shouldn’t be stacked, though. Why not?\nThey represent median wages of distinct groups, not something that is cumulative. If men have a median income of $60,000 and women have a median income of $50,000, does that mean that men and women overall have a median income of $110,000? No! But that’s what these stacked bars imply.\n\nggplot(sex_x_edu, aes(x = edu, y = earn_q50, fill = sex)) +\n  geom_col(position = position_dodge2()) +\n  labs(title = \"Median earnings by sex & education, Maryland, 2021\")\n\n\n\n\n\n\n\nFigure 6\n\n\n\n\n\nSo now we have a chart that represents the data appropriately. We can make it look nicer, but for now we have all the basic components set.\nWhat are all the components here?\n\n\n\n\n\n\nBrainstorming components\n\n\n\n\naxes (x & y)\ntick values (dollar amounts, education levels)–horizontal\nlegend (placement, title, labels, keys)\naxis titles\nbackground\ngridlines (x & y gridlines, x-axis major, y-axis major & minor)\ntitle\nbars with color\ntick marks\nunits (not included)\ntext choices (font, size, boldness)",
    "crumbs": [
      "Weekly notes",
      "2. Components of a chart"
    ]
  },
  {
    "objectID": "weeks/02_components.html#a-nicer-chart",
    "href": "weeks/02_components.html#a-nicer-chart",
    "title": "2. Components of a chart",
    "section": "A nicer chart",
    "text": "A nicer chart\nThat chart is fine but not great. Next we could clean up the axes, their labels, ticks, and gridlines. For each of these components, you should ask yourself if they’re necessary, or what they add to the chart that isn’t already provided through some other means. This helps you maximize your data-to-ink ratio, Wilke (2019)\n\nLive code: clean up this chart\n\ngg &lt;- ggplot(sex_x_edu, aes(x = edu, y = earn_q50, fill = sex)) +\n  geom_col(position = position_dodge2())\n\ngg +\n  scale_y_continuous(labels = dollar_k) +\n  theme(panel.grid.major.x = element_blank()) +\n  theme(panel.grid.minor.y = element_blank()) +\n  theme(axis.ticks = element_blank()) +\n  labs(title = \"Median individual earnings\",\n       subtitle = \"Adults ages 25+ working full time by sex and educational attainment, Maryland, 2021\",\n       y = \"Median earnings\", x = NULL, fill = NULL) +\n  scale_fill_manual(values = gender_pal)\n\n\n\n\n\n\n\n\n\n\nGoal: one option\nThis is one more complicated option of how I might do this. It uses a function from the package stylehaven which I wrote for work, and which you all are free to use. It also uses showtext to set the fonts, which can be very finicky.\n\n# can't get fonts to not be totally weird\nlibrary(showtext)\nshowtext_auto()\nshowtext_opts(dpi = 300)\nsysfonts::font_add_google(\"Barlow Semi Condensed\")\n\n# use both true/false and gender palettes\ncomb_pal &lt;- c(purrr::map_chr(gender_pal, colorspace::darken, amount = 0.2, space = \"HCL\"), tf_pal)\n\nsex_x_edu |&gt;\n  mutate(edu = forcats::fct_recode(edu, \"Some college / Associate's\" = \"Some college or AA\", \"Bachelor's\" = \"Bachelors\")) |&gt;\n  stylehaven::offset_lbls(value = earn_q50, fun = dollar_k, frac = 0.03) |&gt;\n  ggplot(aes(x = edu, y = earn_q50, fill = sex, group = sex)) +\n  geom_col(width = 0.8, position = position_dodge2()) +\n  geom_text(aes(y = y, label = lbl, vjust = just, color = is_small),\n            family = \"Barlow Semi Condensed\", fontface = \"bold\", size = 9.5,\n            position = position_dodge2(width = 0.8)) +\n  geom_text(aes(label = sex, color = sex, x = as.numeric(edu) - 0.18, y = earn_q50 - off/2),\n            data = ~filter(., edu == first(edu)), vjust = 0, hjust = 0,\n            family = \"Barlow Semi Condensed\", fontface = \"bold\", size = 8,\n            position = position_dodge2(width = 0.8)) +\n  scale_fill_manual(values = gender_pal) +\n  scale_color_manual(values = comb_pal) +\n  scale_x_discrete(labels = scales::label_wrap(15)) +\n  scale_y_barcontinuous(breaks = NULL) +\n  theme_minimal(base_family = \"Barlow Semi Condensed\", base_size = 28) +\n  theme(text = element_text(lineheight = 0.5)) +\n  theme(panel.grid = element_blank()) +\n  theme(legend.position = \"none\") +\n  theme(axis.text = element_text(color = \"black\", size = rel(0.9))) +\n  theme(plot.title = element_text(family = \"Barlow Semi Condensed\", face = \"bold\")) +\n  theme(plot.subtitle = ggtext::element_textbox_simple(family = \"Barlow Semi Condensed\", lineheight = 0.7)) +\n  theme(plot.caption = element_text(color = \"gray30\")) +\n  labs(x = NULL, y = NULL,\n       title = \"The male-female wage gap persists across education levels\",\n       subtitle = \"Median individual earnings, Maryland adults ages 25+ working full time by sex and educational attainment, 2021\",\n       caption = \"Source: Analysis of US Census Bureau American Community Survey, 2021 5-year estimates\")",
    "crumbs": [
      "Weekly notes",
      "2. Components of a chart"
    ]
  },
  {
    "objectID": "weeks/09_decisions.html",
    "href": "weeks/09_decisions.html",
    "title": "9. Making responsible decisions",
    "section": "",
    "text": "library(dplyr)\nlibrary(ggplot2)\nlibrary(justviz)\n\nsource(here::here(\"utils/plotting_utils.R\"))\nupdate_geom_defaults(\"col\", list(fill = qual_pal[3]))\ntheme_set(theme_nice())",
    "crumbs": [
      "Weekly notes",
      "9. Making responsible decisions"
    ]
  },
  {
    "objectID": "weeks/09_decisions.html#warm-up",
    "href": "weeks/09_decisions.html#warm-up",
    "title": "9. Making responsible decisions",
    "section": "Warm up",
    "text": "Warm up\n\nYou want to know how UMBC graduate students feel about their job prospects, and how this might differ between students in STEM programs and students in social science programs (you’re not interested in other degrees), so you’re tabling on campus with a survey. The only actual survey question is “Do you feel good about your job prospects after graduation?” Draw a flowchart of the questions you might ask people before you get to the one survey question.\nThere’s a virus circulating that has killed many people, but a vaccine is available and you trust reports that it greatly decreases the chances of dying from the disease. After about a year of a massive vaccination campaign, you find out that the majority of people dying from the disease at the hospital near you were already vaccinated. Does this change your beliefs about the vaccine’s effectiveness? What other information might help explain this?\n\n\n\n\n\n\n\nBrainstorm\n\n\n\n\nhealth of people who are dying before getting sick (comorbidities, etc)\nhow many people already vaccinated",
    "crumbs": [
      "Weekly notes",
      "9. Making responsible decisions"
    ]
  },
  {
    "objectID": "weeks/09_decisions.html#representing-data",
    "href": "weeks/09_decisions.html#representing-data",
    "title": "9. Making responsible decisions",
    "section": "Representing data",
    "text": "Representing data\nSome of the ways we’ve talked about data visualization being misleading are intentional and malicious. That definitely happens, and how often you run into it might depend on your sources of information (Fox News, boardroom presentations, Congress, social media influencers…) but more often it’s just lack of skill and fluency.\n\n\n\n\n\nDeceptive coffee creamer\n\n\n\nWho’s in the data\nOne of the easiest things to mess up is the universe of your data. This is basically your denominator—who or what is included and used as the unit of analysis. I’ve most often found (and made, and corrected) this type of mistake with survey data, because it can be hard to know exactly who’s being asked every question.\nAn easy way to catch this is to read the fine print on your data sources, and to do it routinely because it might change. Some examples:\n\nBirth outcomes: for some measures, unit might be babies; for others, parent giving birth\nACS tables: several tables seem like they match, but one is by household and another is by person. Be especially mindful with tables related to children and family composition—these get messy. 1\nProxies: when I analyzed data on police stops, I tried to figure out a population to compare to. I didn’t have data on how many people in each census tract had a driver’s license, decennial data wasn’t out yet so I didn’t have reliable local counts of population 16 and up by race, so I just used population. It wasn’t ideal.\nRelationships: is a question being asked of parents, or of adults with a child in their household? These aren’t necessarily the same.\n\n1 This one is especially brain-melting: Ratio of Income to Poverty Level in the Past 12 Months by Nativity of Children Under 18 Years in Families and Subfamilies by Living Arrangements and Nativity of Parents. The universe is own children under 18 years in families and subfamilies for whom poverty status is determined.Another example: how would you make sense of this?\n\n\n\nShare of adults reporting having been unfairly stopped by police, Connecticut, 2021\n\n\n\n\n\n\n\n\n\nname\ncategory\ngroup\never_unfairly_stopped\nmultiple_times_3yr\n\n\n\n\nConnecticut\nTotal\nTotal\n15%\n29%\n\n\nConnecticut\nRace/Ethnicity\nWhite\n12%\n16%\n\n\nConnecticut\nRace/Ethnicity\nBlack\n25%\n40%\n\n\nConnecticut\nRace/Ethnicity\nLatino\n20%\n50%\n\n\n\n\n\n\n\nObscuring data\nWe’ve talked some about dealing with missing data, and often the solution to data-related problems is to get more of it. But sometimes it’s important to not be counted, or to not show everything. There are even times when it might be good to intentionally mess up the data (maybe this isn’t the role of the visualizer, however). 2 I would argue that hiding data when necessary should also be part of doing data analysis and viz responsibly. Some examples:\n2 The Census Bureau made the controversial decision to basically do this, via differential privacy. Wang (2021)\nWang, H. L. (2021). For The U.S. Census, Keeping Your Data Anonymous And Useful Is A Tricky Balance. NPR. https://www.npr.org/2021/05/19/993247101/for-the-u-s-census-keeping-your-data-anonymous-and-useful-is-a-tricky-balance\n\nFilling period tracking apps with fake data after Roe v Wade was overturned\nNot adding citizenship to the census or other surveys; not asking about sexual orientation and gender identity. In theory these should both be fine, but in practice they may not be safe for people to disclose, or they could get misused.\nLeaving out parts of your data that could be stigmatizing or lead to misinformation\n\nAn example of this last point:\n\nMy organization’s survey asked a similar set of questions, but we chose not to release the question about getting COVID from the vaccine. The others are valid concerns; that one is misinformation that we didn’t want to repeat even with qualifiers.\n\n\nLack of a pattern\nSometimes the pattern you expect to find in a dataset isn’t there, and that’s okay. You want to go into your work with an open mind, rather than force the data into the story you want it to tell. I’m really into situations where the pattern you think you’re going to find isn’t there, and that’s the story—it might point to a disruption in the usual pattern.\n\n\nSay what you mean\n\nDon’t say “people of color” when you actually mean “Black people” or “Black and Latino people” or something else. This drives me crazy, and I’m sure I’ve done it as well. Sometimes because of small sample sizes or other limitations, you can’t break your data down further than white vs people of color. But if you can disaggregate further, do so, at least in the EDA process. This especially goes for data that deals with something that historically targeted e.g. Black people or indigenous people or some other group.\nAlong those same lines, don’t say BIPOC (Black, Indigenous, and people of color) if you don’t actually have any data to show on indigenous people, or LGBT if you have no data on trans people.",
    "crumbs": [
      "Weekly notes",
      "9. Making responsible decisions"
    ]
  },
  {
    "objectID": "weeks/09_decisions.html#exercise",
    "href": "weeks/09_decisions.html#exercise",
    "title": "9. Making responsible decisions",
    "section": "Exercise",
    "text": "Exercise\nThe youth_risks dataset in the justviz package has a set of questions from the DataHaven Community Wellbeing Survey, where survey respondents are asked to rate the likelihood of young people in their area experiencing different events (DataHaven (n.d.)). The allowed responses are “almost certain,” “very likely,” “a toss up,” “not very likely,” and “not at all likely”; this type of question is called a Likert scale. The universe of this dataset is adults in Connecticut, and the survey was conducted in 2021.\n\nDataHaven. (n.d.). DataHaven Community Wellbeing Survey. https://ctdatahaven.org/reports/datahaven-community-wellbeing-survey\n\nPirrone, A. (2020). Visualizing Likert Scale Data: Same Data, Displayed Seven Different Ways, Nightingale. In Nightingale. https://medium.com/nightingale/seven-different-ways-to-display-likert-scale-data-d0c1c9a9ad59?source=friends_link&sk=60cb93604b71ecc8820cc785ed1afd1a\nStarting with just stacked bars for a single question at a time (see example), explore the data visually and see if you can find an anomaly. (Hint: one of these questions is not like the other.) Browse through Pirrone (2020) to get some ideas of more ways to visualize Likert data, especially ways that will illustrate the pattern well.\n\ndiv_pal &lt;- c('#00748a', '#479886', '#adadad', '#d06b56', '#b83654') # based on carto-color Temps\n\nrisks &lt;- youth_risks |&gt;\n  filter(category %in% c(\"Total\", \"Race/Ethnicity\", \"Income\", \"With children\")) |&gt;\n  mutate(question = forcats::as_factor(question))\n\nrisks |&gt;\n  filter(question == \"Graduate from high school\") |&gt;\n  mutate(value = scales::label_percent(accuracy = 1)(value)) |&gt;\n  tidyr::pivot_wider(id_cols = c(category, group), names_from = response) |&gt;\n  knitr::kable(align = \"llrrrrr\")\n\n\n\nTable 1: Ratings of likelihood that young people will graduate from high school, share of Connecticut adults, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncategory\ngroup\nAlmost certain\nVery likely\nA toss up\nNot very likely\nNot at all likely\n\n\n\n\nTotal\nConnecticut\n39%\n55%\n4%\n0%\n1%\n\n\nRace/Ethnicity\nWhite\n39%\n56%\n4%\n0%\n1%\n\n\nRace/Ethnicity\nBlack\n36%\n55%\n3%\n1%\n4%\n\n\nRace/Ethnicity\nLatino\n44%\n44%\n11%\n0%\n0%\n\n\nIncome\n&lt;$30K\n28%\n61%\n7%\n1%\n2%\n\n\nIncome\n$30K-$100K\n38%\n55%\n6%\n1%\n1%\n\n\nIncome\n$100K+\n44%\n55%\n1%\n0%\n0%\n\n\nWith children\nNo kids\n36%\n57%\n5%\n1%\n1%\n\n\nWith children\nKids in home\n45%\n51%\n4%\n0%\n0%",
    "crumbs": [
      "Weekly notes",
      "9. Making responsible decisions"
    ]
  },
  {
    "objectID": "weeks/05_good_code.html",
    "href": "weeks/05_good_code.html",
    "title": "5. Writing good code",
    "section": "",
    "text": "Think about what you need in order to leave the house for work or school. What things do you need to get out the door—some variation on “phone, wallet, keys”? Think about what influences your list, e.g. maybe you switch modes of transportation, which will decide whether you need car keys, a bike helmet and lights, or a bus card.\nWrite down:\n\nthe things you always need\nthe things you sometimes need",
    "crumbs": [
      "Weekly notes",
      "5. Writing good code"
    ]
  },
  {
    "objectID": "weeks/05_good_code.html#warm-up",
    "href": "weeks/05_good_code.html#warm-up",
    "title": "5. Writing good code",
    "section": "",
    "text": "Think about what you need in order to leave the house for work or school. What things do you need to get out the door—some variation on “phone, wallet, keys”? Think about what influences your list, e.g. maybe you switch modes of transportation, which will decide whether you need car keys, a bike helmet and lights, or a bus card.\nWrite down:\n\nthe things you always need\nthe things you sometimes need",
    "crumbs": [
      "Weekly notes",
      "5. Writing good code"
    ]
  },
  {
    "objectID": "weeks/05_good_code.html#documenting-code",
    "href": "weeks/05_good_code.html#documenting-code",
    "title": "5. Writing good code",
    "section": "Documenting code",
    "text": "Documenting code\nOne of the most important things you can do as a programmer is to document your code. This can be hard to do well, but it’s essential to making sure your code is clear and accountable and that your work can be reproduced or repurposed. (If you’ve followed the “replicability crisis” in the sciences over the past decade or so, you’ve seen what can go very wrong when your work isn’t documented accurately for yourself and others!)\nA common suggestion is to write your code assuming you’ll come back to it in 6 months and need to be able to pick up where you left off. I usually also assume a coworker or colleague will need to rerun or reuse my code, so even if I’m doing something that I’ll remember 6 months from now, they might not know what things mean. It also gets me out of spending unnecessary amounts of time walking interns through an analysis if I can say, “I tried to document everything really well, so read through it, run all the code, and let me know if you need help after that.” Documenting code also helps ease the transition into package development, which requires a lot of documentation.\nI don’t document everything—plenty of my work is routine and straightforward enough—but some of the things I try to always take note of:\n\nAny sort of analysis or process that’s out of the ordinary or complex. Don’t assume you’ll remember later why you used a new approach.\nAnything I know someone else will need to be able to reference. Sometimes I do EDA on something that a coworker will then finish up or need to write about. I need to make sure they can do that accurately.\nOutside sources that don’t come from that specific project. If your project is contained within a set of folders, and you’ve copied data in from some other project, make a note of where it comes from so if you need to update it you know where to get it from.\nDecision-making that you might need to keep track of or argue for later. e.g. a comparison of categories between datasets with a note that says “these categories changed significantly since the previous data collection” will be helpful when someone asks why you didn’t include trends in an analysis.\nReferences. If I came up with some code based on a Stack Overflow post or a blog post somewhere, or I’m building off of someone else’s methodology, I’ll usually include a link in my comments.\n\nThis also applies to simple things like organizing your projects. If you have a bunch of folders called things like “data analysis 1” and they all contain a jumble of different notebooks and scripts for different purposes, and the scripts are all called “analysis_of_stuff.R”, you’re going to lose things easily and not know how different pieces build on each other. Similarly, don’t spend time doing an analysis only to write your data out to a file called “data.csv” and a plot called “map.png”. This might seem obvious, but I’ve seen people do all of these things.\n\nExercises\nGoing back to your list for leaving the house, add notes for how you decide whether you’ll need something. For example, if your laptop is on your “sometimes” list, write down what decides that.\n\n\n\n\n\n\nBrainstorm\n\n\n\n\nCash – if you’re going somewhere that doesn’t take cards / mobile\nSweater – weather / environment\nTea – sleepiness\nEarbuds – length of time out of house / time of day\nWork badge – going to office\nLaptop charger – if not already charged",
    "crumbs": [
      "Weekly notes",
      "5. Writing good code"
    ]
  },
  {
    "objectID": "weeks/05_good_code.html#reusable-code",
    "href": "weeks/05_good_code.html#reusable-code",
    "title": "5. Writing good code",
    "section": "Reusable code",
    "text": "Reusable code\nOne rule of thumb I’ve heard is that it’s fine to repeat your code to do the same thing twice, but if you need to do it a third time, you should write a function. It might mean taking a step back from what you’re working on at the moment, but it’s pretty much always worth the time. Alongside documenting your code in general, it’s important to document your functions—what they do, what the arguments mean, what types of values arguments can take. Try to your functions and their arguments in ways that make it clear what they mean as well.",
    "crumbs": [
      "Weekly notes",
      "5. Writing good code"
    ]
  },
  {
    "objectID": "weeks/05_good_code.html#exercises-1",
    "href": "weeks/05_good_code.html#exercises-1",
    "title": "5. Writing good code",
    "section": "Exercises",
    "text": "Exercises\nBuild out your morning routine into a pseudocode function, complete with arguments. Aim to make it flexible enough that you could use it any day of the week.\n\nExample\nPseudocode\nalways need: keys, wallet, phone, meds, mask\nif I'm biking:\n        bring a helmet\notherwise:\n        bring a bus card\nif I'm working:\n        bring my laptop\nif it's Wednesday:\n        take a covid test\nWorking R example\n\n# PARAMETERS:\n# date: Date object, today's date\n# biking: Logical, whether or not I'll be biking\n# working: Logical, whether or not I'm going to work\n# RETURNS:\n# prints a string\nleave_the_house &lt;- function(date = lubridate::today(), biking = TRUE, working = TRUE) {\n  day_of_week &lt;- lubridate::wday(date, label = TRUE, abbr = FALSE)\n  always_need &lt;- c(\"keys\", \"phone\", \"wallet\", \"meds\")\n  sometimes_need &lt;- c()\n  if (biking) {\n    sometimes_need &lt;- c(sometimes_need, \"helmet\")\n  } else {\n    sometimes_need &lt;- c(sometimes_need, \"bus card\")\n  }\n  if (working) {\n    sometimes_need &lt;- c(sometimes_need, \"laptop\")\n  }\n  \n  need &lt;- c(always_need, sometimes_need)\n  cat(\n    sprintf(\"Happy %s! Today you need:\", day_of_week), \"\\n\",\n    paste(need, collapse = \", \")\n  )\n  if (day_of_week == \"Wednesday\") {\n          cat(\"\\n\\nBut take a COVID test first!\")\n  }\n}\n\nleave_the_house(biking = FALSE)\n\nHappy Thursday! Today you need: \n keys, phone, wallet, meds, bus card, laptop",
    "crumbs": [
      "Weekly notes",
      "5. Writing good code"
    ]
  },
  {
    "objectID": "weeks/05_good_code.html#organization",
    "href": "weeks/05_good_code.html#organization",
    "title": "5. Writing good code",
    "section": "Organization",
    "text": "Organization\nCome up with a structure of directories you like for a project, and stick with it. The notes template repo I setup for this class has a pared down version of what I usually use, but a full version of what I might have, even for a small project, looks like this:\ncool_project          \n ¦--analysis         # EDA, notebooks, and scripts that create output\n |--design           # scripts *only* for creating publishable charts\n ¦--fetch_data       # raw data, often downloaded in a script\n ¦   ¦--fr_comments  # folders for each raw data source\n ¦   °--pums       \n ¦--input_data       # cleaned data that is sourced for the project, maybe cleaned in prep scripts\n ¦--output_data      # data that's a product of analysis in this project\n ¦--plots            # plots that can be distributed or published\n ¦--prep_scripts     # scripts that download, clean, reshape data\n °--utils            # misc scripts & bits of data to use throughout the project\n\n\n\n\n\n\nAn aside: build tools\n\n\n\nBuild tools are outside the scope of this class, but for larger projects especially or projects that will need to be updated over time, they’ll save you a lot of headaches. I have some projects that I rebuild once a year when new ACS data comes out, and I’ve got things down to where I can make one or two calls on the command line flagging the year as a variable, and all the data wrangling and analyses are ready to go. In fact, this site rebuilds from a frozen list of packages every time I push to GitHub, and if that build is successful, it publishes automatically.\nSome tools I use:\n\nGNU Make, the OG build tool\nSnakemake, like GNU Make but written in Python and designed for data analysis\nGitHub actions, including ones specifically for R\nDocker, build a small isolated environment for your projects, some designed for R\nPackage & environment managers: mamba or conda for Python, renv for R",
    "crumbs": [
      "Weekly notes",
      "5. Writing good code"
    ]
  },
  {
    "objectID": "tips/eda.html",
    "href": "tips/eda.html",
    "title": "Spatial and non-spatial EDA",
    "section": "",
    "text": "Remember that when you’re working with data you could put on a map, you want to double check whether you should put it on a map. One of the best ways you can do this is just by doing your regular EDA first—understand your distributions, correlations, outliers, disparities, etc before you think about it spatially. Then start to investigate whether there are spatial patterns. Both spatial and non-spatial visualizations will have their strengths and weaknesses, so you’ll want to pair them such that they complement each other. Here are some examples with the CDC health data. I’ll start looking at preventive care measures (checkup, health insurance, dental visit) and basic health outcomes (asthma, diabetes) for Baltimore and surrounding counties.\nlibrary(dplyr)\nlibrary(justviz)\nlibrary(ggplot2)\nlibrary(sf)\n\ncounty_fips &lt;- tidycensus::fips_codes |&gt;\n  filter(state_code == \"24\") |&gt;\n  select(state_code, county_code, county)\n\n# CDC data still uses 2010 tracts, so I'll download that shapefile\n# keep county FIPS codes to join with names in order to filter\ntracts10 &lt;- tigris::tracts(state = \"24\", cb = TRUE, year = 2010, progress_bar = FALSE) |&gt;\n  select(tract = GEO_ID, state_code = STATE, county_code = COUNTY) |&gt;\n  left_join(county_fips, by = c(\"state_code\", \"county_code\")) |&gt;\n  filter(county %in% c(\"Baltimore city\", \"Baltimore County\", \"Howard County\", \"Anne Arundel County\")) |&gt;\n  # drop \"1400000US\" from FIPS codes, only keep digits til end of string\n  mutate(tract = stringr::str_extract(tract, \"\\\\d+$\"))\n\ntracts10 |&gt;\n  st_drop_geometry() |&gt;\n  count(county)\n\n\n\n\n\ncounty\nn\n\n\n\n\nAnne Arundel County\n105\n\n\nBaltimore County\n214\n\n\nBaltimore city\n200\n\n\nHoward County\n55\nKeeping in mind that Balt city & county each have about twice as many tracts as Anne Arundel, 4 times as many as Howard, so they’ll likely dominate distributions that aren’t split out by county.\nhead(cdc)\n\n\n\n\n\nlevel\nyear\nlocation\nindicator\nvalue\npop\n\n\n\n\nus\n2021\nUS\nHealth insurance\n10.8\n331893745\n\n\nus\n2021\nUS\nCancer (except skin)\n7.0\n331893745\n\n\nus\n2021\nUS\nMental health\n14.7\n331893745\n\n\nus\n2021\nUS\nDiabetes\n11.3\n331893745\n\n\nus\n2020\nUS\nDental visit\n64.8\n331893745\n\n\nus\n2021\nUS\nCurrent asthma\n9.7\n331893745\n\n\n\n\n\n# remind myself what indicators are here\nunique(cdc$indicator)\n\n [1] \"Health insurance\"     \"Cancer (except skin)\" \"Mental health\"       \n [4] \"Diabetes\"             \"Dental visit\"         \"Current asthma\"      \n [7] \"Sleep &lt;7 hours\"       \"Annual checkup\"       \"Mobility disability\" \n[10] \"Depression\"          \n\n# remind myself what geographic levels are here\nlevels(cdc$level)\n\n[1] \"us\"     \"state\"  \"county\" \"tract\" \n\n# filter for indicators I want, and if it's a tract, only the ones in this region\n# cdc data says \"Baltimore City\", FIPS table says \"Baltimore city\"\n# cdc also doesn't say \"County\", just e.g. \"St. Mary's\"\n# want to also have county names for tracts--use a left join to not drop other levels\n# county portion of FIPS code is digits 3 to 5\n# relabel \"health insurance\" to \"no health insurance\" since that's what it actually is\ncdc_subset &lt;- cdc |&gt;\n  mutate(county_code = ifelse(level == \"tract\",\n                         substr(location, start = 3, stop = 5),\n                         NA_character_)) |&gt;\n  left_join(county_fips, by = \"county_code\") |&gt;\n  select(-county_code, -state_code) |&gt;\n  mutate(location = stringr::str_replace(location, pattern = \"City\", replacement = \"city\")) |&gt;\n  # for counties, if string doesn't end with \"city\", replace the ending boundary with \" County\"\n  mutate(location = ifelse(level == \"county\",\n                           stringr::str_replace(location, \"(?&lt;!city)\\\\b$\", \" County\"),\n                           location)) |&gt;\n  filter(indicator %in% c(\"Health insurance\", \"Diabetes\", \"Dental visit\", \"Current asthma\", \"Annual checkup\")) |&gt;\n  # use | to make an OR statement\n  filter((level %in% c(\"us\", \"state\")) |\n           (level == \"county\" & location %in% tracts10$county) |\n           (level == \"tract\" & location %in% tracts10$tract)) |&gt;\n  mutate(value = value / 100) |&gt;\n  mutate(indicator = ifelse(indicator == \"Health insurance\", \"No health insurance\", indicator))\n\ncdc_subset |&gt;\n  distinct(level, location) |&gt;\n  count(level)\n\n\n\n\n\nlevel\nn\n\n\n\n\nus\n1\n\n\nstate\n1\n\n\ncounty\n4\n\n\ntract\n568\nCounting the distinct number of locations per level helped me figure out the string replacements I needed to make above—originally I had 1 county and no tracts, and knew that was wrong.",
    "crumbs": [
      "Tips & tricks",
      "Spatial and non-spatial EDA"
    ]
  },
  {
    "objectID": "tips/eda.html#distributions-etc-of-cdc-data",
    "href": "tips/eda.html#distributions-etc-of-cdc-data",
    "title": "Spatial and non-spatial EDA",
    "section": "Distributions, etc of CDC data",
    "text": "Distributions, etc of CDC data\n\ncdc_subset |&gt;\n  filter(level == \"tract\") |&gt;\n  ggplot(aes(x = value)) +\n  geom_histogram(bins = 25) +\n  facet_wrap(vars(indicator), scales = \"free\")\n\n\n\n\n\n\n\n\nAnnual checkup is about as close to normally distributed as you’ll see with public health data. Dental visit has a long lower tail—many tracts have a majority getting a dental visit but some have rates much lower. Diabetes has most values clustered around ~10% but then a heavy upper tail. Similar for lacking health insurance and asthma.\n\ncdc_subset |&gt;\n  filter(level != \"tract\") |&gt;\n  ggplot(aes(x = value, y = location)) +\n  geom_col() +\n  facet_wrap(vars(indicator), scales = \"free_x\") +\n  theme(panel.spacing.x = unit(0.2, \"in\"))\n\n\n\n\n\n\n\ncdc_subset |&gt;\n  filter(level != \"tract\") |&gt;\n  ggplot(aes(x = value, y = location)) +\n  geom_point(size = 3) +\n  facet_wrap(vars(indicator), scales = \"free_x\") +\n  theme(panel.spacing.x = unit(0.2, \"in\"))\n\n\n\n\n\n\n\n\nBar chart is good for looking at absolute values but in this case not as good as a dot plot for seeing the differences between values. Looking at averages across large areas (state & counties) some of these have very similar values, esp annual checkup and asthma. That points to the disparities at smaller geographies being really important—that amount of variation seen in the histograms is washed out at the county level. Maps might be able to help shine a light on that, especially if the high or low values are clustered in certain areas.\n\ncdc_subset |&gt;\n  filter(level == \"tract\") |&gt;\n  ggplot(aes(x = value, y = county)) +\n  geom_boxplot() +\n  facet_wrap(vars(indicator), scales = \"free_x\", ncol = 1)\n\n\n\n\n\n\n\n\nOn each of these Baltimore city has enormous variations—this makes sense since we have huge disparities within the city. (For the cities I study in Connecticut, this would more often be a city/suburb pattern; Connecticut’s major cities don’t have the sorts of affluent neighborhoods in them that Baltimore has, those neighborhoods are in affluent suburbs instead.)\nSome basic stats testing of each indicator by county:\n\n# split into a list of data frames to keep indicators separate\ncdc_split &lt;- cdc_subset |&gt;\n  filter(level == \"tract\") |&gt;\n  split(~indicator)\n\nstr(cdc_split, max.level = 1)\n\nList of 5\n $ Annual checkup     : tibble [568 × 7] (S3: tbl_df/tbl/data.frame)\n $ Current asthma     : tibble [568 × 7] (S3: tbl_df/tbl/data.frame)\n $ Dental visit       : tibble [568 × 7] (S3: tbl_df/tbl/data.frame)\n $ Diabetes           : tibble [568 × 7] (S3: tbl_df/tbl/data.frame)\n $ No health insurance: tibble [568 × 7] (S3: tbl_df/tbl/data.frame)\n\n\nOne-way ANOVA test of equal means across counties, weighted by tract population. Post-hoc testing with Tukey’s HSD to see which counties have significantly higher or lower mean values. Feel free to ignore if you haven’t gotten this far in stats.\n\n# shorten names, increase margin size\npar(mar = c(5, 10, 4, 2) + 0.1)\n\ncdc_split |&gt;\n  purrr::map(mutate, county = stringr::str_remove(county, \" County\")) |&gt;\n  purrr::map(function(df) {\n  # use aov for one-way ANOVA\n  aov(value ~ county, data = df, weights = pop)\n}) |&gt;\n  purrr::map(TukeyHSD) |&gt;\n  purrr::map(plot, las = 1, cex = 0.4)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n$`Annual checkup`\nNULL\n\n$`Current asthma`\nNULL\n\n$`Dental visit`\nNULL\n\n$Diabetes\nNULL\n\n$`No health insurance`\nNULL\n\n\nLevene’s test of equal variances\n\ncdc_split |&gt;\n  purrr::map(function(df) {\n    car::leveneTest(value ~ county, data = df, weights = pop)\n  })\n\n$`Annual checkup`\nLevene's Test for Homogeneity of Variance (center = median: pop)\n       Df F value    Pr(&gt;F)    \ngroup   3  28.076 &lt; 2.2e-16 ***\n      564                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n$`Current asthma`\nLevene's Test for Homogeneity of Variance (center = median: pop)\n       Df F value    Pr(&gt;F)    \ngroup   3  54.708 &lt; 2.2e-16 ***\n      564                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n$`Dental visit`\nLevene's Test for Homogeneity of Variance (center = median: pop)\n       Df F value    Pr(&gt;F)    \ngroup   3  38.712 &lt; 2.2e-16 ***\n      564                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n$Diabetes\nLevene's Test for Homogeneity of Variance (center = median: pop)\n       Df F value    Pr(&gt;F)    \ngroup   3   86.57 &lt; 2.2e-16 ***\n      564                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n$`No health insurance`\nLevene's Test for Homogeneity of Variance (center = median: pop)\n       Df F value    Pr(&gt;F)    \ngroup   3  32.651 &lt; 2.2e-16 ***\n      564                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nEvery indicator has unequal variance across counties. Again points to important neighborhood-level disparities in at least some of the cities / counties.",
    "crumbs": [
      "Tips & tricks",
      "Spatial and non-spatial EDA"
    ]
  },
  {
    "objectID": "tips/eda.html#spatial-stuff",
    "href": "tips/eda.html#spatial-stuff",
    "title": "Spatial and non-spatial EDA",
    "section": "Spatial stuff",
    "text": "Spatial stuff\n\ntracts10 |&gt;\n  left_join(cdc_subset, by = c(\"tract\" = \"location\", \"county\")) |&gt;\n  rmapshaper::ms_simplify(keep = 0.8) |&gt;\n  ggplot() +\n  geom_sf(aes(fill = value), color = \"white\", linewidth = 0) +\n  facet_wrap(vars(indicator))\n\n\n\n\n\n\n\n\nTwo things that don’t work well here: there are a few tracts without data so they end up in an NA facet. For EDA that’s not a big deal, but for a final project I’d want to handle it. Also, the scale doesn’t work because ranges are very different across indicators. Switch to using split data instead so each panel can get its own color scale:\n\ncdc_split |&gt;\n  purrr::imap(function(df, indicator) {\n    tracts10 |&gt;\n      left_join(df, by = c(\"tract\" = \"location\", \"county\")) |&gt;\n      ggplot() +\n      geom_sf(aes(fill = value), color = \"white\", linewidth = 0) +\n      scale_fill_distiller(palette = \"YlGnBu\", direction = 1) +\n      labs(title = indicator)\n  })\n\n$`Annual checkup`\n\n\n\n\n\n\n\n\n\n\n$`Current asthma`\n\n\n\n\n\n\n\n\n\n\n$`Dental visit`\n\n\n\n\n\n\n\n\n\n\n$Diabetes\n\n\n\n\n\n\n\n\n\n\n$`No health insurance`\n\n\n\n\n\n\n\n\n\nThat also handled the missing values!\nFor a final project I might want to do a couple of these maps alongside a bar chart or dot plot at the state & county level.\nI also might want to try overlaying some of our locations such as brownfields (those could exacerbate asthma) and highways (those definitely exacerbate asthma).\nThere are similar statistical tests you might do for spatial data, such as Moran’s test of spatial autocorrelation (clustering of high/low values).",
    "crumbs": [
      "Tips & tricks",
      "Spatial and non-spatial EDA"
    ]
  },
  {
    "objectID": "tips/fonts.html",
    "href": "tips/fonts.html",
    "title": "Fonts",
    "section": "",
    "text": "There are a few main types of typefaces; most commonly you’ll see serif, sans-serif, and monospace. Serif typefaces have fancy dangly bits on the ends of different lines, sans-serif (i.e. “without serif”) ones don’t, and monospace typefaces have all characters with the same width (such as typewriters or what you most likely use for code).\nText in general and type specifically can be an afterthought in data visualization, and I don’t often see much discussion on it but as always Datawrapper has a good, concise guide (Muth (2024)). Usually, though, a few things you’ll want to think about when choosing typefaces for data visualization (none of these are hard & fast rules):\n\nMuth, L. C. (2024). Which fonts to use for your charts and tables. In Datawrapper. https://blog.datawrapper.de/fonts-for-data-visualization/\n\nLean toward sans-serifs over serifs for data viz\nText should be legible at small sizes, such as on axis ticks\nUse a typeface with varying weights (these get numbered from 300 (light) to 900 (black))\nBe consistent: pick a typeface and stick with it throughout the visualization or the whole project, using varying weights and sizes\nAvoid oldstyle numerals\nKeep it simple and avoid ambiguous characters\n\nMy preference is usually to use slightly narrow sans-serif typefaces that are boxy with numerals of pretty uniform sizes. Barlow Semi Condensed is my go-to for standalone charts or online visualizations; Source Sans 3 is one I use a lot when I’m embedding charts into a document alongside long text and tables, all in the same typeface or another one that pairs well with it. Other good ones on Google Fonts are PT Sans, Roboto, Open Sans; Arial works well too but isn’t on Google.",
    "crumbs": [
      "Tips & tricks",
      "Fonts"
    ]
  },
  {
    "objectID": "tips/fonts.html#typefaces",
    "href": "tips/fonts.html#typefaces",
    "title": "Fonts",
    "section": "",
    "text": "There are a few main types of typefaces; most commonly you’ll see serif, sans-serif, and monospace. Serif typefaces have fancy dangly bits on the ends of different lines, sans-serif (i.e. “without serif”) ones don’t, and monospace typefaces have all characters with the same width (such as typewriters or what you most likely use for code).\nText in general and type specifically can be an afterthought in data visualization, and I don’t often see much discussion on it but as always Datawrapper has a good, concise guide (Muth (2024)). Usually, though, a few things you’ll want to think about when choosing typefaces for data visualization (none of these are hard & fast rules):\n\nMuth, L. C. (2024). Which fonts to use for your charts and tables. In Datawrapper. https://blog.datawrapper.de/fonts-for-data-visualization/\n\nLean toward sans-serifs over serifs for data viz\nText should be legible at small sizes, such as on axis ticks\nUse a typeface with varying weights (these get numbered from 300 (light) to 900 (black))\nBe consistent: pick a typeface and stick with it throughout the visualization or the whole project, using varying weights and sizes\nAvoid oldstyle numerals\nKeep it simple and avoid ambiguous characters\n\nMy preference is usually to use slightly narrow sans-serif typefaces that are boxy with numerals of pretty uniform sizes. Barlow Semi Condensed is my go-to for standalone charts or online visualizations; Source Sans 3 is one I use a lot when I’m embedding charts into a document alongside long text and tables, all in the same typeface or another one that pairs well with it. Other good ones on Google Fonts are PT Sans, Roboto, Open Sans; Arial works well too but isn’t on Google.",
    "crumbs": [
      "Tips & tricks",
      "Fonts"
    ]
  },
  {
    "objectID": "tips/fonts.html#getting-and-using-fonts",
    "href": "tips/fonts.html#getting-and-using-fonts",
    "title": "Fonts",
    "section": "Getting and using fonts",
    "text": "Getting and using fonts\nGetting fonts working in R is very annoying. It is a different type of annoying in every operating system. One of the easier ways to do it is with the showtext package, which also installs one called sysfonts. This lets you register a font within your code, including by downloading one directly from Google Fonts, and is generally compatible with ggplot themes and geoms. However, you’ll get weird rendering in your previews in RStudio, and I haven’t yet figured out whether it’s better or worse since switching from RMarkdown to Quarto (or from macOS to Linux). It gets especially weird in combination with specifying figure sizes, or when using other packages like ggtext and cowplot. You’ll also get long streams of warnings, text will come out super small or super huge, etc.\nMy general advice is to get everything you want set in your chart first before you worry about fonts, and test them out by saving the image (ggsave) or looking at a preview of your document (e.g. quarto::quarto_preview) instead of wondering why it looks bad within RStudio.\n\n\nGoogle Fonts has a collection of articles about typography that I recommend if you want to get more into this. I’m particularly leaning on their definitions to keep from mixing up typeface and font; they describe it as “a typeface is to a song as a font is to an MP3 file.”\nYou’ll need to let your renderer know you’re using showtext via the YAML at the top of your document:\nFor RMarkdown: 1\n1 Supposedly RMarkdown will detect that you’re using showtext, but the whole thing is so finicky you might as well specify it.fig.showtext: true\nFor Quarto:\nknitr:\n  opts_knit:\n    fig.showtext: true\nI’ve also set\nfig-dpi: 150\nwhich might need to be adjusted on different computers.\nYou also need to call showtext_auto() before trying to change any fonts, and you should set the resolution (DPI).\nRegister a font with sysfonts::font_add or sysfonts::font_add_google. The first of these requires you to find a font on your computer, which can be tricky and is less portable than using the ones from Google. You can use sysfonts::font_paths() to find the paths that are searched for fonts on your computer. You can also adjust the weights used—400 for regular and 700 for bold are industry standards, but you can use different weights if they’re available.\n\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(showtext)\n\nsource(here::here(\"utils/plotting_utils.R\"))\n\nshowtext_auto()\nshowtext_opts(dpi = 300)\n\nsysfonts::font_families() # before loading any custom fonts\n\n[1] \"sans\"         \"serif\"        \"mono\"         \"wqy-microhei\"\n\nsysfonts::font_add_google(\"Libre Franklin\")\nsysfonts::font_add_google(\"Open Sans\")\nsysfonts::font_add_google(\"Roboto Condensed\")\nsysfonts::font_add_google(\"Barlow Semi Condensed\", bold.wt = 600) # make it a little less bold\nsysfonts::font_families() # after loading \n\n[1] \"sans\"                  \"serif\"                 \"mono\"                 \n[4] \"wqy-microhei\"          \"Libre Franklin\"        \"Open Sans\"            \n[7] \"Roboto Condensed\"      \"Barlow Semi Condensed\"\n\n\nYou can set the fonts in a ggplot theme, e.g.\ntheme_light(base_family = \"Roboto Condensed\")\nThat setting will then trickle down to any other text-based theme elements in the chart, such as axis titles or legend labels. You could then override specific elements if you want.\n\np &lt;- justviz::wages |&gt;\n  filter(dimension == \"edu\") |&gt;\n  ggplot(aes(x = edu, y = earn_q50, fill = name)) +\n  geom_col(position = position_dodge2(), width = 0.8) +\n  labs(title = \"Median individual earnings by educational attainment, 2021\")\n\np +\n  labs(subtitle = \"Default\") +\n  theme_light()\n\n\n\n\n\n\n\np +\n  labs(subtitle = \"Libre Franklin\") +\n  theme_light(base_family = \"Libre Franklin\")\n\n\n\n\n\n\n\np +\n  labs(subtitle = \"Barlow Semi Condensed\") +\n  theme_light(base_family = \"Barlow Semi Condensed\")\n\n\n\n\n\n\n\n\nTheme settings only affect theme elements, though. If there were a text-based geom, it gets its font set separately.\n\np +\n  geom_text(aes(label = dollar_k(earn_q50), y = earn_q50 - 2e3), \n            position = position_dodge2(width = 0.8), vjust = 1, fontface = \"bold\") +\n  theme_light(base_family = \"Barlow Semi Condensed\")\n\n\n\n\n\n\n\n\nIf you want to change the font of the geom, you have to do it directly:\n\np +\n  geom_text(aes(label = dollar_k(earn_q50), y = earn_q50 - 2e3), \n            position = position_dodge2(width = 0.8), vjust = 1, fontface = \"bold\",\n            family = \"Barlow Semi Condensed\") +\n  theme_light(base_family = \"Barlow Semi Condensed\")\n\n\n\n\n\n\n\n\nLast, if you need to export your chart, you should do that and double check that the text looks right.\n\nfinal_plot &lt;- p +\n  geom_text(aes(label = dollar_k(earn_q50), y = earn_q50 - 2e3), \n            position = position_dodge2(width = 0.8), vjust = 1, fontface = \"bold\",\n            family = \"Barlow Semi Condensed\") +\n  theme_light(base_family = \"Barlow Semi Condensed\")\n\nggsave(here::here(\"inputs/nice_fonts_plot.png\"), final_plot,\n       width = 7, height = 4, dpi = 300, bg = \"white\")\n\nThe saved result:",
    "crumbs": [
      "Tips & tricks",
      "Fonts"
    ]
  },
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": "Syllabus",
    "section": "",
    "text": "At its essence, the aim of data visualization is to move data and its meaning(s) and context(s) from some origin (spreadsheets, observed phenomena, etc.) to a larger audience. It’s a spectrum of incredibly powerful tools for not just understanding and explaining facts, but also for shaping what those facts are and creating the narrative around them. By the end of this course, you will have thought through your role and responsibility in an evolving field, developed a set of best practices that is likely to continue to change, engaged with larger social currents toward your own goals, and strengthened your skills in R.\nBecause this is part of a professional studies program based on open source software and the ethos behind it, the course will be very hands-on and require everyone’s willingness to contribute and participate. Instead of tests and graded homework assignments, we’ll focus on practice, critique, and revision, building continuously on individual projects and shared tools. To some extent, the class structure will mimic a workplace, where you have projects with checkpoints and meetings to brainstorm and workshop your ideas, with time to work both in class and on your own at home.\nBecause data science and data visualization—and the software we use for both—change so quickly, a lot of the community’s discourse happens in less formal settings, such as blogs, social media, podcasts, and workshops, rather than just traditional academic journals and books. Our readings (defined loosely enough to include videos of talks, podcasts, and simply browsing through data visualization projects) will likewise fall along this spectrum, and you’ll have some flexibility in what you read and share.\nAbove all, I want this to be a course that is useful to you as you build a career of critical engagement with data. The schedule is intentionally loose so we can adjust based on skills we may want or need to build upon, and each student’s goals and interests. Please be willing to share what you want to learn, contribute resources, and ask for what you need of me and each other.",
    "crumbs": [
      "Overview",
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#objectives",
    "href": "syllabus.html#objectives",
    "title": "Syllabus",
    "section": "Objectives",
    "text": "Objectives\nThe first half of the course will be focused on non-spatial data visualization; the second half will be focused on spatial data and how to integrate the two. Some of the principles we go over for non-spatial and spatial will differ, but objectives remain the same.\nBy the end of the course, students will:\n\nHave an understanding of the basics of visual perception, and how to use that knowledge to design data visualizations well\nBe familiar with the grammar of graphics framework to think about components and purposes of visual elements\nBe skilled in programming in R and using the ggplot2 data visualization ecosystem\nKnow how to give and receive constructive feedback on visualizations, both their own and others’, and to revise and improve upon their work\nBe able to identify potential harms done by inappropriate or misleading visualizations, and make corrections\nBe able to make, articulate, and argue for good decisions in designing charts and maps\nHave made many, many unpolished visualizations and several polished, presentation-ready ones\n\nSuccessful students will finish the course with finished products for their portfolios of high enough quality to include with applications to jobs or other academic programs:\n\n1–2 completed, presentation-ready data visualization projects\nreproducible, documented code that can be repurposed at another organization\ncontributions to an open source codebase",
    "crumbs": [
      "Overview",
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#materials",
    "href": "syllabus.html#materials",
    "title": "Syllabus",
    "section": "Materials",
    "text": "Materials\n\nReadings\nAll readings will be available to students for free. Many will be open source texts and have code available. Readings will be a mix of theory and practice.\nThe schedule of the course will roughly follow the structure of the book Fundamentals of Data Visualization (Wilke, 2019). Both the book and the source code used to write it are available for free online.\n\nWilke, C. (2019). Fundamentals of data visualization: A primer on making informative and compelling figures (First edition). O’Reilly. https://clauswilke.com/dataviz/\n\nWickham, H., Çetinkaya-Rundel, M., & Grolemund, G. (2023). R for data science: Import, tidy, transform, visualize, and model data (2nd edition). O’Reilly. https://r4ds.hadley.nz/\n\nCairo, A. (2019). How charts lie: Getting smarter about visual information (First edition). W. W. Norton & Company.\n\nYau, N. (2013). Data points: Visualization that means something. John Wiley & Sons, Inc.\nWe’ll also read portions of R for Data Science (Wickham et al., 2023) (also open source), How Charts Lie (Cairo, 2019), and Data Points (Yau, 2013), as well as a variety of other sources of different media. I’ll keep a running list of resources in the online class notes with other tutorials and references.\n\n\nSoftware\nThis is a rough set of the software and tools we will use, with open source software in italics:\n\nR programming language\nggplot2 and related packages\nRStudio or another integrated development environment\nQuarto, a markdown-based publishing system from the same team as RStudio\ngit for version control, GitHub for storage of version-controlled materials, and GitHub Classroom for discussions and submitting code\nBlackboard for assignments and announcements\n\nI’m open to suggestions on any other tools you all think would be useful.\n\n\nOther tools\nIf at all possible, you should have a laptop of your own for this class. All the software we’re using is free and open source, so you should be able to install everything on your computer. If you do not have a laptop, you can borrow one from the library, or, because we will be using git for version tracking and GitHub for storage, you can use a lab computer and make sure to upload your work regularly.\nWe’ll be doing a lot of sketching by hand (you don’t have to be good at drawing), so you’ll need a notebook and pens or pencils that are nice to doodle with. I highly, highly recommend finding a graph paper or dotted notebook.",
    "crumbs": [
      "Overview",
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#schedule",
    "href": "syllabus.html#schedule",
    "title": "Syllabus",
    "section": "Schedule",
    "text": "Schedule\nThe schedule has some flexibility built into it, but tentatively goes as follows:\n\n\n\n\n\n\n\n\nWeek\nSection\nTopic\n\n\n\n\n1\nNon-spatial data viz\nWhat is a chart, and do you need one?\n\n\n2\n\nEncoding data to visuals; making meaning of your data\n\n\n3\n\nWriting good code; working with color\n\n\n4\n\nText and annotation; uncertainty and distribution\n\n\n5\n\nMaking good decisions pt. 1\n\n\n6\n\nAccessibility, literacy, and audience\n\n\n7\n\nStorytelling pt. 1 (empathy & equity); experimentation\n\n\n8\nSpatial data viz\nWhat is a map, and do you need one?\n\n\n\n\nProject 1 due\n\n\n9\n\nEncoding data to space; harmful practices\n\n\n10\n\nColor, text, and annotations pt. 2\n\n\n11\n\nMaking good decisions pt. 2\n\n\n12\n\nStorytelling pt. 2 (history & cohesion); experimentation\n\n\n13\n\nFinal critique; tying up loose ends\n\n\n14\n\nFinishing touches on projects\n\n\n\n\nProject 2 due\n\n\n\n\nClass structure\nA typical class session will be roughly:\n\n\n\nActivity\nTime\n\n\n\n\nWarm-up\n5-10 minutes\n\n\nReport-backs\n10-15 minutes, if any\n\n\nLecture\n1 hour max + questions\n\n\nWorkshop, critique, or lab\nRemaining time",
    "crumbs": [
      "Overview",
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#grading",
    "href": "syllabus.html#grading",
    "title": "Syllabus",
    "section": "Grading",
    "text": "Grading\nIn data visualization there aren’t any perfectly right answers, and there aren’t too many perfectly wrong ones either. As a result, rather than tedious quizzes and problem sets, your grade will reflect the effort you put into developing your process and your critical eye, and how successfully you create compelling stories with data.\n\nParticipation\nThere will be opportunities for participation points every week, including:\n\nBringing in visualizations you’ve found for us to discuss\nOpening your work up for workshopping\nContributing code (there’s an R package in development for this class), with more points given for students with less experience in R—this includes the less glamorous but crucial tasks of testing, debugging, and documenting\nDoing an optional reading or attending a talk and reporting back to the class on some interesting things you learned\nAdding a resource to the class notes\n\nThere will be two opportunities to lose points as well:\n\nBeing mean or unnecessarily harsh in critique\nUnexcused absences (see below)\n\nThere’s no set number of participation points you need—just rack them up when you can, forgo them when you have to, and I’ll scale them at the end of the semester. Notice that most of these involve contributing to your classmates’ growth as well as your own.\n\n\nProjects\nThere will be 2 projects, one midterm and one final, that you’ll be working on throughout the semester. Both will build upon the exercises, and you’ll have lots of time to work on them in class and receive feedback from myself and your peers. The first will be non-spatial data, and the second will be both spatial and non-spatial. You’ll be responsible for moving from a dataset through to a polished visualization that tells a story and has real-world impact. You will also document your process along the way and have check-ins regularly. Each project will also have a semi-formal write-up to explain what you did and why, and to situate your work into the theory and principles we study.\n\n\nOther assignments\nWe’ll have a few more small assignments, including short case studies and peer reviews.\n\n\nGrading scale\nGrades will be rounded to the nearest whole percent.\n\n\n\nGrade\nPercentage\n\n\n\n\nA+\n97% +\n\n\nA\n93-96%\n\n\nA-\n90-92%\n\n\nB+\n87-89%\n\n\nB\n83-86%\n\n\nB-\n80-82%\n\n\nC+\n77-79%\n\n\nC\n73-76%\n\n\nC-\n70-72%\n\n\nD+\n67-69%\n\n\nD\n63-66%\n\n\nD-\n60-62%\n\n\nF\n&lt; 60%\n\n\n\n\n\nGrade distribution\n\n\n\nCategory\nShare of grade\n\n\n\n\nCase studies\n15%\n\n\nPeer review & reflections\n5%\n\n\nParticipation\n20%\n\n\nProject 1 visualization\n20%\n\n\nProject 1 write-up\n5%\n\n\nProject 2 visualization\n25%\n\n\nProject 2 write-up\n10%",
    "crumbs": [
      "Overview",
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#attendance",
    "href": "syllabus.html#attendance",
    "title": "Syllabus",
    "section": "Attendance",
    "text": "Attendance\nAs grad students, your course load is one of many responsibilities you juggle, so I know things will come up from time to time that prevent you from getting to class. If you need to miss class or will be late, just let me know in advance (email or DM), and as long as absences don’t become excessive, it should be fine. If there is some reason you’ll need to miss class several times, such as chronic illness (after all, COVID’s still here), just let me know and we can figure something out. If you can’t attend class but are able to participate remotely, I can stream on Zoom or WebEx.\nUnexcused absences, except for a serious emergency (e.g. you got into a car accident on the way to campus), will cost you participation points. Excused absences will not.",
    "crumbs": [
      "Overview",
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#umbc-policies-and-resources",
    "href": "syllabus.html#umbc-policies-and-resources",
    "title": "Syllabus",
    "section": "UMBC policies and resources",
    "text": "UMBC policies and resources\n\nAccessibility and Disability Accommodations, Guidance and Resources\nAccommodations for students with disabilities are provided for all students with a qualified disability under the Americans with Disabilities Act (ADA & ADAAA) and Section 504 of the Rehabilitation Act who request and are eligible for accommodations. The Office of Student Disability Services (SDS) is the UMBC department designated to coordinate accommodations that creates equal access for students when barriers to participation exist in University courses, programs, or activities.\nIf you have a documented disability and need to request academic accommodations in your courses, please refer to the SDS website at sds.umbc.edu for registration information and office procedures.\nSDS email: disAbility@umbc.edu (disability at umbc dot edu)\nSDS phone: 410-455-2459\nIf you will be using SDS approved accommodations in this class, please contact the instructor to discuss implementation of the accommodations. During remote instruction requirements due to COVID, communication and flexibility will be essential for success.\n\n\nSexual Assault, Sexual Harassment, and Gender Based Violence and Discrimination\nUMBC Policy in addition to federal and state law (to include Title IX) prohibits discrimination and harassment on the basis of sex, sexual orientation, and gender identity in University programs and activities. Any student who is impacted by sexual harassment, sexual assault, domestic violence, dating violence, stalking, sexual exploitation, gender discrimination, pregnancy discrimination, gender-based harassment, or related retaliation should contact the University’s Title IX Coordinator to make a report and/or access support and resources. The Title IX Coordinator can be reached at titleixcoordinator at umbc dot edu or 410-455-1717.\nYou can access support and resources even if you do not want to take any further action. You will not be forced to file a formal complaint or police report. Please be aware that the University may take action on its own if essential to protect the safety of the community.\nIf you are interested in making a report, please use the Online Reporting/Referral Form.  Please note that, if you report anonymously, the University’s ability to respond will be limited.\n\nNotice that Faculty and Teaching Assistants are Responsible Employees with Mandatory Reporting Obligations\nAll faculty members and teaching assistants are considered Responsible Employees, per UMBC’s Policy on Sexual Misconduct, Sexual Harassment, and Gender Discrimination. Faculty and teaching assistants therefore required to report all known information regarding alleged conduct that may be a violation of the Policy to the Title IX Coordinator, even if a student discloses an experience that occurred before attending UMBC and/or an incident that only involves people not affiliated with UMBC.  Reports are required regardless of the amount of detail provided and even in instances where support has already been offered or received.\nWhile faculty members want to encourage you to share information related to your life experiences through discussion and written work, students should understand that faculty are required to report past and present sexual harassment, sexual assault, domestic and dating violence, stalking, and gender discrimination that is shared with them to the Title IX Coordinator so that the University can inform students of their rights, resources, and support.  While you are encouraged to do so, you are not obligated to respond to outreach conducted as a result of a report to the Title IX Coordinator.\nIf you need to speak with someone in confidence, who does not have an obligation to report to the Title IX Coordinator, UMBC has a number of Confidential Resources available to support you: \nRetriever Integrated Health (Main Campus): 410-455-2472; Monday – Friday 8:30 a.m. – 5 p.m.; For After-Hours Support, Call 988.\nCenter for Counseling and Well-Being (Shady Grove Campus): 301-738-6273; Monday-Thursday 10:00a.m. – 7:00 p.m. and Friday 10:00 a.m. – 2:00 p.m. (virtual) Online Appointment Request Form\nPastoral Counseling via The Gathering Space for Spiritual Well-Being: 410-455-6795; i3b at umbc dot edu; Monday – Friday 8:00 a.m. – 10:00 p.m.\n\n\nOther Resources\nWomen’s Center (open to students of all genders): 410-455-2714; womenscenter at umbc dot edu; Monday – Thursday 9:30 a.m. – 5:00 p.m. and Friday 10:00 a.m. – 4 p.m.\nShady Grove Student Resources, Maryland Resources, National Resources.\n\n\nChild Abuse and Neglect\nPlease note that Maryland law and UMBC policy require that faculty report all disclosures or suspicions of child abuse or neglect to the Department of Social Services and_/_or the police even if the person who experienced the abuse or neglect is now over 18.\n\n\n\nPregnant and Parenting Students\nUMBC’s Policy on Sexual Misconduct, Sexual Harassment and Gender Discrimination expressly prohibits all forms of discrimination and harassment on the basis of sex, including pregnancy. Resources for pregnant, parenting and breastfeeding students are available through the University’s Office of Equity and Civil Rights.  Pregnant and parenting students are encouraged to contact the Title IX Coordinator to discuss plans and ensure ongoing access to their academic program with respect to a leave of absence – returning following leave, or any other accommodation that may be needed related to pregnancy, childbirth, adoption, breastfeeding, and/or the early months of parenting.\nIn addition, students who are pregnant and have an impairment related to their pregnancy that qualifies as disability under the ADA may be entitled to accommodations through the Office of Student Disability Services.\n\n\nReligious Observances & Accommodations\nUMBC Policy provides that students should not be penalized because of observances of their religious beliefs, and that students shall be given an opportunity, whenever feasible, to make up within a reasonable time any academic assignment that is missed due to individual participation in religious observances. It is the responsibility of the student to inform the instructor of any intended absences or requested modifications for religious observances in advance, and as early as possible. For questions or guidance regarding religious observances and accommodations, please contact the Office of Equity and Civil Rights at ecr at umbc dot edu.\n\n\nHate, Bias, Discrimination and Harassment\nUMBC values safety, cultural and ethnic diversity, social responsibility, lifelong learning, equity, and civic engagement.\nConsistent with these principles, UMBC Policy prohibits discrimination and harassment in its educational programs and activities or with respect to employment terms and conditions based on race, creed, color, religion, sex, gender, pregnancy, ancestry, age, gender identity or expression, national origin, veterans status, marital status, sexual orientation, physical or mental disability, or genetic information.\nStudents (and faculty and staff) who experience discrimination, harassment, hate, or bias based upon a protected status or who have such matters reported to them should use the online reporting/referral form to report discrimination, hate, or bias incidents. You may report incidents that happen to you anonymously. Please note that, if you report anonymously, the University’s ability to respond may be limited.\n\n\nUMBC Writing Center\nThe Academic Success Center offers free writing assistance through our Writing Center, which is located on the first floor of the Library. We also offer online and asynchronous tutoring. Writing tutors are students like you who receive ongoing training to stay up-to-date on the best tutoring techniques. To make an appointment, please visit http://academicsuccess.umbc.edu/writing-center",
    "crumbs": [
      "Overview",
      "Syllabus"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References by topic",
    "section": "",
    "text": "This is most of what’s in my Zotero bibliography for this class arranged by topic.\n\nMain texts\nFoundational books and chapters\n\n\nCairo (2019)\nWickham et al. (2023)\nWilke (2019)\nYau (2013)\n\n\n\n\n\nCairo, A. (2019). How charts lie: Getting smarter about visual information (First edition). W. W. Norton & Company.\n\n\nWickham, H., Çetinkaya-Rundel, M., & Grolemund, G. (2023). R for data science: Import, tidy, transform, visualize, and model data (2nd edition). O’Reilly. https://r4ds.hadley.nz/\n\n\nWilke, C. (2019). Fundamentals of data visualization: A primer on making informative and compelling figures (First edition). O’Reilly. https://clauswilke.com/dataviz/\n\n\nYau, N. (2013). Data points: Visualization that means something. John Wiley & Sons, Inc.\n\n\n\n\nGeneral references\nReferences, frameworks, and grammar of graphics\n\n\nBBC (2010)\nCogley & Setlur (2022)\nChang (2018)\nDu Bois et al. (2018)\nD’Ignazio & Klein (2020)\nD’Ignazio (2015)\nGilmore (2023)\nKirk (2016)\nKosara (2019)\nMunzner (2014)\nRibecca (2024)\nSchwabish (n.d.-a)\nSchwabish (n.d.-b)\nSchwabish (2021)\nWickham et al. (2023)\nWickham (2010)\nWilke (2019)\n\n\n\n\n\nBBC. (2010). Hans Rosling’s 200 Countries, 200 Years, 4 Minutes - The Joy of Stats - BBC Four. https://www.youtube.com/watch?v=jbkSRLYSojo\n\n\nChang, W. (2018). R graphics cookbook: Practical recipes for visualizing data (Second edition). O’Reilly. https://r-graphics.org/\n\n\nCogley, B., & Setlur, V. (2022). Functional Aesthetics for Data Visualization. John Wiley and Sons.\n\n\nD’Ignazio, C. (2015). What would feminist data visualization look like? https://civic.mit.edu/feminist-data-visualization.html\n\n\nD’Ignazio, C., & Klein, L. F. (2020). Data feminism. The MIT Press. https://data-feminism.mitpress.mit.edu/\n\n\nDu Bois, W. E. B., Battle-Baptiste, W., & Rusert, B. (2018). W.E.B Du Bois’s data portraits: Visualizing Black America (First edition). The W.E.B. Du Bois Center At the University of Massachusetts Amherst ; Princeton Architectural Press.\n\n\nGilmore, R. W. (2023). Abolition Geography.\n\n\nKirk, A. (2016). Data visualisation: A handbook for data driven design. SAGE.\n\n\nKosara, R. (2019). The DataSaurus, Anscombe’s Quartet, and why summary statistics need to be taken with a grain of salt. https://www.youtube.com/watch?v=RbHCeANCbW0\n\n\nMunzner, T. (2014). Visualization Analysis and Design. A K Peters/CRC Press. https://doi.org/10.1201/b17511\n\n\nRibecca, S. (2024). The Data Visualisation Catalogue. https://datavizcatalogue.com/\n\n\nSchwabish, J. (n.d.-a). Catherine D’Ignazio and Lauren Klein (142). Retrieved January 30, 2024, from https://policyviz.com/podcast/episode-142-catherine-dignazio-and-lauren-klein/\n\n\nSchwabish, J. (n.d.-b). Sarah Williams (191). Retrieved January 18, 2024, from https://policyviz.com/podcast/episode-191-sarah-williams/\n\n\nSchwabish, J. (2021). Better Data Visualizations : A Guide for Scholars, Researchers, and Wonks. Columbia University Press.\n\n\nWickham, H. (2010). A Layered Grammar of Graphics. Journal of Computational and Graphical Statistics, 19(1), 3–28. https://doi.org/10.1198/jcgs.2009.07098\n\n\nWickham, H., Çetinkaya-Rundel, M., & Grolemund, G. (2023). R for data science: Import, tidy, transform, visualize, and model data (2nd edition). O’Reilly. https://r4ds.hadley.nz/\n\n\nWilke, C. (2019). Fundamentals of data visualization: A primer on making informative and compelling figures (First edition). O’Reilly. https://clauswilke.com/dataviz/\n\n\n\n\nAesthetics & styling\nColor, visual perception, annotations, text, and styleguides\n\n\nViz Palette (2019)\nAisch (2019)\nBertini & Stefaner (n.d.)\nBrewer (2006)\nGramazio et al. (2017)\nDatawrapper (2021)\nFrance (2020)\nHeer & Bostock (2010)\nKebonye et al. (2023)\nKim et al. (2021)\nKirk (2015)\nLiu & Heer (2018)\nMuth (2022)\nMuth (2024c)\nMuth (2024b)\nMuth (2024a)\nSkau & Kosara (2016)\nSetlur & Stone (2016)\nUrban Institute (2023)\nWorld Health Organization (2023)\n\n\n\n\n\nAisch, G. (2019). Chroma.js palette helper. https://gka.github.io/palettes\n\n\nBertini, E., & Stefaner, M. (n.d.). Color with Karen Schloss (119). Retrieved February 7, 2024, from https://datastori.es/119-color-with-karen-schloss/\n\n\nBrewer, C. A. (2006). Basic Mapping Principles for Visualizing Cancer Data Using Geographic Information Systems (GIS). American Journal of Preventive Medicine, 30(2), S25–S36. https://doi.org/10.1016/j.amepre.2005.09.007\n\n\nDatawrapper. (2021). What to consider when choosing colors for data visualization. https://academy.datawrapper.de/article/140-what-to-consider-when-choosing-colors-for-data-visualization\n\n\nFrance, T. (2020). Choosing Fonts for Your Data Visualization, Nightingale. In Nightingale. https://nightingaledvs.com/choosing-fonts-for-your-data-visualization/\n\n\nGramazio, C. C., Laidlaw, D. H., & Schloss, K. B. (2017). Colorgorical: Creating discriminable and preferable color palettes for information visualization. IEEE Transactions on Visualization and Computer Graphics. https://doi.org/10.1109/TVCG.2016.2598918\n\n\nHeer, J., & Bostock, M. (2010). Crowdsourcing graphical perception: Using mechanical turk to assess visualization design. Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, 203–212. https://doi.org/10.1145/1753326.1753357\n\n\nKebonye, N. M., Agyeman, P. C., Seletlo, Z., & Eze, P. N. (2023). On exploring bivariate and trivariate maps as visualization tools for spatial associations in digital soil mapping: A focus on soil properties. Precision Agriculture, 24(2), 511–532. https://doi.org/10.1007/s11119-022-09955-7\n\n\nKim, D. H., Setlur, V., & Agrawala, M. (2021). Towards Understanding How Readers Integrate Charts and Captions: A Case Study with Line Charts. Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems, 1–11. https://doi.org/10.1145/3411764.3445443\n\n\nKirk, A. (2015). Make grey your best friend. In Visualising Data. https://visualisingdata.com/2015/01/make-grey-best-friend/\n\n\nLiu, Y., & Heer, J. (2018). Somewhere Over the Rainbow: An Empirical Assessment of Quantitative Colormaps. Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems, 1–12. https://doi.org/10.1145/3173574.3174172\n\n\nMuth, L. C. (2022). What to consider when using text in data visualizations. In Datawrapper. https://blog.datawrapper.de/text-in-data-visualizations/\n\n\nMuth, L. C. (2024a). How to choose a color palette for choropleth maps. In Datawrapper. https://blog.datawrapper.de/how-to-choose-a-color-palette-for-choropleth-maps/\n\n\nMuth, L. C. (2024b). How to choose an interpolation for your color scale - Datawrapper Blog. In Datawrapper. https://blog.datawrapper.de/interpolation-for-color-scales-and-maps/\n\n\nMuth, L. C. (2024c). Which fonts to use for your charts and tables. In Datawrapper. https://blog.datawrapper.de/fonts-for-data-visualization/\n\n\nSetlur, V., & Stone, M. C. (2016). A Linguistic Approach to Categorical Color Assignment for Data Visualization. IEEE Transactions on Visualization and Computer Graphics, 22(1), 698–707. https://doi.org/10.1109/TVCG.2015.2467471\n\n\nSkau, D., & Kosara, R. (2016). Arcs, Angles, or Areas: Individual Data Encodings in Pie and Donut Charts. Computer Graphics Forum, 35(3), 121–130. https://doi.org/10.1111/cgf.12888\n\n\nUrban Institute. (2023). Urban Institute Data Visualization style guide. http://urbaninstitute.github.io/graphics-styleguide/\n\n\nViz Palette. (2019). https://projects.susielu.com/viz-palette\n\n\nWorld Health Organization. (2023). WHO Data Design Language v.0.9.2. https://apps.who.int/gho/data/design-language/\n\n\n\n\nUnderstanding data\nDecision-making, uncertainty, missing data, and logical fallacies\n\n\nAisch (2016)\nBertini & Stefaner (n.d.-a)\nBertini & Stefaner (n.d.-b)\nCox et al. (2022)\nCorrell & Gleicher (2014)\nCorrell et al. (2018)\nEtter (2023)\nHamel et al. (2020)\nHamel et al. (2021)\nKay et al. (2016)\nKrackov & Marikos (2021)\nKay (2024)\nKirk (2016)\nKirk (n.d.)\nLee et al. (2021)\nmimimimimi (2024)\nNation (2024)\nNyame-Mensah (2022)\nPillai et al. (2024)\nSadler (2016)\nSchwabish (n.d.)\nSilver (2015)\nSimeoni (2023)\nSmith (2023)\n\n\n\n\n\nAisch, G. (2016). Why we used jittery gauges in our live election forecast. In vis4.net. https://vis4.net/blog/jittery-gauges-election-forecast\n\n\nBertini, E., & Stefaner, M. (n.d.-a). Cognitive Bias and Visualization with Evanthia Dimara (116). Retrieved February 14, 2024, from https://datastori.es/116-cognitive-bias-and-visualization-with-evanthia-dimara/\n\n\nBertini, E., & Stefaner, M. (n.d.-b). Visualizing Uncertainty with Jessica Hullman and Matthew Kay (134). Retrieved January 30, 2024, from https://datastori.es/134-visualizing-uncertainty-with-jessica-hullman-and-matthew-kay/\n\n\nCorrell, M., & Gleicher, M. (2014). Error Bars Considered Harmful: Exploring Alternate Encodings for Mean and Error. IEEE Transactions on Visualization and Computer Graphics, 20(12), 2142–2151. https://doi.org/10.1109/TVCG.2014.2346298\n\n\nCorrell, M., Moritz, D., & Heer, J. (2018). Value-Suppressing Uncertainty Palettes. Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems, 1–11. https://doi.org/10.1145/3173574.3174216\n\n\nCox, C., Amin, K., Kates, J., & Published, J. M. (2022). Why Do Vaccinated People Represent Most COVID-19 Deaths Right Now? In KFF. https://www.kff.org/policy-watch/why-do-vaccinated-people-represent-most-covid-19-deaths-right-now/\n\n\nEtter, E. (2023). Data Visualization: A Subjective Lens on Reality. In Nightingale. https://nightingaledvs.com/data-visualization-a-subjective-lens-on-reality/\n\n\nHamel, L., Kirzinger, A., Muñana, C., & Published, M. B. (2020). KFF COVID-19 Vaccine Monitor: December 2020. In KFF. https://www.kff.org/coronavirus-covid-19/report/kff-covid-19-vaccine-monitor-december-2020/\n\n\nHamel, L., Lopes, L., & Published, M. B. (2021). KFF COVID-19 Vaccine Monitor: What Do We Know About Those Who Want to “Wait and See” Before Getting a COVID-19 Vaccine? In KFF. https://www.kff.org/coronavirus-covid-19/poll-finding/kff-covid-19-vaccine-monitor-wait-and-see/\n\n\nKay, M. (2024). Mjskay/ggdist. https://github.com/mjskay/ggdist\n\n\nKay, M., Kola, T., Hullman, J. R., & Munson, S. A. (2016). When (ish) is My Bus?: User-centered Visualizations of Uncertainty in Everyday, Mobile Predictive Systems. Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems, 5092–5103. https://doi.org/10.1145/2858036.2858558\n\n\nKirk, A. (n.d.). Alvin Chang (S2 E6). Retrieved February 2, 2024, from https://visualisingdata.com/2020/12/explore-explain-s2-e6-alvin-chang/\n\n\nKirk, A. (2016). Gauging election reaction. In Visualising Data. https://visualisingdata.com/2016/11/gauging-election-reaction/\n\n\nKrackov, A., & Marikos, S. (2021). Asterisk Nation: One Tribe’s Challenge to Find Data About its Population. In Nightingale. https://nightingaledvs.com/asterisk-nation-one-tribes-challenge-to-find-data-about-its-population/\n\n\nLee, C., Yang, T., Inchoco, G. D., Jones, G. M., & Satyanarayan, A. (2021). Viral Visualizations: How Coronavirus Skeptics Use Orthodox Data Practices to Promote Unorthodox Science Online. Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems, 1–18. https://doi.org/10.1145/3411764.3445211\n\n\nmimimimimi. (2024). MimiOnuoha/missing-datasets. https://github.com/MimiOnuoha/missing-datasets\n\n\nNation, Z. (2024). Zonination/perceptions. https://github.com/zonination/perceptions\n\n\nNyame-Mensah, A. (2022). When Oversimplification Obscures. In Nightingale. https://nightingaledvs.com/when-oversimplification-obscures/\n\n\nPillai, D., Artiga, S., Hamel, L., Schumacher, S., Kirzinger, A., Rao, A., & Published, A. K. (2024). Understanding the Diversity in the Asian Immigrant Experience in the U.S.: The 2023 KFF/LA Times Survey of Immigrants. In KFF. https://www.kff.org/racial-equity-and-health-policy/poll-finding/understanding-the-diversity-in-the-asian-immigrant-experience/\n\n\nSadler, R. C. (2016). How ZIP codes nearly masked the lead problem in Flint. In The Conversation. http://theconversation.com/how-zip-codes-nearly-masked-the-lead-problem-in-flint-65626\n\n\nSchwabish, J. (n.d.). Joe Sharpe and Mike Orwell (242). Retrieved February 13, 2024, from https://policyviz.com/podcast/episode-242-joe-sharpe-and-mike-orwell/\n\n\nSilver, N. (2015). The Most Diverse Cities Are Often The Most Segregated. In FiveThirtyEight. https://fivethirtyeight.com/features/the-most-diverse-cities-are-often-the-most-segregated/\n\n\nSimeoni, F. (2023). Querying the Quantification of Queer. In Nightingale. https://nightingaledvs.com/querying-the-quantification-of-the-queer/\n\n\nSmith, N. (2023). How not to be fooled by viral charts. https://www.noahpinion.blog/p/how-not-to-be-fooled-by-viral-charts\n\n\n\n\nStorytelling\nTelling a story and making a point\n\n\nFratczak (2023)\nHullman & Diakopoulos (2011)\nSeaberry (2018)\n\n\n\n\n\nFratczak, M. (2023). Can Datavis Make Unpalatable Data More Enjoyable? In Nightingale. https://nightingaledvs.com/can-datavis-make-unpalatable-data-more-enjoyable/\n\n\nHullman, J., & Diakopoulos, N. (2011). Visualization Rhetoric: Framing Effects in Narrative Visualization. IEEE Transactions on Visualization and Computer Graphics, 17(12), 2231–2240. https://doi.org/10.1109/TVCG.2011.255\n\n\nSeaberry, C. (2018). CT Data Story: Housing Segregation in Greater New Haven. DataHaven. https://ctdatahaven.org/reports/ct-data-story-housing-segregation-greater-new-haven\n\n\n\n\nSocial justice & ethics\nData viz for action in the real world\n\n\n“Dispersion & Disparity” Research Project Results (2023)\nAlderman & Inwood (2024)\nBocoupLLC (2017)\nElghany (2023)\nHahn (2023)\nHolder (2022)\nLevy-Rubinett (2020)\nMakulec (2020)\nNey (2023)\nJustice Policy Institute & Prison Policy Initiative (2022)\nUniversity of Richmond Digital Scholarship Lab (n.d.)\nThomas et al. (2020)\n\n\n\n\n\nAlderman, D. H., & Inwood, J. F. J. (2024). Black communities are using mapping to document and restore a sense of place. In The Conversation. http://theconversation.com/black-communities-are-using-mapping-to-document-and-restore-a-sense-of-place-221299\n\n\nBocoupLLC. (2017). A Data Point Walks Into a Bar: Designing Data For Empathy - Lisa Charlotte Rost. https://www.youtube.com/watch?v=8XgF-RmNwUc\n\n\n“Dispersion & Disparity” Research Project Results. (2023). https://3iap.com/dispersion-disparity-equity-centered-data-visualization-research-project-Wi-58RCVQNSz6ypjoIoqOQ/\n\n\nElghany, S. (2023). How Ethical Data Visualization Tells the Human Story. In Nightingale. https://nightingaledvs.com/ethical-data-visualization-tells-the-human-story/\n\n\nHahn, J. (2023). \"Data replicates the existing systems of power\" says Pulitzer Prize-winner Mona Chalabi. Dezeen. https://www.dezeen.com/2023/11/16/mona-chalabi-pulitzer-prize-winner/\n\n\nHolder, E. (2022). Unfair Comparisons: How Visualizing Social Inequality Can Make It Worse, Nightingale. In Nightingale. https://nightingaledvs.com/unfair-comparisons-how-visualizing-social-inequality-can-make-it-worse/\n\n\nJustice Policy Institute, & Prison Policy Initiative. (2022). Where people in prison come from: The geography of mass incarceration in Maryland. https://www.prisonpolicy.org/origin/md/2020/report.html\n\n\nLevy-Rubinett, I. (2020). With Great Visualization Comes Great Responsibility. In Nightingale. https://nightingaledvs.com/with-great-visualization-comes-great-responsibility/\n\n\nMakulec, A. (2020). Ten Considerations Before you Create another Chart about COVID-19. In Nightingale. https://medium.com/nightingale/ten-considerations-before-you-create-another-chart-about-covid-19-27d3bd691be8\n\n\nNey, J. (2023). Mapping Inequality Can Drive Social Impact. In Nightingale. https://nightingaledvs.com/mapping-inequality-can-drive-social-impact/\n\n\nThomas, T., Drewery, M., Greif, M., Kennedy, I., Ramiller, A., Toomet, O., & Hernandez, J. (2020). Baltimore Eviction Map. Eviction Research Network, UC Berkeley. https://evictionresearch.net/maryland/report/baltimore.html\n\n\nUniversity of Richmond Digital Scholarship Lab. (n.d.). Mapping Inequality: Redlining in New Deal America. In American Panorama: An Atlas of United States History. Retrieved November 10, 2022, from https://dsl.richmond.edu/panorama/redlining/\n\n\n\n\nSpatial data\nSpatial is special\n\n\nUS Census Bureau (2021)\nEricson (2011)\nSimmon (2024)\nNussbaumer Knaflic (n.d.)\nWiseman (2015)\nWong (2024)\n\n\n\n\n\n\n\n\n\n\nEricson, M. (2011). When Maps Shouldn’t Be Maps. In ericson.net. https://www.ericson.net/content/2011/10/when-maps-shouldnt-be-maps/\n\n\nNussbaumer Knaflic, C. (n.d.). Maps with Kenneth Field (41). Retrieved March 27, 2024, from https://storytellingwithdata.libsyn.com/storytelling-with-data-41-maps-with-kenneth-field\n\n\nSimmon, R. (2024). From Space to Story in Data Journalism, Nightingale. In Nightingale. https://nightingaledvs.com/from-space-to-story-in-data-journalism/\n\n\nUS Census Bureau. (2021). Appendix B: Measures of Residential Segregation. In Guidance for Housing Patterns Data Users. https://www.census.gov/topics/housing/housing-patterns/guidance/appendix-b.html\n\n\nWiseman, A. (2015). Bad Maps Are Everywhere These Days. Here’s How to Avoid Being Fooled. Bloomberg CityLab. https://www.bloomberg.com/news/articles/2015-06-25/how-to-avoid-being-fooled-by-bad-maps\n\n\nWong, D. (2024). The SAGE Handbook of Spatial Analysis. SAGE Publications, Ltd. https://doi.org/10.4135/9780857020130\n\n\n\n\n Back to top",
    "crumbs": [
      "References by topic"
    ]
  },
  {
    "objectID": "tips/exporting.html",
    "href": "tips/exporting.html",
    "title": "Exporting and distributing your work",
    "section": "",
    "text": "ggsave(filename = here::here(\"plots/wages_by_race_x_sex.png\"), \n       plot = wage_chart,\n       width = 5, height = 5, dpi = 300, bg = \"white\")",
    "crumbs": [
      "Tips & tricks",
      "Exporting and distributing your work"
    ]
  },
  {
    "objectID": "tips/exporting.html#tldr",
    "href": "tips/exporting.html#tldr",
    "title": "Exporting and distributing your work",
    "section": "",
    "text": "ggsave(filename = here::here(\"plots/wages_by_race_x_sex.png\"), \n       plot = wage_chart,\n       width = 5, height = 5, dpi = 300, bg = \"white\")",
    "crumbs": [
      "Tips & tricks",
      "Exporting and distributing your work"
    ]
  },
  {
    "objectID": "tips/exporting.html#exporting-your-work-when-why-how",
    "href": "tips/exporting.html#exporting-your-work-when-why-how",
    "title": "Exporting and distributing your work",
    "section": "Exporting your work: when, why & how",
    "text": "Exporting your work: when, why & how\n\nWhen & why\nSo far we’ve just left our charts in their notebooks. Sometimes that’s fine—you can use RMarkdown or Quarto notebooks to write a paper or report or build a website, then render the whole thing directly to another format. I do a lot of my work in Quarto notebooks, and these generally take one of three routes:\n\nEDA or drafts of sections of a project that’s being written elsewhere\n\nSpend just enough time adjusting figure sizes, layout, fonts, etc to be legible\nRender to a markdown document (set format: gfm, short for GitHub-flavored markdown)\nPush it to GitHub so my coworkers & I can reference it\nMost likely don’t save charts outside of the notebook\n\nFull standalone documents (blog posts, reports to distribute, webpages)\n\nSpend a lot of time getting figure sizes, layout, fonts, color, etc exactly right\nRender to something portable depending on the project type (pdf, html, docx)\nIf colleagues or clients might want separate copies of charts, export each chart into its own hi-res image file (usually pdf and png)\nIf clients might use charts for presentations, etc, attach our logo to the chart when exporting it\n\nChart-only projects not part of a larger document\n\nSpend a lot of time getting figure sizes, layout, fonts, color, etc exactly right\nMake sure charts are self-contained (annotations and titles give full picture, source in caption)\nRender to markdown to have reproducible reference to how charts were created\nExport each chart into its own hi-res image file (usually pdf and png; svg if working with graphic designers) with our logo\n\n\nThe first important thing to do to export your work is to make sure it renders. Documents like RMarkdown or Quarto need to be self-contained; if you assign some variable in your console, then reference it in your Quarto document, you’ll get an error that the variable is undefined. That’s because the knitting/rendering process by default happens in a separate environment. That is, the notebook only knows what you’ve told it directly. That’s by design to make your work reproducible. If you’re collaborating with someone else and they can’t reproduce your work, what was the point in doing it?\nRendering a document in certain formats (html, markdown) will actually save a local copy of each image. If I render a notebook called cool_analysis.qmd into cool_analysis.md, it will actually create a folder in the same directory called cool_analysis_files; in that folder will be all the images that are displayed in the markdown document. That’s because some formats, particularly text-based ones, don’t have a way of directly embedding images; they just reference an image file from somewhere else. Those images will be named after the code chunks that created them (one good reason to name your chunks!). They’ll have whatever characteristics were set either for the document defaults or for the specific chunk (dimensions, resolution).\n\n\nHow\nIf you need to distribute your images, it’s better to be intentional about how they’re saved.\n\nMake a folder where you write all your charts (unsurprisingly I usually call it “plots”).\nWrite your code to export the charts to that folder within code chunks so they’re always up to date. Don’t just do it one time in the console, make it part of your workflow.\nOpen the image in a separate program (your OS’s default image preview program is fine) to verify that things came out the way you expected and wanted. Zoom in to make sure the resolution is high enough.\nAdjust dimensions, etc as necessary.\n\nThe easiest way to export a ggplot-based chart is with ggplot2::ggsave:\n\nlibrary(ggplot2)\nlibrary(dplyr)\n\nsource(here::here(\"utils/plotting_utils.R\"))\n\nwage_chart &lt;- justviz::wages |&gt;\n  filter(dimension == \"race_x_sex\") |&gt;\n  mutate(race_eth = forcats::fct_reorder(race_eth, earn_q50, .fun = max),\n         name = forcats::fct_reorder(name, earn_q50, .fun = max, .desc = TRUE),\n         sex = forcats::fct_reorder(sex, earn_q50, .fun = mean, .desc = FALSE)) |&gt;\n  ggplot(aes(x = earn_q50, y = race_eth, group = race_eth)) +\n  geom_path(color = \"gray70\", size = 2) +\n  geom_point(aes(color = sex), size = 4) +\n  scale_x_continuous(labels = dollar_k) +\n  scale_color_manual(values = c(Men = qual_pal[3], Women = qual_pal[6])) +\n  facet_grid(rows = vars(name)) +\n  labs(title = \"Median individual earnings by race/ethnicity and sex\",\n       subtitle = \"Maryland adults ages 25+ working full time, 2021\",\n       x = NULL, y = NULL, color = NULL) +\n  theme_nice() +\n  theme(panel.grid.major.x = element_line(),\n        panel.grid.major.y = element_line(linetype = \"dotted\", color = \"gray50\"),\n        legend.position = \"bottom\")\n\nwage_chart\n\n\n\n\n\n\n\nggsave(filename = here::here(\"plots/wages_by_race_x_sex.png\"), \n       plot = wage_chart,\n       width = 5, height = 5, dpi = 300, bg = \"white\")\n\nNow there’s a chart saved to ./plots/wages_by_race_x_sex.png that is ready to post somewhere, use in a document outside of R, or distribute.\nDouble check that the file type is what you expected—if you don’t give ggsave a file type or graphics device to use, it will infer from the path’s extension (in this example, the path I gave has a “.png” extension, so it write out a PNG file). There are ways you can tinker with the graphics devices, but in general the default device used for the extension you choose is fine. 1\n1 The argument bg = \"white\" is necessary for PNG files because otherwise they’ll have a transparent background by default. This isn’t the case with other file types like PDF.2 I realized this the hard/annoying way a few years ago. I was working with a graphic designer, pushing plots to GitHub for her to edit. She thought I kept messing with them after I’d said they were ready for her. Finally I realized I had made randomly-placed labels with ggrepel but hadn’t set the seed, so every time I edited the notebook, the placement changed slightly, which got picked up in Git as me having changed the charts. When you set a seed, (pseudo)random numbers will be generated in the same sequence, avoiding this problem.One last tip: if anything in your chart depends on randomization (ggplot2::position_jitter or anything from the ggrepel package for example, or if you’re taking a random subset of data like with sample or dplyr::slice_sample) set a seed in the same chunk as the plot. Many of these functions will have an argument for this. If not, call set.seed(1) or some other number before the code that has the randomization. 2",
    "crumbs": [
      "Tips & tricks",
      "Exporting and distributing your work"
    ]
  },
  {
    "objectID": "tips/merging.html",
    "href": "tips/merging.html",
    "title": "Merging data",
    "section": "",
    "text": "library(dplyr)\nlibrary(justviz)\nlibrary(ggplot2)\nHere are some notes on merging data from different data frames. A lot of the functions here come from dplyr, including all the *_join ones.",
    "crumbs": [
      "Tips & tricks",
      "Merging data"
    ]
  },
  {
    "objectID": "tips/merging.html#types-of-joins",
    "href": "tips/merging.html#types-of-joins",
    "title": "Merging data",
    "section": "Types of joins",
    "text": "Types of joins\nThere are different types of joins that are defined by what data you want to keep and under what circumstances. These are consistent across many different languages (e.g. same terminology in R should apply in most/all SQL variants). The ones you’ll use most often are left joins and inner joins; when in doubt, a left join is safer than an inner join.\nThere’s an overly complicated chapter in R for Data Science on joins. There are some less complicated examples in the dplyr docs.\n\n\n\nAn illustration of joins\n\n\nImagine we’re joining two tables of data for counties A, B, C, D, and E, one row per county. The left table, housing, has housing information for each county but is missing County B. The right table, income, has income information for counties A, B, and E. That means there are a total of 5 counties, but only 2 of them are in both tables.\n\nLeft join will include every county that’s in housing, regardless of whether it’s also in income. There will be a row for income variables, but their values will be NA.\nInner join will include every county that’s in both housing and income.\nRight join is like left join: it will include every county that’s in income, regardless of whether it’s also in housing.\nFull join will include every county in either table.\n\n\nset.seed(1)\nhousing &lt;- data.frame(county = c(\"A\", \"C\", \"D\", \"E\"), \n                      homeownership = runif(4),\n                      vacancy = runif(4, min = 0, max = 0.1))\nincome &lt;- data.frame(county = c(\"A\", \"B\", \"E\"), \n                     poverty = runif(3))\n\nleft_join(housing, income, by = \"county\")\n\n\n\n\n\ncounty\nhomeownership\nvacancy\npoverty\n\n\n\n\nA\n0.2655087\n0.0201682\n0.6291140\n\n\nC\n0.3721239\n0.0898390\nNA\n\n\nD\n0.5728534\n0.0944675\nNA\n\n\nE\n0.9082078\n0.0660798\n0.2059746\n\n\n\n\n\ninner_join(housing, income, by = \"county\")\n\n\n\n\n\ncounty\nhomeownership\nvacancy\npoverty\n\n\n\n\nA\n0.2655087\n0.0201682\n0.6291140\n\n\nE\n0.9082078\n0.0660798\n0.2059746\n\n\n\n\n\nright_join(housing, income, by = \"county\")\n\n\n\n\n\ncounty\nhomeownership\nvacancy\npoverty\n\n\n\n\nA\n0.2655087\n0.0201682\n0.6291140\n\n\nE\n0.9082078\n0.0660798\n0.2059746\n\n\nB\nNA\nNA\n0.0617863\n\n\n\n\n\nfull_join(housing, income, by = \"county\")\n\n\n\n\n\ncounty\nhomeownership\nvacancy\npoverty\n\n\n\n\nA\n0.2655087\n0.0201682\n0.6291140\n\n\nC\n0.3721239\n0.0898390\nNA\n\n\nD\n0.5728534\n0.0944675\nNA\n\n\nE\n0.9082078\n0.0660798\n0.2059746\n\n\nB\nNA\nNA\n0.0617863\n\n\n\n\n\n\nThere are other joins that might be useful for filtering, but that don’t add any new columns. Semi joins return the rows of the left table that have a match in the right table, and anti joins return the rows of the left table that do not have a match in the right table. If you were making separate charts on housing and income, but wanted your housing chart to only include counties that are also in your income data, semi join would help.\n\nsemi_join(housing, income, by = \"county\")\n\n\n\n\n\ncounty\nhomeownership\nvacancy\n\n\n\n\nA\n0.2655087\n0.0201682\n\n\nE\n0.9082078\n0.0660798",
    "crumbs": [
      "Tips & tricks",
      "Merging data"
    ]
  },
  {
    "objectID": "tips/merging.html#joining-justviz-datasets",
    "href": "tips/merging.html#joining-justviz-datasets",
    "title": "Merging data",
    "section": "Joining justviz datasets",
    "text": "Joining justviz datasets\n\nacs_tract &lt;- acs |&gt; filter(level == \"tract\")\n\nhead(acs_tract)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nlevel\ncounty\nname\ntotal_pop\nwhite\nblack\nlatino\nasian\nother_race\ndiversity_idx\nforeign_born\ntotal_hh\nhomeownership\ntotal_cost_burden\ntotal_severe_cost_burden\nowner_cost_burden\nowner_severe_cost_burden\nrenter_cost_burden\nrenter_severe_cost_burden\nno_vehicle_hh\nmedian_hh_income\nages25plus\nless_than_high_school\nhigh_school_grad\nsome_college_or_aa\nbachelors\ngrad_degree\npov_status_determined\npoverty\nlow_income\narea_sqmi\npop_density\n\n\n\n\ntract\nAllegany County\n24001000100\n3474\n0.98\n0.00\n0.01\n0.00\n0.01\n0.0826466\n0.01\n1577\n0.78\n0.18\n0.08\n0.12\n0.07\n0.39\n0.14\n0.06\n56232\n2671\n0.09\n0.47\n0.28\n0.07\n0.08\n3461\n0.12\n0.35\n187.932766\n18.48533\n\n\ntract\nAllegany County\n24001000200\n4052\n0.75\n0.19\n0.02\n0.00\n0.03\n0.4674189\n0.03\n1390\n0.86\n0.20\n0.12\n0.18\n0.11\n0.33\n0.18\n0.04\n66596\n3255\n0.15\n0.49\n0.24\n0.08\n0.05\n2949\n0.11\n0.30\n48.072019\n84.29020\n\n\ntract\nAllegany County\n24001000500\n2304\n0.66\n0.19\n0.05\n0.01\n0.07\n0.6236263\n0.04\n683\n0.60\n0.20\n0.04\n0.09\n0.02\n0.36\n0.07\n0.13\n47781\n1283\n0.09\n0.37\n0.38\n0.11\n0.06\n1777\n0.27\n0.51\n4.422954\n520.91879\n\n\ntract\nAllegany County\n24001000600\n3005\n0.91\n0.02\n0.01\n0.00\n0.07\n0.2348727\n0.00\n1374\n0.70\n0.20\n0.09\n0.14\n0.04\n0.36\n0.23\n0.10\n48607\n2141\n0.07\n0.35\n0.35\n0.13\n0.09\n2910\n0.14\n0.37\n1.582466\n1898.93501\n\n\ntract\nAllegany County\n24001000700\n3233\n0.93\n0.02\n0.01\n0.00\n0.04\n0.1844055\n0.02\n1462\n0.49\n0.37\n0.16\n0.25\n0.07\n0.48\n0.24\n0.25\n36090\n2045\n0.13\n0.38\n0.33\n0.08\n0.08\n3217\n0.28\n0.58\n0.712350\n4538.49941\n\n\ntract\nAllegany County\n24001000800\n1932\n0.89\n0.01\n0.06\n0.01\n0.03\n0.2934502\n0.02\n786\n0.48\n0.52\n0.25\n0.35\n0.19\n0.68\n0.30\n0.17\n27130\n1253\n0.14\n0.48\n0.28\n0.06\n0.04\n1926\n0.35\n0.61\n1.263143\n1529.51773\n\n\n\n\n\nhead(ej_natl)\n\n\n\n\n\ntract\nindicator\nvalue_ptile\nd2_ptile\nd5_ptile\n\n\n\n\n24001000100\npm25\n20\n18\n25\n\n\n24001000100\nozone\n7\n7\n9\n\n\n24001000100\ndiesel\n19\n18\n24\n\n\n24001000100\nair_cancer\n6\n20\n31\n\n\n24001000100\nair_respiratory\n4\n14\n20\n\n\n24001000100\nreleases_to_air\n44\n34\n50\n\n\n\n\n\n\nACS data has several geographies, including census tracts (I’ve subset for just tract data). Their ID (GEOID, or FIPS codes) are in the column name. The EPA data is only by tract, and its column of IDs is labeled tract. So we’ll be joining name from acs_tract with tract from ej_natl (this is the nationwide rankings, not the state ones).\n\nn_distinct(acs_tract$name)\n\n[1] 1460\n\nn_distinct(ej_natl$tract)\n\n[1] 1475\n\n\nThere are 15 tracts that are included in the EPA data but not the ACS data. That’s because those are tracts with no population that I dropped from the ACS table when I made it. I can check up on that with an anti-join (not running this here but it confirms that these are all zero-population tracts).\n\npop &lt;- tidycensus::get_acs(\"tract\", table = \"B01003\", state = \"MD\", year = 2022)\n\nanti_join(ej_natl, acs_tract, by = c(\"tract\" = \"name\")) |&gt;\n  distinct(tract) |&gt;\n  inner_join(pop, by = c(\"tract\" = \"GEOID\"))\n\nThere’s another hiccup for merging data here: the ACS data is in a wide format (each variable has its own column), while the EPA data is in a long format (one column gives the indicator, then different types of values have their own columns). Those formatting differences could be awkward because you’d end up with some values repeated. The easiest thing to do is select just the data you’re interested in, either by selecting certain columns or filtering rows, then reshape, then join.\nLet’s say I’m interested in the relationship, if any, between demographics and a few waste-related risk factors (proximity to wastewater, hazardous waste, and superfund sites). I’ll filter ej_natl for just those 2 indicators and reshape it so the columns have the value percentiles for each of those two risk factors (not the adjusted percentiles). Then I’ll select the columns I want from acs, then join them.\nThe tidyr::pivot_wider and tidyr::pivot_longer functions can be confusing, but there are some good examples in the docs and a lot of Stack Overflow posts on them. Basically here I’m reshaping from a long shape to a wide shape, so I’ll use pivot_wider.\n\n# in practice I would do this all at once, but want to keep the steps separate\n# so they're more visible\nwaste_long &lt;- ej_natl |&gt;\n  filter(indicator %in% c(\"haz_waste\", \"superfund\", \"wastewater\"))\n\nhead(waste_long)\n\n\n\n\n\ntract\nindicator\nvalue_ptile\nd2_ptile\nd5_ptile\n\n\n\n\n24001000100\nsuperfund\n54\n43\n62\n\n\n24001000100\nhaz_waste\n10\n10\n12\n\n\n24001000100\nwastewater\n53\n40\n58\n\n\n24001000200\nsuperfund\n89\n73\n81\n\n\n24001000200\nhaz_waste\n35\n43\n44\n\n\n24001000200\nwastewater\n66\n65\n69\n\n\n\n\n\n# id_cols are the anchor of the pivoting\n# only using value_ptile as a value column, not scaled ones\nwaste_wide &lt;- waste_long |&gt; \n  tidyr::pivot_wider(id_cols = tract, \n                     names_from = indicator, \n                     values_from = value_ptile)\n\nhead(waste_wide)\n\n\n\n\n\ntract\nsuperfund\nhaz_waste\nwastewater\n\n\n\n\n24001000100\n54\n10\n53\n\n\n24001000200\n89\n35\n66\n\n\n24001000500\n90\n42\n7\n\n\n24001000600\n93\n50\n63\n\n\n24001000700\n92\n60\n68\n\n\n24001000800\n89\n78\n75\n\n\n\n\n\n\nThen the columns I’m interested in from the ACS data (For good measure, I’ll also add a variable to indicate that a tract is in Baltimore city):\n\nacs_demo &lt;- acs_tract |&gt;\n  select(name, county, total_pop, white, poverty, foreign_born, homeownership) |&gt;\n  mutate(is_baltimore = ifelse(county == \"Baltimore city\", \"Baltimore\", \"Other counties\"))\n\nhead(acs_demo)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nname\ncounty\ntotal_pop\nwhite\npoverty\nforeign_born\nhomeownership\nis_baltimore\n\n\n\n\n24001000100\nAllegany County\n3474\n0.98\n0.12\n0.01\n0.78\nOther counties\n\n\n24001000200\nAllegany County\n4052\n0.75\n0.11\n0.03\n0.86\nOther counties\n\n\n24001000500\nAllegany County\n2304\n0.66\n0.27\n0.04\n0.60\nOther counties\n\n\n24001000600\nAllegany County\n3005\n0.91\n0.14\n0.00\n0.70\nOther counties\n\n\n24001000700\nAllegany County\n3233\n0.93\n0.28\n0.02\n0.49\nOther counties\n\n\n24001000800\nAllegany County\n1932\n0.89\n0.35\n0.02\n0.48\nOther counties\n\n\n\n\n\n\nSo each of these two data frames has a column of tract IDs, and several columns of relevant values. I only want tracts that are in both datasets, so I’ll use an inner join.\n\nwaste_x_demo &lt;- inner_join(acs_demo, waste_wide, by = c(\"name\" = \"tract\"))\n\nhead(waste_x_demo)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nname\ncounty\ntotal_pop\nwhite\npoverty\nforeign_born\nhomeownership\nis_baltimore\nsuperfund\nhaz_waste\nwastewater\n\n\n\n\n24001000100\nAllegany County\n3474\n0.98\n0.12\n0.01\n0.78\nOther counties\n54\n10\n53\n\n\n24001000200\nAllegany County\n4052\n0.75\n0.11\n0.03\n0.86\nOther counties\n89\n35\n66\n\n\n24001000500\nAllegany County\n2304\n0.66\n0.27\n0.04\n0.60\nOther counties\n90\n42\n7\n\n\n24001000600\nAllegany County\n3005\n0.91\n0.14\n0.00\n0.70\nOther counties\n93\n50\n63\n\n\n24001000700\nAllegany County\n3233\n0.93\n0.28\n0.02\n0.49\nOther counties\n92\n60\n68\n\n\n24001000800\nAllegany County\n1932\n0.89\n0.35\n0.02\n0.48\nOther counties\n89\n78\n75\n\n\n\n\n\n\n\nggplot(waste_x_demo, aes(x = poverty, y = haz_waste)) +\n  geom_point(aes(color = is_baltimore, shape = is_baltimore), alpha = 0.7) +\n  scale_color_manual(values = c(\"purple\", \"gray60\")) +\n  scale_shape_manual(values = c(\"circle\", \"triangle\"))\n\n\n\n\n\n\n\n\nIs there a pattern? Maybe, maybe not, but now we know how to investigate it. There’s definitely something up with Baltimore though.",
    "crumbs": [
      "Tips & tricks",
      "Merging data"
    ]
  },
  {
    "objectID": "tips/merging.html#joining-with-a-crosswalk",
    "href": "tips/merging.html#joining-with-a-crosswalk",
    "title": "Merging data",
    "section": "Joining with a crosswalk",
    "text": "Joining with a crosswalk\nThis follows the example in the docs for the justviz::xwalk_tract_10_to_20 dataset. This has columns for 2010 tract GEOIDs, 2020 GEOIDs, and weights to use for converting values based on 2010 geographies to those based on 2020 geographies. This is necessary if you want to join the CDC health data with the ACS or EJSCREEN data, because the CDC data uses 2010 geographies. Let’s expand the example from the docs to merge health indicators with the previous demographic and environmental ones—say we want to know if there’s a relationship between adult asthma rates and waste-related risks.\n\n# these are based on 2010 geographies\nasthma10 &lt;- cdc |&gt;\n  filter(level == \"tract\",\n         indicator == \"Current asthma\")\n\nhead(asthma10)\n\n\n\n\n\nlevel\nyear\nlocation\nindicator\nvalue\npop\n\n\n\n\ntract\n2021\n24005408200\nCurrent asthma\n8.6\n1853\n\n\ntract\n2021\n24003706700\nCurrent asthma\n8.8\n4890\n\n\ntract\n2021\n24015030506\nCurrent asthma\n11.1\n8223\n\n\ntract\n2021\n24005401301\nCurrent asthma\n11.3\n3891\n\n\ntract\n2021\n24005411302\nCurrent asthma\n9.3\n3310\n\n\ntract\n2021\n24005402403\nCurrent asthma\n11.3\n2283\n\n\n\n\n\n# calculate a weighted mean of 2010 tract rates, weighted sum of 2010 counts (pop)\n# both grouped by 2020 tract\nasthma20 &lt;- asthma10 |&gt;\n  inner_join(xwalk_tract_10_to_20, by = c(\"location\" = \"tract10\")) |&gt;\n  group_by(tract20) |&gt;\n  # don't know why cdc rates aren't divided by 100 so do it now\n  summarise(asthma = weighted.mean(value, weight) / 100,\n            adult_pop = sum(pop * weight)) |&gt;\n  ungroup()\n\nwaste_x_demo_x_cdc &lt;- waste_x_demo |&gt;\n  left_join(asthma20, by = c(\"name\" = \"tract20\"))\n\nhead(waste_x_demo_x_cdc)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nname\ncounty\ntotal_pop\nwhite\npoverty\nforeign_born\nhomeownership\nis_baltimore\nsuperfund\nhaz_waste\nwastewater\nasthma\nadult_pop\n\n\n\n\n24001000100\nAllegany County\n3474\n0.98\n0.12\n0.01\n0.78\nOther counties\n54\n10\n53\n0.1040000\n3718.000\n\n\n24001000200\nAllegany County\n4052\n0.75\n0.11\n0.03\n0.86\nOther counties\n89\n35\n66\n0.0920000\n4564.000\n\n\n24001000500\nAllegany County\n2304\n0.66\n0.27\n0.04\n0.60\nOther counties\n90\n42\n7\n0.1149507\n2735.419\n\n\n24001000600\nAllegany County\n3005\n0.91\n0.14\n0.00\n0.70\nOther counties\n93\n50\n63\n0.1038916\n2979.443\n\n\n24001000700\nAllegany County\n3233\n0.93\n0.28\n0.02\n0.49\nOther counties\n92\n60\n68\n0.1160000\n3387.000\n\n\n24001000800\nAllegany County\n1932\n0.89\n0.35\n0.02\n0.48\nOther counties\n89\n78\n75\n0.1190000\n2213.000\n\n\n\n\n\n\nSo now we’ve got a data frame with some demographic indicators, some environmental risks, and rates of a major health outcome. Expanding the previous question to include asthma rates:\n\nggplot(waste_x_demo_x_cdc, aes(x = haz_waste, y = asthma)) +\n  geom_point(aes(color = is_baltimore, shape = is_baltimore), alpha = 0.7) +\n  scale_color_manual(values = c(\"purple\", \"gray60\")) +\n  scale_shape_manual(values = c(\"circle\", \"triangle\"))\n\n\n\n\n\n\n\n\nSo there does seem to be a correlation between the two variables, with Baltimore city tracts standing out on both. 1\n1 It’s worth noting that the CDC data uses crude rates, not age-adjusted ones, which we don’t have to get into but it becomes important with age-related health conditions like cancer. That’s why I chose asthma instead.",
    "crumbs": [
      "Tips & tricks",
      "Merging data"
    ]
  },
  {
    "objectID": "tips/merging.html#joining-data-with-different-shapes",
    "href": "tips/merging.html#joining-data-with-different-shapes",
    "title": "Merging data",
    "section": "Joining data with different shapes",
    "text": "Joining data with different shapes\nLast, an example of a situation where you might want to join one long data frame with one wide one. If I want to compare poverty rate to each of the waste risk factors, and create small multiple charts, I’ll need a variable to facet on. So I’ll go back to waste_long and join that to the ACS data. Because of those differing shapes, the ACS data will be repeated, once for each waste indicator.\n\nwaste_long_x_demo &lt;- waste_long |&gt;\n  inner_join(acs_demo, by = c(\"tract\" = \"name\")) |&gt;\n  select(tract, is_baltimore, indicator, value_ptile, poverty)\n\nhead(waste_long_x_demo)\n\n\n\n\n\ntract\nis_baltimore\nindicator\nvalue_ptile\npoverty\n\n\n\n\n24001000100\nOther counties\nsuperfund\n54\n0.12\n\n\n24001000100\nOther counties\nhaz_waste\n10\n0.12\n\n\n24001000100\nOther counties\nwastewater\n53\n0.12\n\n\n24001000200\nOther counties\nsuperfund\n89\n0.11\n\n\n24001000200\nOther counties\nhaz_waste\n35\n0.11\n\n\n24001000200\nOther counties\nwastewater\n66\n0.11\n\n\n\n\n\n\nAs you can see, the first of these tracts (24001000100) takes up 3 rows, one for each of the 3 waste indicators. To match it, that tract’s poverty rate (0.12) is repeated 3 times.\n\nggplot(waste_long_x_demo, aes(x = value_ptile, y = poverty)) +\n  geom_point(aes(color = is_baltimore, shape = is_baltimore), alpha = 0.7) +\n  scale_color_manual(values = c(\"purple\", \"gray60\")) +\n  scale_shape_manual(values = c(\"circle\", \"triangle\")) +\n  facet_wrap(vars(indicator), nrow = 1) +\n  theme(legend.position = \"bottom\")",
    "crumbs": [
      "Tips & tricks",
      "Merging data"
    ]
  },
  {
    "objectID": "weeks/16_context.html",
    "href": "weeks/16_context.html",
    "title": "16. Putting spatial data in context",
    "section": "",
    "text": "Sketch a map of a place you’re familiar with. Mark off locations associated with something secret, or that people might want privacy around, or that are in some way stigmatized, things that even if people know the location is there they might not want their activity publicized. You don’t have to identify anything. For example, the places you used to go when you cut school as a teenager, places you know people sell or use drugs, methadone clinics.\nThere are lots of ways to provide context for the data in your maps; we’ll only look at a few.",
    "crumbs": [
      "Weekly notes",
      "16. Putting spatial data in context"
    ]
  },
  {
    "objectID": "weeks/16_context.html#warm-up",
    "href": "weeks/16_context.html#warm-up",
    "title": "16. Putting spatial data in context",
    "section": "",
    "text": "Sketch a map of a place you’re familiar with. Mark off locations associated with something secret, or that people might want privacy around, or that are in some way stigmatized, things that even if people know the location is there they might not want their activity publicized. You don’t have to identify anything. For example, the places you used to go when you cut school as a teenager, places you know people sell or use drugs, methadone clinics.\nThere are lots of ways to provide context for the data in your maps; we’ll only look at a few.",
    "crumbs": [
      "Weekly notes",
      "16. Putting spatial data in context"
    ]
  },
  {
    "objectID": "weeks/16_context.html#basemaps",
    "href": "weeks/16_context.html#basemaps",
    "title": "16. Putting spatial data in context",
    "section": "Basemaps",
    "text": "Basemaps\nPut your map on a map. There are lots of options; Google Maps and OpenStreetMap are both very popular for map tiles, but they contain a lot of colors and symbols. For data visualization, simple grayscale or black & white basemaps work well, like the map tiles from Stamen (now distributed through Stadia) and Carto. These often require an API key. 1\n1 One thing to note with the ggmap package is that several of the providers have gone out of business since the package was first released, and the functions for them haven’t all been removed. Don’t be surprised if you get errors downloading map tiles from one provider—just try another one. There are also a few other packages to download basemaps but none are updated often.Also note that map tiles will generally be in the web Mercator projection (CRS 4369), so if you’re layering any geoms on top of them, you’ll want to transform into 4369. There are some ways to transform the tiles but they don’t generally look good.\n\nGetting and saving API keys\nWe had a fair amount of trouble with this in class, but API keys can be tricky to get and store. Even for a service that doesn’t seem like someone could cause any trouble with it, you want to treat your API keys like passwords—it’s a good habit to get into. Many packages that use APIs will have functions to help you save your key, and they’ll generally save to a specific environment variable. For example, tidycensus::census_api_key will save your key to the environment variable CENSUS_API_KEY, and that’s where its functions will also look for the key when you make API calls. This allows you to use a secret value (the key) without explicitly using it in your code. Some of these sorts of functions, including the ones in ggmap, won’t save the key between sessions by default; the ggmap::register_* helper functions take an argument write = TRUE in order to save the key permanently. That will mean the key is written to a file called .Renviron, usually saved to your computer’s home directory, which gets read every time you start an R session on that computer.\nBoth Google Maps and Stamen/Stadia require API keys, and Google requires the API key be from an account with billing set up, even if you won’t hit your free tier quota. Stadia keys are free, and you can register from their site. Look at the help docs for details and the link to get a key: ?ggmap::register_stadiamaps. Copy your key from that website, then IN YOUR R CONSOLE, not a script or notebook, call the registration function (in this case ggmap::register_stadiamaps) with your key. Remember that the key is like a password. In order to not have to do this every session and to make sure you have access to it in a notebook, set write = TRUE when you save the key.\nThe process will vary some with other API services, but the gist is generally the same:\n\nRegister for a service\nCopy the API key\nSave it somewhere R will be able to retrieve it securely, usually with a helper function\nUse the service without referencing the key itself in your code\n\n\nlibrary(dplyr)\nlibrary(sf)\nlibrary(justviz)\nlibrary(ggmap) # easy access to downloading basemaps\n\nTo get map tiles, you’ll need either coordinates for the center of the map and a zoom level (e.g. for Google maps), or a bounding box (e.g. for Stamen/Stadia). Unless you want to set up billing for Google Cloud, stick to Stadia which are more designed for data visualization anyway. There are several different options that you can use in the maptype argument; see the package docs for what’s available. Adjust the zoom to get more or less detail, but note that zooming in more will require more tiles, which will take longer to download.\n\nbalt_tracts &lt;- tracts_sf |&gt;\n  filter(county == \"Baltimore city\")\n\nbalt_merge &lt;- st_union(balt_tracts)\n\n# center coordinates need to be a vector of two values\nbalt_centroid &lt;- st_centroid(balt_merge) |&gt;\n  st_coordinates()\n\n# bbox needs to be a vector of left, bottom, right, top with those names\n# put a small buffer around the merged shape to get a little extra space on the sides\n# transform into feet because that's easier than figuring out lat-long degree buffers\nbalt_bbox &lt;- balt_merge  |&gt;\n  st_transform(2248) |&gt;\n  st_buffer(dist = 1 * 5280) |&gt; # one mile buffer\n  st_transform(st_crs(balt_tracts)) |&gt;\n  st_bbox() |&gt;\n  setNames(c(\"left\", \"bottom\", \"right\", \"top\"))\n\ngoogle_road &lt;- get_map(\n  location = balt_centroid,\n  source = \"google\",\n  zoom = 12,\n  maptype = \"roadmap\"\n) # see other options\n\ngoogle_sat &lt;- get_map(\n  location = balt_centroid,\n  source = \"google\",\n  zoom = 12,\n  maptype = \"satellite\"\n)\n\nstamen_toner &lt;- get_map(\n  location = balt_bbox,\n  source = \"stadia\",\n  maptype = \"stamen_toner\"\n)\n\nstamen_bg &lt;- get_map(\n  location = balt_bbox,\n  source = \"stadia\",\n  maptype = \"stamen_toner_background\"\n)\n\nstamen_zoom &lt;- get_map(\n  location = balt_bbox,\n  source = \"stadia\",\n  zoom = 13,\n  maptype = \"stamen_toner_background\"\n)\n\nstamen_lite &lt;- get_map(\n  location = balt_bbox,\n  source = \"stadia\",\n  maptype = \"stamen_toner_lite\"\n)\n\nstamen_terrain &lt;- get_map(\n  location = balt_bbox,\n  source = \"stadia\",\n  maptype = \"stamen_terrain_background\"\n)\n\nLooking at the package source, there are actually a few other styles that are available from Stadia but not documented: in addition to what’s in the docs, you can use “alidade_smooth”, “alidade_smooth_dark”, and “outdoors”.\n\n# need to set inherit.aes = FALSE for all geom_sf layers\n# otherwise they'll try to inherit the aesthetics of the basemap\nggmap(google_road) +\n  geom_sf(data = art_sf, inherit.aes = FALSE, color = \"orchid\") +\n  labs(title = \"Google roadmap, hard to read\") +\n  coord_sf(ndiscr = 0)\n\n\n\n\n\n\n\n# why is this still called darken if you can use any color?\nggmap(google_road, darken = c(0.5, \"white\")) +\n  geom_sf(data = art_sf, inherit.aes = FALSE, color = \"orchid\") +\n  labs(title = \"Google roadmap, faded\") +\n  coord_sf(ndiscr = 0)\n\n\n\n\n\n\n\nggmap(google_sat) +\n  geom_sf(data = art_sf, inherit.aes = FALSE, color = \"orchid\") +\n  labs(title = \"Google satellite\") +\n  coord_sf(ndiscr = 0)\n\n\n\n\n\n\n\nggmap(stamen_toner) +\n  geom_sf(data = art_sf, inherit.aes = FALSE, color = \"orchid\") +\n  labs(title = \"Stamen toner\") +\n  coord_sf(ndiscr = 0)\n\n\n\n\n\n\n\nggmap(stamen_bg) +\n  geom_sf(data = art_sf, inherit.aes = FALSE, color = \"orchid\") +\n  labs(title = \"Stamen toner background\") +\n  coord_sf(ndiscr = 0)\n\n\n\n\n\n\n\nggmap(stamen_zoom) +\n  geom_sf(data = art_sf, inherit.aes = FALSE, color = \"orchid\") +\n  labs(title = \"Stamen toner background, zoom = 13\") +\n  coord_sf(ndiscr = 0)\n\n\n\n\n\n\n\nggmap(stamen_lite) +\n  geom_sf(data = art_sf, inherit.aes = FALSE, color = \"orchid\") +\n  labs(title = \"Stamen toner lite\") +\n  coord_sf(ndiscr = 0)\n\n\n\n\n\n\n\nggmap(stamen_terrain) +\n  geom_sf(data = art_sf, inherit.aes = FALSE, color = \"orchid\") +\n  labs(title = \"Stamen terrain background\") +\n  coord_sf(ndiscr = 0)\n\n\n\n\n\n\n\n\nThere are also OSM tiles in ggspatial with a few styles, but they require the raster package (whereas the sf ecosystem is moving toward terra) and seem to rely on deprecated functions from other packages. I don’t love it…\n\nlibrary(prettymapr) # not using but needed for ggspatial\n\nggplot(art_sf) +\n  ggspatial::annotation_map_tile(zoom = 12, progress = \"none\", type = \"cartolight\") +\n  geom_sf(color = \"orchid\") +\n  labs(title = \"Carto light via ggspatial\") +\n  coord_sf(ndiscr = 0)\n\n\n\n\n\n\n\n\nMy personal favorite is Stamen toner background (no text labels) with the darken argument to fade the color to gray.\nYou can also get specific types of data from OpenStreetMap and use them to create the background yourself with ggplot—we probably won’t have time to get into this but it lets you do something comparable to what in web mapping would be vector tiles.",
    "crumbs": [
      "Weekly notes",
      "16. Putting spatial data in context"
    ]
  },
  {
    "objectID": "weeks/16_context.html#other-locations",
    "href": "weeks/16_context.html#other-locations",
    "title": "16. Putting spatial data in context",
    "section": "Other locations",
    "text": "Other locations\nPut other stuff on your map. If there are specific locations that provide important context, you might try adding a few essential ones (don’t overdo it). Same goes for regional boundaries, major roads, landmarks to help your reading orient themself, buffers around a location, etc. A few examples I’ve done:\n\nUsing dashed lines to mark an area used in some analyses as a proxy for the focus region\nUsing points to mark locations of jails and prisons for a map about tracts incarcerated people come from\nOverlaying lines of major highways (I’m not a huge fan of this if the data isn’t related to highways, but people ask for it sometimes)\nLayering a dot density map of population today by race over historic redlining maps\n\n\nwaste_sf &lt;- balt_tracts |&gt;\n  left_join(ej_natl, by = c(\"geoid\" = \"tract\")) |&gt;\n  filter(indicator == \"haz_waste\")\n\n# filter by geometry to only get brownfields in city\nbalt_brownfields &lt;- brownfields_sf |&gt;\n  st_transform(st_crs(balt_tracts)) |&gt;\n  st_filter(balt_merge) |&gt;\n  filter(!is_archived)\n\nggplot(waste_sf) +\n  geom_sf(aes(fill = value_ptile), color = \"white\", linewidth = 0.1, alpha = 0.9) +\n  # force a legend\n  geom_sf(aes(shape = \"Brownfield\"), data = balt_brownfields, size = 2, alpha = 0.9) +\n  geom_sf(data = balt_merge, color = \"black\", linewidth = 0.2, fill = NA) +\n  scale_fill_fermenter(palette = \"YlGnBu\", direction = 1) +\n  coord_sf(ndiscr = 0) +\n  labs(\n    title = \"Proximity to hazardous waste treatment, national percentile by tract\",\n    fill = \"Percentile\", shape = NULL\n  ) +\n  guides(fill = guide_colorbar(order = 0))\n\n\n\n\n\n\n\n\n\nair_sf &lt;- balt_tracts |&gt;\n  left_join(ej_natl, by = c(\"geoid\" = \"tract\")) |&gt;\n  filter(indicator == \"pm25\")\n\nbalt_highways &lt;- highways_sf |&gt;\n  st_transform(st_crs(balt_tracts)) |&gt;\n  st_filter(balt_merge)\n\nggplot(air_sf) +\n  geom_sf(aes(fill = value_ptile), color = \"white\", linewidth = 0.1, alpha = 0.9) +\n  # force a legend\n  geom_sf(aes(color = \"Major highways\"), data = balt_highways, linewidth = 0.3) +\n  geom_sf(data = balt_merge, color = \"black\", linewidth = 0.2, fill = NA) +\n  scale_fill_fermenter(palette = \"YlGnBu\", direction = 1) +\n  scale_color_manual(values = c(\"gray20\")) +\n  coord_sf(ndiscr = 0) +\n  labs(\n    title = \"Elevated particulate matter, national percentile by tract\",\n    fill = \"Percentile\", color = NULL\n  ) +\n  guides(fill = guide_colorbar(order = 0))",
    "crumbs": [
      "Weekly notes",
      "16. Putting spatial data in context"
    ]
  },
  {
    "objectID": "weeks/16_context.html#putting-it-all-together",
    "href": "weeks/16_context.html#putting-it-all-together",
    "title": "16. Putting spatial data in context",
    "section": "Putting it all together",
    "text": "Putting it all together\nHere’s everything: a basemap, a choropleth, and some points. Replace the previous ggplot with ggmap, and make sure each layer gets a data argument.\n\nggmap(stamen_bg) +\n  geom_sf(aes(fill = value_ptile), data = waste_sf, color = \"white\", linewidth = 0.1, alpha = 0.9, inherit.aes = FALSE) +\n  # force a legend\n  geom_sf(aes(shape = \"Brownfield\"), data = balt_brownfields, size = 2, alpha = 0.9, inherit.aes = FALSE) +\n  geom_sf(data = balt_merge, color = \"black\", linewidth = 0.2, fill = NA, inherit.aes = FALSE) +\n  scale_fill_fermenter(palette = \"YlGnBu\", direction = 1) +\n  coord_sf(ndiscr = 0) +\n  labs(\n    title = \"Proximity to hazardous waste treatment, national percentile by tract\",\n    fill = \"Percentile\", shape = NULL\n  ) +\n  guides(fill = guide_colorbar(order = 0))",
    "crumbs": [
      "Weekly notes",
      "16. Putting spatial data in context"
    ]
  },
  {
    "objectID": "weeks/16_context.html#things-that-arent-data",
    "href": "weeks/16_context.html#things-that-arent-data",
    "title": "16. Putting spatial data in context",
    "section": "Things that aren’t data",
    "text": "Things that aren’t data\nTell a story outside of your map. Pair the map with text, photos, or other media that provide context and history. Story maps can do this well, and many news outlets run maps in their articles that help you use a map to understand the text and vice versa.",
    "crumbs": [
      "Weekly notes",
      "16. Putting spatial data in context"
    ]
  },
  {
    "objectID": "weeks/16_context.html#bottom-line",
    "href": "weeks/16_context.html#bottom-line",
    "title": "16. Putting spatial data in context",
    "section": "Bottom line",
    "text": "Bottom line\nFor a complex analysis or history, a map is often not enough. Practice writing about your data and maps, or collaborate with people who do this well.",
    "crumbs": [
      "Weekly notes",
      "16. Putting spatial data in context"
    ]
  },
  {
    "objectID": "weeks/17_sad_maps.html",
    "href": "weeks/17_sad_maps.html",
    "title": "17. Avoiding sad maps",
    "section": "",
    "text": "Think of a cause, issue, or argument that you’ve been involved in at some point that’s tied to a place. It can be something big and serious like an activist campaign, something simple like a neighborhood cleanup, or just the assets and locations that make you think your city or neighborhood is the best place to live. Sketch a map that you could use in support of this work. What are the locations you need to show in order to persuade people of your argument? How can you provide the context necessary to truly understand the issue?",
    "crumbs": [
      "Weekly notes",
      "17. Avoiding sad maps"
    ]
  },
  {
    "objectID": "weeks/17_sad_maps.html#warm-up",
    "href": "weeks/17_sad_maps.html#warm-up",
    "title": "17. Avoiding sad maps",
    "section": "",
    "text": "Think of a cause, issue, or argument that you’ve been involved in at some point that’s tied to a place. It can be something big and serious like an activist campaign, something simple like a neighborhood cleanup, or just the assets and locations that make you think your city or neighborhood is the best place to live. Sketch a map that you could use in support of this work. What are the locations you need to show in order to persuade people of your argument? How can you provide the context necessary to truly understand the issue?",
    "crumbs": [
      "Weekly notes",
      "17. Avoiding sad maps"
    ]
  },
  {
    "objectID": "weeks/17_sad_maps.html#being-a-responsible-mapmaker",
    "href": "weeks/17_sad_maps.html#being-a-responsible-mapmaker",
    "title": "17. Avoiding sad maps",
    "section": "Being a responsible mapmaker",
    "text": "Being a responsible mapmaker\nIf you remember nothing else from this class, remember this: AVOID MAKING SAD MAPS. This is a term I learned at a conference on the launch of the 500 Cities Project, the predecessor to the project whose data is in justviz::cdc. I don’t remember which one of the panelists said it, and I have no idea where my notes are from the conference, but it’s stuck with me. The panelist defined “sad maps” as maps, often choropleths, that just show hotspots of the same depressing patterns across neighborhoods over and over without supplying any sense of neighborhoods’ assets or source of strength.\nAt that point, I’d been making maps for a year or two, and I’ve been making maps for probably seven years since, and I’m still guilty of making many sad maps. It’s unavoidable, especially when it’s your job to make maps about public health and inequality. That’s why your task isn’t to never make a sad map, but to try to find other ways of doing things. Maybe that means showing distributions instead of neighborhood vs neighborhood or town vs town comparisons, or contextualizing the sad data with community strengths and history, or using a visualization that’s not a map, or not visualizing it at all, or having different people at the table making decisions on how to tell the story. It might also mean using the names and boundaries of places that residents use, which aren’t always the same as what the government or researchers use. 1\n1 The Anti-Eviction Mapping Project, one of the projects for the second case study, describes the challenge of balancing research on hardship with empowering stories, and give a lot of credit to the fact that their work is community-led.This was one of the major reasons I had for pushing my organization away from choropleths—I got tired of making the same sad map. It was doubly frustrating because I lived in one of those “sad” hotspots in New Haven, and I didn’t like thinking I might be reinforcing my neighbors’ or my own disenfranchisement.\nHere’s an example with Baltimore:\n\n\n\n\n\n\n\n\n\nThose are different variables, and while the two from the CDC Places data (poor mental health and diabetes rate) are in part modeled using data from the ACS, our naive hypothesis would be that the maps look different. But they look pretty similar—not exactly the same of course, but with similar patterns. I’d consider these sad maps, and I wouldn’t want to present them all together like this without additional historical context.\nHowever, in Baltimore we have an example of the sad map being reclaimed in a way: Dr. Lawrence Brown coined the “Black Butterfly” (Brown (2021)) to describe the pattern of majority-Black neighborhoods fanning out in East and West Baltimore, with the “White L” cutting through them. He’s then studied how these sad indicators play out across the Black Butterfly, but uses this data in a way that aims to be empowering. It’s become the sort of image you can’t unsee once you see it, and it invites others to describe their research and surroundings in that same framework (here’s one example, Urban Institute (2019)).\n\nBrown, L. T. (2021). The black butterfly: The harmful politics of race and space in America. Johns Hopkins University Press.\n\nUrban Institute. (2019). The Black Butterfly: Racial Segregation and Investment Patterns in Baltimore. https://urbn.is/baltimore\nHere are those same maps with majority-Black tracts outlined:\n\n\n\n\n\n\n\n\n\nAgain, it’s not exactly the same every time, but one of the purposes of data visualization is understanding and contextualizing patterns, and one of the purposes of this sort of branding is bringing that idea to life. After all, when was the last time you saw an urban farming program named after a data visualization??",
    "crumbs": [
      "Weekly notes",
      "17. Avoiding sad maps"
    ]
  },
  {
    "objectID": "weeks/15_color_text_mapping.html",
    "href": "weeks/15_color_text_mapping.html",
    "title": "15. Color, text, and annotations in mapping",
    "section": "",
    "text": "library(dplyr)\nlibrary(sf)\nlibrary(justviz)\nlibrary(ggplot2)",
    "crumbs": [
      "Weekly notes",
      "15. Color, text, and annotations in mapping"
    ]
  },
  {
    "objectID": "weeks/15_color_text_mapping.html#color",
    "href": "weeks/15_color_text_mapping.html#color",
    "title": "15. Color, text, and annotations in mapping",
    "section": "Color",
    "text": "Color\nAs usual, Lisa Charlotte Muth has done great, simple roundups of working with color: see Muth (2024b) and Muth (2024a).\n\nMuth, L. C. (2024b). How to choose an interpolation for your color scale - Datawrapper Blog. In Datawrapper. https://blog.datawrapper.de/interpolation-for-color-scales-and-maps/\n\nMuth, L. C. (2024a). How to choose a color palette for choropleth maps. In Datawrapper. https://blog.datawrapper.de/how-to-choose-a-color-palette-for-choropleth-maps/\nIn data visualization, you’ll generally need to put something on your map besides just boundaries. One of the big ways you’ll do this is by making choropleths or other thematic maps.\nWith a choropleth, the main encoding for your data is the color. Color is generally not read as accurately as encodings like length, position, and angle, so you need to weigh some pros and cons before deciding to make a choropleth. You can maximize the legibility of your color—wide variation in lightness and saturation, some variation in hue.\nHere’s an example of a choropleth with a bad palette. It’s not incorrect as far as how the values are encoded to color, but it’s not good. Why not? Try to change the assignment of bad_pal to make it better.\n\nacs_sf &lt;- tracts_sf |&gt;\n  left_join(acs, by = c(\"geoid\" = \"name\", \"county\")) |&gt;\n  st_transform(2248)\n\nbad_pal &lt;- RColorBrewer::brewer.pal(n = 5, name = \"YlGnBu\")[c(1, 5)]\n\nggplot(acs_sf, aes(fill = foreign_born)) +\n  geom_sf(linewidth = 0, color = \"white\") +\n  scale_fill_gradient(low = bad_pal[1], high = bad_pal[2])",
    "crumbs": [
      "Weekly notes",
      "15. Color, text, and annotations in mapping"
    ]
  },
  {
    "objectID": "weeks/15_color_text_mapping.html#breaks",
    "href": "weeks/15_color_text_mapping.html#breaks",
    "title": "15. Color, text, and annotations in mapping",
    "section": "Breaks",
    "text": "Breaks\nThe big decision is a choropleth is whether you want to keep the color scale continuous or cut it into bins, and if so, how. I almost always cut my choropleths into bins, but some people argue against it. Color scales broken into bins can be easier to read quickly, but you do lose some of the granularity. This is the part that Lisa Charlotte Muth called a “compromise between truthfulness and usefulness”, and it’s one of those decisions you make as a data viz designer that will impact how the data are perceived.\nThere are many different algorithms for deciding on breaks, and some are more appropriate than others depending on the data. In my day job, I almost always use natural breaks (also called Jenks breaks after the statistician who came up with them); these are well-suited for showing skewed distributions, and since most of my work is about inequality of various sorts, it’s often a good choice. The downside is the breaks aren’t obviously clear or easy to explain (some people say if you’re going to cut your data, you need to explain the breaks, and I never do…). On the other hand, if a value ranges from 0% to 50%, and you break it into 0-9, 10-19, …, it might not show the pattern in the data as well but it’s easy to understand (these are usually called pretty breaks).\nYou should also inspect whether the bins make sense and are useful: if you have a value that ranges from 0% to 10%, are 5 different bins really that important? Maybe they are for something where any increase is urgent, like child lead poisoning, but in many cases it might not be. Muth’s posts on color give examples of this as well.\nThere are several functions that will help you cut your data into breaks (including base R’s cut); I wrote one in a work package just for Jenks breaks. The classInt package has several of these algorithms, and now has a function to easily cut your data up for you.\n\npal &lt;- RColorBrewer::brewer.pal(n = 5, name = \"PuBuGn\")\n\nchoro1 &lt;- acs_sf |&gt;\n  mutate(brk = classInt::classify_intervals(var = foreign_born, n = 5, \n                                            style = \"quantile\")) |&gt;\n  ggplot(aes(fill = brk)) +\n  geom_sf(linewidth = 0, color = \"white\") +\n  scale_fill_manual(values = pal)\n\nchoro1\n\n\n\n\n\n\n\n\nThe function to make this choropleth-histogram exploration plot is in the lab.",
    "crumbs": [
      "Weekly notes",
      "15. Color, text, and annotations in mapping"
    ]
  },
  {
    "objectID": "weeks/15_color_text_mapping.html#does-this-scale",
    "href": "weeks/15_color_text_mapping.html#does-this-scale",
    "title": "15. Color, text, and annotations in mapping",
    "section": "Does this scale?",
    "text": "Does this scale?\nSometimes you choose an algorithm for your data that doesn’t work well once you change something—maybe you zoom out to include more areas, or you want a different number of breaks. If you switch to a larger area (e.g. all tracts in the state) or a smaller area (e.g. all tracts in Howard County, only 59), you might need to adjust the algorithm and/or number of bins. It also, once again, might depend on your purpose: a planning group looking at towns in a small region might only want to know relative high, low, and middle values (3 bins), whereas the state’s public health department might need much more granular breaks (5 or maybe more bins).",
    "crumbs": [
      "Weekly notes",
      "15. Color, text, and annotations in mapping"
    ]
  },
  {
    "objectID": "weeks/15_color_text_mapping.html#text",
    "href": "weeks/15_color_text_mapping.html#text",
    "title": "15. Color, text, and annotations in mapping",
    "section": "Text",
    "text": "Text\nYou wouldn’t want to put text labels on every tract, because there would just be too many, but you could on something with fewer geographies, like counties in the metro area. We can also start using boundaries to give context and make things easier to read. You can use geom_sf_text to place labels near the centroid of each shape.\n\ncounty_sf &lt;- tracts_sf |&gt;\n  group_by(county) |&gt;\n  summarise() \n\nggplot(county_sf) +\n  geom_sf() +\n  geom_sf_text(aes(label = stringr::str_wrap(county, width = 12)), size = 3, vjust = 0)",
    "crumbs": [
      "Weekly notes",
      "15. Color, text, and annotations in mapping"
    ]
  },
  {
    "objectID": "weeks/15_color_text_mapping.html#annotations",
    "href": "weeks/15_color_text_mapping.html#annotations",
    "title": "15. Color, text, and annotations in mapping",
    "section": "Annotations",
    "text": "Annotations\nI usually don’t use scales and north arrows because it’s just not relevant to my usual audience. If I were in a field like planning or environmental engineering, I’d probably use them all the time. The package ggspatial has ggplot add-ons for this. Here’s an example of the scale annotation with the defaults:\n\nchoro1 +\n  ggspatial::annotation_scale(unit_category = \"imperial\")",
    "crumbs": [
      "Weekly notes",
      "15. Color, text, and annotations in mapping"
    ]
  },
  {
    "objectID": "weeks/06_color.html",
    "href": "weeks/06_color.html",
    "title": "6. Color",
    "section": "",
    "text": "Code\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(justviz)\n\nsource(here::here(\"utils/plotting_utils.R\"))",
    "crumbs": [
      "Weekly notes",
      "6. Color"
    ]
  },
  {
    "objectID": "weeks/06_color.html#warm-up",
    "href": "weeks/06_color.html#warm-up",
    "title": "6. Color",
    "section": "Warm-up",
    "text": "Warm-up\n\nColor perception\nWhich row uses a darker color?\n\n\n\n\n\n\n\n\n\nWhich line uses a darker color?\n\n\n\n\n\n\n\n\n\nWhich line uses a darker color?\n\n\n\n\n\n\n\n\n\nHow many purple dots are in each row?\n\n\n\n\n\n\n\n\n\n\n\nColors in R\nIf you don’t already have a color selection tool on your computer 1 you can install the colourpicker package that has a color picker addin for RStudio.\n1 Maybe the thing I miss most since switching from Mac to Linux is the color picker app Sip, definitely recommend it",
    "crumbs": [
      "Weekly notes",
      "6. Color"
    ]
  },
  {
    "objectID": "weeks/06_color.html#chapters",
    "href": "weeks/06_color.html#chapters",
    "title": "6. Color",
    "section": "Chapters",
    "text": "Chapters\nWe’ll walk through Wilke chapters 4 and 19–I don’t have a ton to add until we get to mapping.\n\nWilke chapter 4\nWilke chapter 19\nDatawrapper (2021)\n\n\nDatawrapper. (2021). What to consider when choosing colors for data visualization. https://academy.datawrapper.de/article/140-what-to-consider-when-choosing-colors-for-data-visualization",
    "crumbs": [
      "Weekly notes",
      "6. Color"
    ]
  },
  {
    "objectID": "weeks/06_color.html#tools",
    "href": "weeks/06_color.html#tools",
    "title": "6. Color",
    "section": "Tools",
    "text": "Tools\n\nColorBrewer (access to these palettes comes with ggplot)\nCarto Colors (access comes with the rcartocolor package)\nViz Palette generator & preview\nGregor Aisch’s chroma palettes generator\nColorgorical categorical color generator 2\n\n2 I just read a post making fun of Colorgorical for leaning toward puke green colors; haven’t used it in a while but heads up I guess?",
    "crumbs": [
      "Weekly notes",
      "6. Color"
    ]
  },
  {
    "objectID": "weeks/06_color.html#types-of-color-palettes",
    "href": "weeks/06_color.html#types-of-color-palettes",
    "title": "6. Color",
    "section": "Types of color palettes",
    "text": "Types of color palettes\nThe main types of color palettes are:\n\nsequential / quantitative: values are numeric and continuous; values and colors (saturation, lightness, hue) increase in some way in tandem\ndiverging: values are likely numeric, but colors trend in opposite directions\nqualitative / categorical: values are not numeric / continuous, and colors should not imply continuity\n\nColorBrewer and Carto Colors are great because they have options for all three of these.\nThese are rough examples using ColorBrewer palettes; in practice you might want to make some adjustments to these.",
    "crumbs": [
      "Weekly notes",
      "6. Color"
    ]
  },
  {
    "objectID": "weeks/06_color.html#exercises",
    "href": "weeks/06_color.html#exercises",
    "title": "6. Color",
    "section": "Exercises",
    "text": "Exercises\n\nlocal_counties &lt;- c(\"Baltimore city\", \"Baltimore County\", \"Harford County\", \"Howard County\", \"Anne Arundel County\")\nacs_county &lt;- acs |&gt;\n  filter(level %in% c(\"us\", \"state\", \"county\")) |&gt;\n  mutate(local1 = forcats::as_factor(name) |&gt;\n           forcats::fct_other(keep = c(local_counties, \"United States\", \"Maryland\"), other_level = \"Other counties\"),\n         local2 = forcats::fct_collapse(local1, \"Outside Baltimore\" = c(\"Baltimore County\", \"Harford County\", \"Howard County\", \"Anne Arundel County\")) |&gt;\n           forcats::fct_relevel(\"Outside Baltimore\", \"Other counties\", after = Inf))\n\ntheme_set(theme_nice())\nknitr::kable(head(acs_county))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nlevel\ncounty\nname\ntotal_pop\nwhite\nblack\nlatino\nasian\nother_race\ndiversity_idx\nforeign_born\ntotal_hh\nhomeownership\ntotal_cost_burden\ntotal_severe_cost_burden\nowner_cost_burden\nowner_severe_cost_burden\nrenter_cost_burden\nrenter_severe_cost_burden\nno_vehicle_hh\nmedian_hh_income\nages25plus\nless_than_high_school\nhigh_school_grad\nsome_college_or_aa\nbachelors\ngrad_degree\npov_status_determined\npoverty\nlow_income\narea_sqmi\npop_density\nlocal1\nlocal2\n\n\n\n\nus\nNA\nUnited States\n331097593\n0.59\n0.12\n0.19\n0.06\n0.05\n0.7376547\n0.14\n125736353\n0.65\n0.29\n0.14\n0.21\n0.09\n0.45\n0.23\n0.08\n75149\n226600992\n0.11\n0.26\n0.28\n0.21\n0.13\n323275448\n0.13\n0.29\nNA\nNA\nUnited States\nUnited States\n\n\nstate\nNA\nMaryland\n6161707\n0.49\n0.29\n0.11\n0.06\n0.05\n0.7905359\n0.16\n2318124\n0.67\n0.29\n0.13\n0.21\n0.09\n0.46\n0.23\n0.09\n98461\n4260095\n0.09\n0.24\n0.25\n0.22\n0.20\n6034320\n0.09\n0.21\nNA\nNA\nMaryland\nMaryland\n\n\ncounty\nNA\nAllegany County\n68161\n0.87\n0.07\n0.02\n0.01\n0.03\n0.3379538\n0.02\n27462\n0.70\n0.23\n0.10\n0.16\n0.07\n0.39\n0.17\n0.10\n55248\n47914\n0.09\n0.41\n0.29\n0.12\n0.09\n61098\n0.16\n0.36\n422.19881\n161.4429\nOther counties\nOther counties\n\n\ncounty\nNA\nAnne Arundel County\n588109\n0.65\n0.17\n0.09\n0.04\n0.05\n0.6745018\n0.09\n221704\n0.75\n0.26\n0.11\n0.20\n0.08\n0.44\n0.19\n0.04\n116009\n409052\n0.06\n0.22\n0.27\n0.25\n0.19\n577547\n0.06\n0.14\n414.80643\n1417.7914\nAnne Arundel County\nOutside Baltimore\n\n\ncounty\nNA\nBaltimore County\n850737\n0.54\n0.30\n0.06\n0.06\n0.04\n0.7241974\n0.12\n328611\n0.67\n0.29\n0.14\n0.20\n0.09\n0.48\n0.24\n0.08\n88157\n589611\n0.08\n0.24\n0.26\n0.23\n0.18\n830921\n0.10\n0.23\n598.35821\n1421.7855\nBaltimore County\nOutside Baltimore\n\n\ncounty\nNA\nBaltimore city\n584548\n0.27\n0.61\n0.06\n0.03\n0.04\n0.6483744\n0.08\n247232\n0.48\n0.37\n0.19\n0.26\n0.13\n0.47\n0.26\n0.26\n58349\n410221\n0.13\n0.28\n0.24\n0.18\n0.17\n564634\n0.20\n0.38\n80.94606\n7221.4510\nBaltimore city\nBaltimore city\n\n\n\n\n\nFind some ways to improve the use of color in these charts, including whether color even should be used. Before writing any code, write down:\n\nHow color is used / what color signifies\nWhat’s wrong with that use of color\nHow you want to change it\n\nHere are some charts that are bad because they use color inappropriately.\n\nacs_county |&gt;\n  mutate(name = forcats::as_factor(name) |&gt; forcats::fct_reorder(homeownership)) |&gt;\n  ggplot(aes(x = name, y = homeownership, fill = name)) +\n  geom_col(width = 0.8) +\n  coord_flip() +\n  labs(x = NULL, y = NULL, \n       title = \"Homeownership rate by location\", \n       subtitle = \"US, Maryland, and Maryland counties, 2022\")\n\n\n\n\n\n\n\n\n\nacs_county |&gt;\n  mutate(name = forcats::as_factor(name) |&gt; forcats::fct_reorder(homeownership)) |&gt;\n  ggplot(aes(x = name, y = homeownership, fill = local2)) +\n  geom_col(width = 0.7, color = \"gray20\", linewidth = 0.2) +\n  coord_flip() +\n  labs(x = NULL, y = NULL, \n       title = \"Homeownership rate by location\", \n       subtitle = \"US, Maryland, and Maryland counties, 2022\") +\n  scale_fill_brewer(palette = \"GnBu\")\n\n\n\n\n\n\n\n\nHere’s a chart that’s okay but not great; it uses color in a way that’s not wrong but not effective either.\n\nacs_county |&gt;\n  mutate(name = forcats::as_factor(name) |&gt; forcats::fct_reorder(homeownership)) |&gt;\n  ggplot(aes(x = name, y = homeownership, fill = local2)) +\n  geom_col(width = 0.8) +\n  coord_flip() +\n  labs(x = NULL, y = NULL, \n       title = \"Homeownership rate by location\", \n       subtitle = \"US, Maryland, and Maryland counties, 2022\")\n\n\n\n\n\n\n\n\nHere’s one that uses color when it should actually use something else to convey its pattern. What type of chart would be more appropriate?\n\nacs_county |&gt;\n  mutate(name = forcats::as_factor(name) |&gt; forcats::fct_reorder(homeownership)) |&gt;\n  ggplot(aes(x = name, y = homeownership, fill = median_hh_income)) +\n  geom_col(width = 0.8) +\n  coord_flip() +\n  labs(x = NULL, y = NULL, \n       title = \"Homeownership rate vs median household income by location\", \n       subtitle = \"US, Maryland, and Maryland counties, 2022\")",
    "crumbs": [
      "Weekly notes",
      "6. Color"
    ]
  },
  {
    "objectID": "weeks/07_annotations.html",
    "href": "weeks/07_annotations.html",
    "title": "7. Text and annotations",
    "section": "",
    "text": "“Until the systems of power recognise different categories, the data I’m reporting on is also flawed,” she added.\nIn a bid to account for these biases, and any biases of her own, Chalabi is transparent about her sources and often includes disclaimers about her own decision-making process and about any gaps or uncertainties in the data.\n“I try to produce journalism where I’m explaining my methods to you,” she said. “If I can do this, you can do this, too. And it’s a very democratising experience, it’s very egalitarian.”\nIn an ideal scenario, she is able to integrate this background information into the illustrations themselves, as evidenced by her graphics on anti-Asian hate crimes and the ethnic cleansing of Uygurs in China.\nBut at other times, context is relegated to the caption to ensure the graphic is as grabby as possible.\n“What I have found is literally every single word that you add to an image reduces engagement, reduces people’s willingness or ability to absorb the information,” Chalabi said.\n“So there is a tension there. How can you be accurate and get it right without alienating people by putting up too much information? That’s a really, really hard balance.”\nMona Chalabi in Hahn (2023)\n\nHahn, J. (2023). \"Data replicates the existing systems of power\" says Pulitzer Prize-winner Mona Chalabi. Dezeen. https://www.dezeen.com/2023/11/16/mona-chalabi-pulitzer-prize-winner/",
    "crumbs": [
      "Weekly notes",
      "7. Text and annotations"
    ]
  },
  {
    "objectID": "weeks/07_annotations.html#big-picture-providing-context-and-making-meaning",
    "href": "weeks/07_annotations.html#big-picture-providing-context-and-making-meaning",
    "title": "7. Text and annotations",
    "section": "",
    "text": "“Until the systems of power recognise different categories, the data I’m reporting on is also flawed,” she added.\nIn a bid to account for these biases, and any biases of her own, Chalabi is transparent about her sources and often includes disclaimers about her own decision-making process and about any gaps or uncertainties in the data.\n“I try to produce journalism where I’m explaining my methods to you,” she said. “If I can do this, you can do this, too. And it’s a very democratising experience, it’s very egalitarian.”\nIn an ideal scenario, she is able to integrate this background information into the illustrations themselves, as evidenced by her graphics on anti-Asian hate crimes and the ethnic cleansing of Uygurs in China.\nBut at other times, context is relegated to the caption to ensure the graphic is as grabby as possible.\n“What I have found is literally every single word that you add to an image reduces engagement, reduces people’s willingness or ability to absorb the information,” Chalabi said.\n“So there is a tension there. How can you be accurate and get it right without alienating people by putting up too much information? That’s a really, really hard balance.”\nMona Chalabi in Hahn (2023)\n\nHahn, J. (2023). \"Data replicates the existing systems of power\" says Pulitzer Prize-winner Mona Chalabi. Dezeen. https://www.dezeen.com/2023/11/16/mona-chalabi-pulitzer-prize-winner/",
    "crumbs": [
      "Weekly notes",
      "7. Text and annotations"
    ]
  },
  {
    "objectID": "weeks/07_annotations.html#text",
    "href": "weeks/07_annotations.html#text",
    "title": "7. Text and annotations",
    "section": "Text",
    "text": "Text\n\nA data visualization is not a piece of art meant to be looked at only for its aesthetically pleasing features. Instead, its purpose is to convey information and make a point. To reliably achieve this goal when preparing visualizations, we have to place the data into context and provide accompanying titles, captions, and other annotations. – Wilke (2019) ch. 22\n\nWilke, C. (2019). Fundamentals of data visualization: A primer on making informative and compelling figures (First edition). O’Reilly. https://clauswilke.com/dataviz/\n\nThe type of text you use, phrasing, and placement all depend on where your visualizations will go, who will read them, and how they might be distributed. For example, I might put less detail in the titles and labels of a chart that will be part of a larger publication than a chart that might get distributed on its own (I’ll also tend towards more straightforward chart types and simpler analyses for something standalone).\n\nUses of text\nHere’s a good rundown on how to use text\n\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(justviz)\nsource(here::here(\"utils/plotting_utils.R\"))\n# source(here::here(\"utils/misc.R\"))\nbalt_metro &lt;- readRDS(here::here(\"utils/balt_metro.rds\"))\n\n# set a default theme from the one I defined in plotting_utils.R\ntheme_set(theme_nice())\n\nIdentify all the text in this chart, what purpose it serves, and whether that could be done better through other means.\n\nacs |&gt;\n  filter(level %in% c(\"us\", \"state\") | name %in% balt_metro) |&gt;\n  mutate(name = forcats::fct_reorder(name, total_cost_burden)) |&gt;\n  mutate(level2 = forcats::fct_other(name, keep = c(\"United States\", \"Maryland\", \"Baltimore city\"))) |&gt;\n  stylehaven::offset_lbls(value = total_cost_burden, frac = 0.025, fun = scales::label_percent()) |&gt;\n  ggplot(aes(x = name, y = total_cost_burden, fill = level2)) +\n  geom_col(width = 0.8) +\n  geom_text(aes(label = lbl, hjust = just, y = y), color = \"white\", fontface = \"bold\") +\n  scale_y_barcontinuous() +\n  coord_flip() +\n  labs(title = \"Baltimore city has a higher rate of cost burden than the state or nation\",\n       subtitle = \"Share of households that are cost burdened, Maryland, 2022\",\n       caption = \"Source: US Census Bureau American Community Survey, 2022 5-year estimates\",\n       fill = \"fill\") +\n  theme(panel.grid.major.y = element_blank(),\n        panel.grid.major.x = element_line()) \n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBrainstorm\n\n\n\n\n\n\n\n\n\n\n\nText\nPurpose\nCould be better?\n\n\n\n\nTitle\nTakeaway, what you’re looking at in context\n\n\n\nSubtitle\nSpecifics of what’s being measured\nDepending on context, maybe put cost burden definition here\n\n\nIndependent axis\nLocations\n\n\n\nIndependent axis title\nWhat’s on the axis\nNot necessary; we know what these names are\n\n\nLegend title\nWhat colors mean\n\n\n\nLegend labels\nLocation types\nDrop the legend, put any additional info in subtitle\n\n\nDependent axis title\nMeaning of variable being measured\nCan remove since it’s in the subtitle, but some styleguides may say keep it\n\n\nCaption\nSource\nCould put definition of cost burden here\n\n\nDependent axis labels\nSpecify meaning of breaks along axis\nCan drop because redundant\n\n\nDirect data labels on bars\nValues of each data point",
    "crumbs": [
      "Weekly notes",
      "7. Text and annotations"
    ]
  },
  {
    "objectID": "weeks/07_annotations.html#other-annotations",
    "href": "weeks/07_annotations.html#other-annotations",
    "title": "7. Text and annotations",
    "section": "Other annotations",
    "text": "Other annotations\nThere are other annotations that are useful too. You might mark off a region to show a cluster of points, or a period in time. There are 2 approaches to this with ggplot: using geoms (geom_text, geom_hline, etc) or annotation layers (annotate, annotation_custom). The main difference is that annotations aren’t mapped to data the way geoms are. Because of that, I almost only use geoms for annotations, and usually make a small data frame just for the data that goes into the annotations to avoid hard-coding too much.\nAn example from DataHaven’s most recent books: we wanted to explicitly put evictions into a policy context, so we marked off the end of the federal eviction moratorium and the prepandemic average count as a threshhold. Without those labeled lines, you could tell that there was an abrupt drop in evictions, then a steep rise in them about a year and a half later, then counts that are higher than at the beginning of 2020. But unless you had followed eviction trends and COVID relief policies, you might not know why any of those things occurred.\n\n\n\nFrom Abraham et al. (2023)\n\nAbraham, M., Seaberry, C., Davila, K., & Carr, A. (2023). Greater New Haven Community Wellbeing Index 2023. https://ctdatahaven.org/reports/greater-new-haven-community-wellbeing-index",
    "crumbs": [
      "Weekly notes",
      "7. Text and annotations"
    ]
  },
  {
    "objectID": "weeks/07_annotations.html#exercises",
    "href": "weeks/07_annotations.html#exercises",
    "title": "7. Text and annotations",
    "section": "Exercises",
    "text": "Exercises\nThis chart doesn’t have labels for its axes, but you know it’s unemployment rates in Baltimore and Maryland. How accurately can we guess what the labels would be?\n\n\n\n\n\n\n\n\n\nNext, what annotations would be helpful for contextualizing this trend?\n\n\n\n\n\n\nBrainstorm: contextualizing information\n\n\n\n\nTimespan–years on axis\nSource\nUnits of measurement\nHistorical events",
    "crumbs": [
      "Weekly notes",
      "7. Text and annotations"
    ]
  },
  {
    "objectID": "weeks/14_geos.html",
    "href": "weeks/14_geos.html",
    "title": "14. Manipulating geographies",
    "section": "",
    "text": "Go to https://native-land.ca/. This is a crowd-sourced / community-build atlas of indigenous communities’ land before colonization, focused on the Americas and Oceania. Find the place you live now, or where you grew up, or somewhere else familiar to you, and explore it on the map. Are there any names that you recognize as the names of towns, states, rivers, or anything else? Were you familiar with these groups already? Can you think of any other examples of geographic boundaries that are contested, unjust, or could be up for debate?\n\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(sf) \n\nThese are just a few quick notes on how and why you might adjust your geographies. You’ll work through these examples in a practice notebook.",
    "crumbs": [
      "Weekly notes",
      "14. Manipulating geographies"
    ]
  },
  {
    "objectID": "weeks/14_geos.html#warm-up",
    "href": "weeks/14_geos.html#warm-up",
    "title": "14. Manipulating geographies",
    "section": "",
    "text": "Go to https://native-land.ca/. This is a crowd-sourced / community-build atlas of indigenous communities’ land before colonization, focused on the Americas and Oceania. Find the place you live now, or where you grew up, or somewhere else familiar to you, and explore it on the map. Are there any names that you recognize as the names of towns, states, rivers, or anything else? Were you familiar with these groups already? Can you think of any other examples of geographic boundaries that are contested, unjust, or could be up for debate?\n\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(sf) \n\nThese are just a few quick notes on how and why you might adjust your geographies. You’ll work through these examples in a practice notebook.",
    "crumbs": [
      "Weekly notes",
      "14. Manipulating geographies"
    ]
  },
  {
    "objectID": "weeks/14_geos.html#cartographic-boundaries",
    "href": "weeks/14_geos.html#cartographic-boundaries",
    "title": "14. Manipulating geographies",
    "section": "Cartographic boundaries",
    "text": "Cartographic boundaries\nSo far we’ve used the Census Bureau’s cartographic boundaries, which are designed for mapping more than for spatial analysis. That’s the cb = TRUE in our previous tigris calls. Now get tracts with this set to false. How are the shapes different? Which has more tracts?\n\nall_tracts &lt;- tigris::tracts(state = \"24\", cb = TRUE, progress_bar = FALSE) |&gt;\n  select(geoid = GEOID, county = NAMELSADCO) |&gt;\n  st_transform(2248)\n\ntracts_no_cb &lt;- tigris::tracts(state = \"24\", cb = FALSE, progress_bar = FALSE) |&gt;\n  select(geoid = GEOID) |&gt;\n  st_transform(2248)\n  \nplot(all_tracts$geometry)\n\n\n\n\n\n\n\nplot(tracts_no_cb$geometry)",
    "crumbs": [
      "Weekly notes",
      "14. Manipulating geographies"
    ]
  },
  {
    "objectID": "weeks/14_geos.html#combining-geometries",
    "href": "weeks/14_geos.html#combining-geometries",
    "title": "14. Manipulating geographies",
    "section": "Combining geometries",
    "text": "Combining geometries\nGoing back to the CB version, we’ll make a map of tracts with county boundaries. With shapefiles from the Census Bureau, we can expect the boundaries to be accurate, but if you’re using data that you’re less confident in, you should know how to aggregate your geometries. (For example, if I map data based on neighborhood boundaries I got from a small town’s city hall with no GIS staff, I’m going to expect there could be some gaps between neighborhoods or boundaries that don’t line up exactly with the official town borders or more high-resolution files from the Census.)\n\ncounties &lt;- all_tracts |&gt;\n  group_by(county) |&gt;  # add another line here to combine geometries by county\n  summarise()\n\nggplot() +\n  geom_sf(data = all_tracts, linewidth = 0.1, color = \"gray40\", fill = \"white\") +\n  geom_sf(data = counties, fill = NA, linewidth = 0.4, color = \"gray20\")",
    "crumbs": [
      "Weekly notes",
      "14. Manipulating geographies"
    ]
  },
  {
    "objectID": "weeks/14_geos.html#simplification",
    "href": "weeks/14_geos.html#simplification",
    "title": "14. Manipulating geographies",
    "section": "Simplification",
    "text": "Simplification\nSometimes when you have a lot of shapes, especially very small ones, you’ll want to simplify them. This can make them easier to tell apart in a map, faster to plot (although geom_sf is much faster than when they first added it), and smaller in file size. Imagine a map of counties for the whole US: you don’t need to see all the precise contours of every county. This becomes especially important if you do web mapping. There are different algorithms for how to simplify shapes, but essentially they all remove nodes along the path of a shape.\nsf::st_simplify takes a tolerance in meters that signals how far from their original paths the simplified shapes are allowed to go. For small tolerances, you can’t really tell any information has been removed. You’ll experiment with Dorchester County, which has only a few tracts but several small islands.\n\nall_tracts |&gt;\n  filter(county == \"Baltimore County\") |&gt;\n  select(geometry) |&gt;\n  st_simplify(dTolerance = 50) |&gt;\n  plot()\n\n\n\n\n\n\n\n\nmapshaper is a javascript-based tool to do a lot of geometry manipulation on the command line or in a browser, but there’s an R package that wraps around it. It’s got better simplification algorithms that are less prone to gaps. Instead of a tolerance, you give a percentage of how many nodes to keep—the default is only 5%. Crank the value down until it falls apart.\n\nall_tracts |&gt;\n  filter(county == \"Baltimore County\") |&gt;\n  rmapshaper::ms_simplify(keep_shapes = TRUE, keep = 0.8) |&gt;\n  select(geometry) |&gt;\n  plot()\n\n\n\n\n\n\n\n\nIt’s less important here with these smaller data frames, but with larger, more complex data, data in a web app, or data being passed between servers, you’ll want to pay attention to file sizes and memory.\n\nobject.size(all_tracts)\n\n2233672 bytes\n\nobject.size(all_tracts |&gt; rmapshaper::ms_simplify(keep = 0.2))\n\n1418624 bytes",
    "crumbs": [
      "Weekly notes",
      "14. Manipulating geographies"
    ]
  },
  {
    "objectID": "weeks/12_maps.html",
    "href": "weeks/12_maps.html",
    "title": "12. Spatial data",
    "section": "",
    "text": "Imagine you have a friend staying with you. They want to go for a walk around your neighborhood, without a phone or other source of directions. Without looking anything up, sketch a map of your neighborhood for them. Don’t focus on accuracy, just on the places and landmarks that characterize the area.\nThink back through the past few days and list all the maps you remember using. These don’t have to be maps of real places or strictly used for navigation.",
    "crumbs": [
      "Weekly notes",
      "12. Spatial data"
    ]
  },
  {
    "objectID": "weeks/12_maps.html#warm-up",
    "href": "weeks/12_maps.html#warm-up",
    "title": "12. Spatial data",
    "section": "",
    "text": "Imagine you have a friend staying with you. They want to go for a walk around your neighborhood, without a phone or other source of directions. Without looking anything up, sketch a map of your neighborhood for them. Don’t focus on accuracy, just on the places and landmarks that characterize the area.\nThink back through the past few days and list all the maps you remember using. These don’t have to be maps of real places or strictly used for navigation.",
    "crumbs": [
      "Weekly notes",
      "12. Spatial data"
    ]
  },
  {
    "objectID": "weeks/12_maps.html#what-is-a-map",
    "href": "weeks/12_maps.html#what-is-a-map",
    "title": "12. Spatial data",
    "section": "What is a map?",
    "text": "What is a map?\nThese are the same questions as we talked about the first week with non-spatial data. Many of the answers will be similar as well.\nJust like with data visualization, many of our sources can explain all sorts of things about maps without saying what a map is. One interesting definition comes from John Nelson at Esri:\n\nA map is a model of a specific phenomenon in the real world—a communication device. You take an idea or an insight that’s in your brain and magically transport it into somebody else’s. – O’Donohue (n.d.)\n\nO’Donohue, D. (n.d.). Communicating With Maps - The Art Of Cartography. Retrieved March 31, 2024, from https://mapscaping.com/podcast/communicating-with-maps-the-art-of-cartography/",
    "crumbs": [
      "Weekly notes",
      "12. Spatial data"
    ]
  },
  {
    "objectID": "weeks/12_maps.html#why-make-a-map",
    "href": "weeks/12_maps.html#why-make-a-map",
    "title": "12. Spatial data",
    "section": "Why make a map?",
    "text": "Why make a map?\n\n\n\n\n\n\nWarning\n\n\n\nA confession: in a lot of situations, I am anti-map. Just like with charts versus tables, bulletpoints, or just a sentence, maps—especially thematic maps—tend to be overused. I’ve been asked to make maps of phenomena that have little or nothing to do with geography, just because people like maps. I won’t rant again about making data viz decisions for the sake of being cute, but the first question in how to make a map should always be whether you actually should. Kenneth Field gets into this in his interview on Storytelling with Data. Much like using aggregates in place of finer-grained values, you can gain simplicity but lose accuracy and ease of understanding your data. It’s a trade-off, so don’t trade away important aspects of your data just to be cute!\n\n\n\nExplore\nExplain\nBoth\n\nUnlike with nonspatial data, with maps we’re more accustomed to exploration being part of the process. Think about it: how often do you dig around through a chart to find a bar or point that represents you and the places that are familiar to you, versus when you see a map?",
    "crumbs": [
      "Weekly notes",
      "12. Spatial data"
    ]
  },
  {
    "objectID": "weeks/12_maps.html#how-are-maps-used",
    "href": "weeks/12_maps.html#how-are-maps-used",
    "title": "12. Spatial data",
    "section": "How are maps used?",
    "text": "How are maps used?\n\n\n\n\n\n\nBrainstorming\n\n\n\n\nPositive / constructive\n\n\n\n\n\nNegative / destructive",
    "crumbs": [
      "Weekly notes",
      "12. Spatial data"
    ]
  },
  {
    "objectID": "weeks/12_maps.html#types-of-spatial-data",
    "href": "weeks/12_maps.html#types-of-spatial-data",
    "title": "12. Spatial data",
    "section": "Types of spatial data",
    "text": "Types of spatial data\nWhat type of spatial data you have will affect what you can do with it and how you go about mapping it. The main types are:\n\nPoint data, where observations are characterized by their location. These points might also have attributes associated with them.\nGrid data, where measurements are taken at locations distributed throughout an area. These don’t need to be on a strict square grid; values might be interpolated between measured locations. Rasters are a common form of this where observations are more or less taken on a grid, often represented as one pixel per location (either observed or interpolated).\nAreal data, where values are associated with two-dimensional regions. These are often based on man-made boundaries such as countries, towns, etc.\n\nWe’ll be working with areal data the most. It has some drawbacks that we’ll talk about, but it is also flexible. For example, we have easy access to tract boundaries that cover all of Maryland. We have lots of data associated with each of those tracts. We can also join those tract geographies with other spatial data, including points within the tracts or overlaps between tracts and some other area.",
    "crumbs": [
      "Weekly notes",
      "12. Spatial data"
    ]
  },
  {
    "objectID": "weeks/12_maps.html#spatial-data-in-r",
    "href": "weeks/12_maps.html#spatial-data-in-r",
    "title": "12. Spatial data",
    "section": "Spatial data in R",
    "text": "Spatial data in R\n\nThe spatial ecosystem\nA few packages to install first (cross your fingers):\n\npkgs &lt;- c(\n  \"sf\",         # sf : spatial data :: dplyr : tabular data\n  \"osmdata\",    # batch download data from OpenStreetMap\n  \"terra\",      # sf-friendly raster tools\n  \"ggspatial\",  # map annotations for ggplot\n  \"tigris\",     # download shapefiles from the Census Bureau TIGER database\n  \"ggmap\",      # basemaps for ggplot\n  \"rmapshaper\", # command line tool to edit geographies\n  \"geojsonio\"   # work with geojson data\n)\n\ninstall.packages(pkgs)\n\nMuch of the R spatial ecosystem has migrated from an older set of packages (sp, rgeos, rgdal) to a newer one (sf, plus its friends like stars and terra) designed to fit into the tidyverse packages. 1 sf is also nice because many of the functions are based on PostGIS, so if you’ve worked with spatial databases (or might in the future) it’s easy to translate code.\n1 This was actually a major concerted effort over the past few years because the original maintainer of many of those packages was retiring.There are a few sf-based shapefiles in the justviz package, and many others are easy to get (tigris is very easy to use for Census Bureau data, and we’ll use osmdata to get OpenStreetMap shapefiles and data).\nsf objects can be data frames with a geometry column, so you can work with them in many of the same ways as regular data frames; you might also see just plain geometry objects without any data attached.\n\n\nPlotting\nsf objects have a plot method, so you can just call plot(very_cool_sf_data) and get a basic plot:\n\nlibrary(justviz)\nlibrary(sf)\nlibrary(dplyr)\nlibrary(ggplot2)\n\nhead(tracts_sf)\n\n\n\n\n\ncounty\ngeoid\ngeometry\n\n\n\n\nBaltimore city\n24510160300\nPOLYGON ((-76.64612 39.3031…\n\n\nBaltimore city\n24510020100\nPOLYGON ((-76.59047 39.2916…\n\n\nBaltimore County\n24005402602\nPOLYGON ((-76.76732 39.3673…\n\n\nBaltimore city\n24510190300\nPOLYGON ((-76.64696 39.2881…\n\n\nBaltimore city\n24510271801\nPOLYGON ((-76.68336 39.3517…\n\n\nAnne Arundel County\n24003730403\nPOLYGON ((-76.62029 39.1510…\n\n\n\n\n\n# just showing county column\nplot(tracts_sf[, \"county\"])\n\n\n\n\n\n\n\n\n\n\nA note about object-oriented programming (OOP): When you have a function that’s a method of some type of object, just as plot, you can look at help pages for that function by specifying the object type. So ?plot.sf will show you the docs for plot as it applies specifically to sf objects.\nggplot also has geoms for spatial data.\n\nggplot(tracts_sf) +\n  geom_sf(aes(fill = county))\n\n\n\n\n\n\n\n\nYou can aggregate these geometries like you would other data frames, and you can layer the spatial geoms like you would in other types of ggplots.\n\ncounties_sf &lt;- tracts_sf |&gt;\n  group_by(county) |&gt;\n  # if you don't include args to summarise, you'll just get the union \n  # of the geometries\n  summarise()\n\ncounties_sf\n\n\n\n\n\ncounty\ngeometry\n\n\n\n\nAnne Arundel County\nPOLYGON ((-76.4746 38.98177…\n\n\nBaltimore County\nMULTIPOLYGON (((-76.38591 3…\n\n\nBaltimore city\nPOLYGON ((-76.71139 39.3495…\n\n\nHoward County\nPOLYGON ((-76.70608 39.2181…\n\n\n\n\n\nggplot(tracts_sf) +\n  geom_sf(aes(fill = county)) +\n  geom_sf(data = counties_sf, fill = NA, color = \"black\", linewidth = 0.6) \n\n\n\n\n\n\n\n\n\n\nProjections\nFor small geographies like these, the map projection won’t change too much of how the map looks, but for the US it’s very noticeable. Check the CRS first.\n\n# drop island areas that aren't states\nstates_sf &lt;- tigris::states(cb = TRUE, progress_bar = FALSE) |&gt;\n  select(state = STUSPS) |&gt;\n  filter(state %in% state.abb) # dataset in base R\n\nsf::st_crs(states_sf)\n\nCoordinate Reference System:\n  User input: NAD83 \n  wkt:\nGEOGCRS[\"NAD83\",\n    DATUM[\"North American Datum 1983\",\n        ELLIPSOID[\"GRS 1980\",6378137,298.257222101,\n            LENGTHUNIT[\"metre\",1]]],\n    PRIMEM[\"Greenwich\",0,\n        ANGLEUNIT[\"degree\",0.0174532925199433]],\n    CS[ellipsoidal,2],\n        AXIS[\"latitude\",north,\n            ORDER[1],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        AXIS[\"longitude\",east,\n            ORDER[2],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n    ID[\"EPSG\",4269]]\n\nstates_sf |&gt;\n  ggplot() +\n  geom_sf() +\n  labs(title = \"4269: straight from the Census\")\n\n\n\n\n\n\n\nstates_sf |&gt;\n  # common crs for GPS, etc\n  sf::st_transform(4326) |&gt;\n  ggplot() +\n  geom_sf() +\n  labs(title = \"4326: unprojected\")\n\n\n\n\n\n\n\nstates_sf |&gt;\n  sf::st_transform(4326) |&gt;\n  # rotate to avoid splitting Alaska\n  sf::st_shift_longitude() |&gt;\n  ggplot() +\n  geom_sf() +\n  labs(title = \"4326 + shifting\")\n\n\n\n\n\n\n\nstates_sf |&gt;\n  # mercator\n  sf::st_transform(3857) |&gt;\n  ggplot() +\n  geom_sf() +\n  labs(title = \"3857: web mercator\")\n\n\n\n\n\n\n\nstates_sf |&gt;\n  # us natl atlas\n  sf::st_transform(9311) |&gt;\n  ggplot() +\n  geom_sf() +\n  labs(title = \"9311: US equal area\")\n\n\n\n\n\n\n\n\n\n\nExercise\nThe codes to refer to different projections are standardized, and there are several different databases. GeoRepository seems like the easiest to work with (if you have another favorite please share!). Use the text search to find a projection that is:\n\nDesigned for Maryland\nUses feet (units will be listed as “ftUS”) 2\nIn the NAD83 system (not NAD27)\n\n2 Using feet for your units (or a different projection that uses meters) is useful for doing things like buffers or measuring distances; we’ll do a bit of this later.You might find more than one that fits these requirements, just pick one. (For this sort of work you won’t be able to see any difference.)\nFill in the missing CRS code below and draw the maps. What do you notice?\n\ncrs_code &lt;- 2248\n\nmd_sf &lt;- states_sf |&gt; filter(state == \"MD\")\n\nmd_sf |&gt;\n  sf::st_transform(crs_code) |&gt;\n  ggplot() +\n  geom_sf() +\n  labs(title = stringr::str_glue(\"Maryland, CRS {crs_code}\"))\n\n\n\n\n\n\n\nstates_sf |&gt;\n  sf::st_transform(crs_code) |&gt;\n  ggplot() +\n  geom_sf() +\n  labs(title = stringr::str_glue(\"US, CRS {crs_code}\"))",
    "crumbs": [
      "Weekly notes",
      "12. Spatial data"
    ]
  },
  {
    "objectID": "weeks/01_walkthrough.html",
    "href": "weeks/01_walkthrough.html",
    "title": "1. Walkthrough",
    "section": "",
    "text": "Making a chart might seem like a pretty straightforward task: you have data, you make a chart. But there are a lot of things to consider, and they depend on all sorts of factors, like the type of data, your audience, your purpose in visualizing it, the context, and on and on.\nFirst question you should ask yourself:",
    "crumbs": [
      "Weekly notes",
      "1. Walkthrough"
    ]
  },
  {
    "objectID": "weeks/01_walkthrough.html#do-you-even-need-a-chart",
    "href": "weeks/01_walkthrough.html#do-you-even-need-a-chart",
    "title": "1. Walkthrough",
    "section": "Do you even need a chart?",
    "text": "Do you even need a chart?\n\n\n\n\n\n\n\n\nFigure 1\n\n\n\n\n\nThat chart is fine, but it takes up a lot of space to only show 2 numbers. Oftentimes projects that require visualization have caps on length, such as grant applications or print reports where every additional page adds to the budget. In a lot of contexts, you’re probably better off with written text, either just a written sentence if it’s for a report or article, or bullet points or a pull-out quote. You’ll often see documents with something formatted like this:\n\n\nMaryland adults ages 25 and up had median earnings of about $55,000 in 2021, more than $11,000 higher than the US average.\n\n\nMaybe you have a lot of data and your audience wants to see every number exactly, or you just have 2 or 3 numbers but you’re short on space: a nicely formatted table might be better.\n\n\n\n\nTable 1: Median individual earnings, adults ages 25+ with positive earnings, 2021\n\n\n\n\n\n\n\n\n\nLocation\nMedian earnings\n\n\n\n\nUS\n$43,961\n\n\nMaryland\n$55,108\n\n\n\n\n\n\n\n\n\n\nBut if you have somewhere in between a couple data points and too many data points, then making a chart could work. So we’re gonna make a lot of charts.\nIn fact, this data gets at a super pet peeve of mine that can be illustrated well with data visualization.",
    "crumbs": [
      "Weekly notes",
      "1. Walkthrough"
    ]
  },
  {
    "objectID": "weeks/01_walkthrough.html#the-wage-gaps",
    "href": "weeks/01_walkthrough.html#the-wage-gaps",
    "title": "1. Walkthrough",
    "section": "The wage gaps",
    "text": "The wage gaps\nSo we already established that Maryland has higher earnings, on average, than the US. Usually when people say “the wage gap,” they’re talking about women’s pay versus men’s pay, e.g. women make x cents on the dollar.1\n1 All of this analysis is based on Census data, which only records gender as male and female, so that’s what we’ve got to work with.\n\n\n\n\n\n\n\nFigure 2\n\n\n\n\n\nAgain, super important point but doesn’t quite warrant a chart. You’d get a better sense of the comparison that’s more memorable by just writing:\n\n\nWorking women in Maryland make about 78 cents on the male dollar.\n\n\n(And then in the footnotes include the specifications of “among adults ages 25 and up with positive earnings in 2021.”)\nOne explanation often given for women’s pay being less is that women are more likely to work part-time, and part-time jobs pay much less on average.\n\n\n\n\n\n\n\n\nFigure 3\n\n\n\n\n\nSo now we’ve established that there are differences in median earnings between women and men, and between full-time and part-time workers, and that Maryland has a higher median earning than the US as a whole. One of the frameworks we’ll use for understanding our data is data feminism (D’Ignazio, 2015; D’Ignazio & Klein, 2020), which looks at how data can be used to highlight intersections between the patterns we’ve established.\n\nD’Ignazio, C. (2015). What would feminist data visualization look like? https://civic.mit.edu/feminist-data-visualization.html\n\nD’Ignazio, C., & Klein, L. F. (2020). Data feminism. The MIT Press. https://data-feminism.mitpress.mit.edu/\nA good place to start, then, is earnings by sex and full-time status. Let’s make a table:\n\n\n\n\nTable 2: Median individual earnings, adults ages 25+ with positive earnings by sex and full-time status, 2021\n\n\n\n\n\n\n\n\n\nStatus\nMen\nWomen\n\n\n\n\nFull-time\n$72,222\n$62,802\n\n\nPart-time\n$21,579\n$19,200\n\n\n\n\n\n\n\n\n\n\nJust kidding, that’s hard to get a pattern from, and we now have enough comparisons to warrant a chart:\n\n\n\n\n\n\n\n\nFigure 4\n\n\n\n\n\nThe gaps are actually smaller within groups (full-time men vs full-time women or part-time men vs part-time women) than across (all men vs all women). So the tendency of women to work part time explains some of the wage gap, but not all of it. If the difference were only about work status, we would expect to see no within-group gaps here.2\n2 We’ll talk about annotations in a couple weeks, and we’ll return to these charts to practice. Think about what you might want to annotate on these multidimensional charts.Wages by educational attainment aren’t surprising—higher educational attainment corresponds to higher incomes. This is just for adults working full-time.\n\n\n\n\n\n\n\n\nFigure 5\n\n\n\n\n\nWe might as well look at both education and sex. Let’s naively assume the pattern will be the same for men and women.\n\n\n\n\n\n\n\n\nFigure 6\n\n\n\n\n\nEven among only full-time workers, there are gaps at every education level so severe that men at any given education level are paid roughly as much as women at the next level above them. For example, after getting a graduate degree and working full time, women only earn about $1,100 more than men with only a bachelor’s. That’s a pattern that would be hard to glean from just reading a table of numbers or from only seeing the within-groups ratios.\nHere’s a chart of those within-groups ratios that is technically fine but practically not very useful:\n\n\n\n\n\n\n\n\nFigure 7\n\n\n\n\n\nWhich is more important to know, the gap within education levels, or the gap across education levels? Or both? What’s missing that makes this chart alone not very useful?\nSo far we’ve worked through identifying several important patterns using just a very simple type of chart that people commonly know how to read—the charts are almost identical, just different slices of the data. But other chart types might help with highlighting different things, such as those gaps.",
    "crumbs": [
      "Weekly notes",
      "1. Walkthrough"
    ]
  },
  {
    "objectID": "weeks/01_walkthrough.html#exercises",
    "href": "weeks/01_walkthrough.html#exercises",
    "title": "1. Walkthrough",
    "section": "Exercises",
    "text": "Exercises\n\nRevise the code to break down values by race/ethnicity and sex instead of education and sex (i.e. filter for dimension == \"race\" and dimension == \"race_x_sex\" instead of education.)\nBrowse through the Data Visualisation Catalogue (Ribecca, 2024) and see if you find any other chart types that would be appropriate for visualizing the sex and education chart (Figure 6). Which chart types would be better, especially for highlighting the gaps or exploring other aspects of the story? What about (Figure 7)?\nFollowing question 2, Wilke gives a brief example in section 6.3 of switching from bar charts to dot plots or heatmaps; how could this be adapted here?\n\n\n\n\nRibecca, S. (2024). The Data Visualisation Catalogue. https://datavizcatalogue.com/",
    "crumbs": [
      "Weekly notes",
      "1. Walkthrough"
    ]
  },
  {
    "objectID": "weeks/index.html",
    "href": "weeks/index.html",
    "title": "Weekly notes",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nTitle\n\n\nModified\n\n\n\n\n\n\n0. Definitions\n\n\nFebruary 6, 2024\n\n\n\n\n1. Walkthrough\n\n\nFebruary 13, 2024\n\n\n\n\n10. Accessibility and literacy\n\n\nMarch 17, 2024\n\n\n\n\n11. Project 1 troubleshooting\n\n\nMarch 28, 2024\n\n\n\n\n12. Spatial data\n\n\nApril 4, 2024\n\n\n\n\n13. Mapping data to space\n\n\nApril 10, 2024\n\n\n\n\n14. Manipulating geographies\n\n\nApril 16, 2024\n\n\n\n\n15. Color, text, and annotations in mapping\n\n\nApril 16, 2024\n\n\n\n\n16. Putting spatial data in context\n\n\nApril 18, 2024\n\n\n\n\n17. Avoiding sad maps\n\n\nApril 24, 2024\n\n\n\n\n2. Components of a chart\n\n\nFebruary 13, 2024\n\n\n\n\n3. Visual encoding\n\n\nFebruary 15, 2024\n\n\n\n\n3b. Encoding cheatsheets\n\n\nFebruary 13, 2024\n\n\n\n\n4. Learning about your data\n\n\nFebruary 8, 2024\n\n\n\n\n5. Writing good code\n\n\nFebruary 16, 2024\n\n\n\n\n6. Color\n\n\nFebruary 20, 2024\n\n\n\n\n7. Text and annotations\n\n\nMarch 6, 2024\n\n\n\n\n8. Uncertainty and distributions\n\n\nFebruary 29, 2024\n\n\n\n\n9. Making responsible decisions\n\n\nMarch 10, 2024\n\n\n\n\n\nNo matching items\n\n Back to top",
    "crumbs": [
      "Weekly notes"
    ]
  },
  {
    "objectID": "weeks/03b_encoding_refs.html",
    "href": "weeks/03b_encoding_refs.html",
    "title": "3b. Encoding cheatsheets",
    "section": "",
    "text": "A few references on visual encodings that we’ll continue to go back to:\n\nThe chapter of Data Points on Blackboard\nChapter 2 of (Wilke, 2019) about aesthetics\n\n\nWilke, C. (2019). Fundamentals of data visualization: A primer on making informative and compelling figures (First edition). O’Reilly. https://clauswilke.com/dataviz/\n\n\n\nFrom Yau (2013) p. 104\n\nYau, N. (2013). Data points: Visualization that means something. John Wiley & Sons, Inc.\n\n\n\n\n\nFrom Munzner (2014) p. 94\n\nMunzner, T. (2014). Visualization Analysis and Design. A K Peters/CRC Press. https://doi.org/10.1201/b17511\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Weekly notes",
      "3b. Encoding cheatsheets"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Setup",
    "section": "",
    "text": "This is a collection of notes to fill in examples and walkthroughs for the Spring 2024 session of GES 778. It will be updated throughout the semester.\nFirst, install the course’s companion R package, justviz. This is in the class’s GitHub organization.\n\n# install.packages(\"devtools\")\ndevtools::install_github(\"umbc-viz/justviz\")\n\nNext, some packages that aren’t required by the justviz package but will get us started for class and are used in these notes:\n\ncran_pkgs &lt;- c(\"ggplot2\",     # data visualization powerhouse\n               \"dplyr\",       # nice for calculations and data manipulation\n               \"tidyr\",       # data reshaping\n               \"forcats\",     # handling factors\n               \"stringr\",     # string cleanup\n               \"showtext\",    # nice fonts\n               \"ggrepel\",     # force-repelled placement of labels\n               \"osmdata\",     # easy access to sf data from OpenStreetMap\n               \"tigris\",      # shapefiles from Census Bureau TIGER\n               \"patchwork\",   # assembling plots\n               \"ggmap\",       # basemaps in ggplot\n               \"ggtext\",      # use rich text / markdown in ggplot\n               \"here\",        # marks project roots\n               \"knitr\",       # preps conversions between markdown and other formats\n               \"gt\",          # like ggplot but for tables  \n               \"rcartocolor\") # color palettes\n\ninstall.packages(cran_pkgs)\n\n\n\n\n Back to top",
    "crumbs": [
      "Overview",
      "Setup"
    ]
  }
]